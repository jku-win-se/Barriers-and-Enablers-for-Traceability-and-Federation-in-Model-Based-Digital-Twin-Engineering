{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3e457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created: paper\\Conceptual Framework of Information Flow Synchronization Throughout the Building Lifecycle.csv\n",
      "File created: paper\\Modelbased Trustworthiness Evaluation of Autonomous CyberPhysical Production Systems A Systematic Mapping Study.csv\n",
      "File created: paper\\Survey and Practice on Architecture and Deployment Method of Digital Twin System for Intelligent Substation.csv\n",
      "File created: paper\\Architecting Digital Twins.csv\n",
      "File created: paper\\How Can Digital Twins Support the Net Zero Vision.csv\n",
      "File created: paper\\A REVIEW OF DIGITAL TWIN APPLICATIONS IN CONSTRUCTION.csv\n",
      "File created: paper\\Service Computing for Industry 40 State of the Art Challenges and Research Opportunities.csv\n",
      "File created: paper\\A Computer Science Perspective on Digital Transformation in Production.csv\n",
      "File created: paper\\Survey on Cloud Robotics Architecture and ModelDriven Reference Architecture for Decentralized Multicloud HeterogeneousRobotics Platform.csv\n",
      "File created: paper\\A Bibliometric Analysis on Modelbased Systems Engineering.csv\n",
      "File created: paper\\Modeldriven systemperformance engineering for cyberphysical systems.csv\n",
      "File created: paper\\CloudBased Battery Digital Twin Middleware Using ModelBased Development.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Function to extract name and eid data from the References string\n",
    "def extract_name_eid(reference_text):\n",
    "    # Use regex to find all patterns of name and eid\n",
    "    matches = re.findall(r\"<(.*?), \\{eid: (\\d+)\\}>\", reference_text)\n",
    "    return matches\n",
    "\n",
    "# Main function to create CSV files from the input references\n",
    "def create_csv_files_from_references(input_csv_path):\n",
    "    # Create the \"paper\" folder if it doesn't exist\n",
    "    os.makedirs(\"paper\", exist_ok=True)\n",
    "    \n",
    "    # Read the input CSV file\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Loop through each row in the CSV\n",
    "    for _, row in df.iterrows():\n",
    "        title = row['Title']\n",
    "        references = row['References']\n",
    "        \n",
    "        # Extract name and eid pairs from the References text\n",
    "        name_eid_pairs = extract_name_eid(references)\n",
    "        \n",
    "        # Create a DataFrame with the extracted results\n",
    "        df_output = pd.DataFrame(name_eid_pairs, columns=['Name', 'EID'])\n",
    "        \n",
    "        # Replace invalid characters for file names in the title\n",
    "        safe_title = \"\".join([c for c in title if c.isalnum() or c in (\" \", \"_\")]).strip()\n",
    "        \n",
    "        # Save the DataFrame to a CSV file named after the Title value\n",
    "        #output_path = f\"{safe_title}.csv\"\n",
    "        #df_output.to_csv(output_path, index=False)\n",
    "        #print(f\"File created: {output_path}\")\n",
    "        \n",
    "         # Define the output path within the \"paper\" folder\n",
    "        output_path = os.path.join(\"paper\", f\"{safe_title}.csv\")\n",
    "        \n",
    "        # Save the DataFrame to a CSV file named after the Title value\n",
    "        df_output.to_csv(output_path, index=False)\n",
    "        print(f\"File created: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "create_csv_files_from_references(\"SOTA_SECO_CPS_2024-11-27_16-9-26.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b467efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### with EID\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Function to extract name and eid data from the References string, adding prefix to eid\n",
    "def extract_name_eid(reference_text):\n",
    "    # Use regex to find all patterns of name and eid, adding the prefix to eid\n",
    "    matches = re.findall(r\"<(.*?), \\{eid: (\\d+)\\}>\", reference_text)\n",
    "    # Add '2-s2.0-' prefix to each eid\n",
    "    return [(name, f\"2-s2.0-{eid}\") for name, eid in matches]  # 2-s2.0-\n",
    "\n",
    "# Main function to create CSV files from the input references\n",
    "def create_csv_files_from_references(input_csv_path):\n",
    "    # Create the \"paper\" folder if it doesn't exist\n",
    "    os.makedirs(\"paper\", exist_ok=True)\n",
    "    \n",
    "    # Read the input CSV file\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Loop through each row in the CSV\n",
    "    for _, row in df.iterrows():\n",
    "        title = row['Title']\n",
    "        references = row['References']\n",
    "        \n",
    "        # Extract name and eid pairs from the References text with prefix\n",
    "        name_eid_pairs = extract_name_eid(references)\n",
    "        \n",
    "        # Create a DataFrame with the extracted results\n",
    "        df_output = pd.DataFrame(name_eid_pairs, columns=['Name', 'EID'])\n",
    "        \n",
    "        # Replace invalid characters for file names in the title\n",
    "        safe_title = \"\".join([c for c in title if c.isalnum() or c in (\" \", \"_\")]).strip()\n",
    "        \n",
    "        # Define the output path within the \"paper\" folder\n",
    "        output_path = os.path.join(\"paper\", f\"{safe_title}.csv\")\n",
    "        \n",
    "        # Save the DataFrame to a CSV file named after the Title value\n",
    "        df_output.to_csv(output_path, index=False)\n",
    "        print(f\"File created: {output_path}\")\n",
    "\n",
    "# Usage\n",
    "create_csv_files_from_references(\"SOTA_SECO_CPS_2024-11-14_19-47-22.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28c65b",
   "metadata": {},
   "source": [
    "### Import Snowballed papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fe674a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbs\n",
    "import re\n",
    "\n",
    "# Options for plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "sbs.set('paper')\n",
    "\n",
    "# Import litstudy\n",
    "path = os.path.abspath(os.path.join('..'))\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)\n",
    "\n",
    "import litstudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a158a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOTA_SECO_CPS_2024-11-28_17-38-53\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "currentDateAndTime = datetime.now()\n",
    "filename = (f'SOTA_SECO_CPS_{currentDateAndTime.year}-{currentDateAndTime.month}-{currentDateAndTime.day}'\n",
    "            f'_{currentDateAndTime.hour}-{currentDateAndTime.minute}-{currentDateAndTime.second}')\n",
    "filename_xlsx = (f'SOTA_SECO_CPS_{currentDateAndTime.year}-{currentDateAndTime.month}-{currentDateAndTime.day}'\n",
    "            f'_{currentDateAndTime.hour}-{currentDateAndTime.minute}-{currentDateAndTime.second}.xlsx')\n",
    "\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "728a85f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 papers loaded from selected papers list\n"
     ]
    }
   ],
   "source": [
    "############################## LOAD Cleaned papers ###################################\n",
    "\n",
    "# with open('AAA/studies.txt', 'r') as file:\n",
    "#     data = file.read().splitlines()\n",
    "#     print(data)\n",
    "    \n",
    "# Load csv file\n",
    "docs_bib = litstudy.load_scopus_csv('paper/Survey on Cloud Robotics Architecture and ModelDriven Reference Architecture for Decentralized Multicloud HeterogeneousRobotics Platform.csv')\n",
    "print(len(docs_bib), 'papers loaded from selected papers list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "45ab4a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:41<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib, search_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f824bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 papers found on Scopus\n",
      "27 papers were not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4fa5c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bib = docs_bib_scopus # docs_bib_scopus | docs_bib_SemanticScholar | docs_bib_CrossRef\n",
    "docs_filtered = docs_bib # .filter_docs(lambda d: d.publication_year >= 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "76c78e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3390/robotics7030047\n",
      "10.1016/j.infsof.2011.11.009\n",
      "10.5220/0005806101780185\n",
      "10.1109/ACCESS.2020.3000437\n",
      "10.1109/TASE.2014.2376492\n",
      "10.1109/ISCC.2016.7543796\n",
      "10.1109/DSAA.2019.00081\n",
      "10.1145/2890784\n",
      "10.1109/SysEng.2016.7753148\n",
      "10.1109/IROS.2013.6697184\n",
      "10.1109/ACCESS.2016.2574979\n",
      "10.1109/TWC.2018.2813363\n",
      "10.1109/ACCESS.2019.2929296\n",
      "10.1007/978-3-030-36150-1_56\n",
      "10.1109/CIOT.2016.7872914\n",
      "10.1007/s00146-017-0792-6\n",
      "10.1002/cpe.3708\n",
      "10.1145/2245276.2232049\n",
      "10.3390/app10072574\n",
      "10.1109/MCOM.2018.1700131\n",
      "10.1109/TASE.2013.2244883\n",
      "10.1145/3326066\n",
      "10.1109/TII.2016.2530404\n",
      "10.1016/j.ijpe.2019.07.033\n",
      "10.1109/MC.2018.3011041\n",
      "10.1109/aCCESS.2020.2967218\n",
      "10.1016/j.jnca.2017.09.002\n",
      "10.1117/12.926523\n",
      "10.1109/SYSOSE.2017.7994942\n",
      "A distributed architecture for supervision of autonomous multi-robot missions: Application to air-sea scenarios, {eid: 84982299368, doi: 10.1007/s10514-016-9603-z}>, <Edge-enabled autonomous navigation and computer vision as a service: A study on mobile robots onboard energy consumption and computing requirements\n",
      "A reference architecture for social head gaze generation in social robotics, {eid: 84947079556, doi: 10.1007/s12369-015-0315-x}>, <Executing robot task models in dynamic environments\n",
      "A survey on cloud computing security: Issues, threats and solution\n",
      "CAPER: A crossapplication permissioned blockchain, {eid: 85080339886, doi: 10.14778/3342263.3342275}>, <Towards a unified taxonomy and architecture of cloud frameworks\n",
      "Cloud robotics: A software architecture\n",
      "Constraintdriven dynamic workflow for automation of big data analytics based on GraphPlan, {eid: 85032354849, doi: 10.1109/ICWS.2017.120}>, <Big data analytic service discovery using social service network with domain ontology and workflow awareness, {eid: 84990967375, doi: 10.1109/ICWS.2016.49}>, <QoS and customizable transaction-aware selection for big data analytics on automatic service composition\n",
      "Cooperative heterogeneous multirobot systems: A survey\n",
      "DAvinCi: A cloud computing framework for service robots, {eid: 77955831617, doi: 10.1109/ROBOT.2010.5509469}>, <Robot as a service in cloud computing, {eid: 78449242569, doi: 10.1109/SOSE.2010.44}>, <Robots as-A-service in cloud computing: Search and rescue in large-scale disasters case study, {eid: 85046948076, doi: 10.1109/CCNC.2018.8319200}>, <ROSLink: Bridging ROS with the Internet-of-Things for cloud robotics, {eid: 85019669935, doi: 10.1007/978-3-319-54927-9_8}>, <Cloud robotics: Architecture, challenges and applications\n",
      "Digital twindriven product design, manufacturing and service with big data\n",
      "End-toend optimization for geo-distributed MapReduce\n",
      "Hybrid robot-as-aservice (RaaS) platform (using MQTT and CoAP), {eid: 85074816758, doi: 10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00171}>, <Data acquisition framework for cloud robotics, {eid: 85077768182, doi: 10.1109/ICAwST.2019.8923436}>, <Fog-enabled multi-robot systems, {eid: 85048094747, doi: 10.1109/CFEC.2018.8358727}>, <A knowledge interchange format (KIF) for robots in cloud, {eid: 85081161989, doi: 10.1109/ICCES45898.2019.9002191}>, <Cloud-based robotic system for crowd control in smart cities using hybrid intelligent generic algorithm, {eid: 85081916718, doi: 10.1007/s12652-020-01758-w}>, <Deep reinforcement learning for autonomous Internet of Things: Model, applications and challenges, {eid: 85089540507, doi: 10.1109/COMST.2020.2988367}>, <A domain-specific software architecture for adaptive intelligent systems, {eid: 0029288791, doi: 10.1109/32.385968}>, <Reference architecture for robot teleoperation: Development details and practical use, {eid: 0035310607, doi: 10.1016/S0967-0661(00)00121-0}>, <Towards a taxonomy of services for developing service-oriented robotic systems\n",
      "Improving high-impact numerical weather prediction with lidar and drone observations, {eid: 85088749302, doi: 10.1175/BAMS-D-19-0119.1}>, <A survey on blockchain-based Internet service architecture: Requirements, challenges, trends, and future\n",
      "Industry 4.0\n",
      "Model-driven separation of concerns for service robotics, {eid: 85015193041, doi: 10.1145/3023147.3023151}>, <A reference architecture for deploying component-based robot software and comparison with existing tools, {eid: 85049632320, doi: 10.1109/IRC.2018.00026}>, <A reference architecture for satellite control systems, {eid: 85060922715, doi: 10.1007/s11334-019-00322-w}>, <Material-integrated cluster computing in selfadaptive robotic materials using mobile multi-agent systems, {eid: 85059562703, doi: 10.1007/s10586-018-02894-x}>, <Multipotent systems: Combining planning, self-organization, and reconfiguration in modular robot ensembles, {eid: 85058911392, doi: 10.3390/s19010017}>, <An implementing framework for Holonic manufacturing control with multiple robotvision stations, {eid: 67349215793, doi: 10.1016/j.engappai.2009.03.001}>, <A generic controller architecture for intelligent robotic systems, {eid: 78649317088, doi: 10.1016/j.rcim.2010.07.013}>, <Human exploration of mars, design reference architecture 5.0, {eid: 77952821004, doi: 10.1109/AERO.2010.5446736}>, <Lunar in-situ resource utilization in the ISECG human lunar exploration reference architecture\n",
      "MyBot: Cloud-based service robot using service-oriented architecture\n",
      "None, {eid: 84892029522, doi: 10.1007/978-3-540-68899-0}>, <Realising interoperability between OPC UA and OCF\n",
      "QoS-aware rule-based Trafficefficient multiobjective service selection in big data space, {eid: 85052636646, doi: 10.1109/ACCESS.2018.2867633}>, <Technology update on the unified architecture framework (UAF)\n",
      "Reliable cloud-based robot services, {eid: 84893564549, doi: 10.1109/IECON.2013.6700526}>, <Reliable, cloud-based communication for multi-robot systems, {eid: 84906516213, doi: 10.1109/TePRA.2014.6869142}>, <Cloud-based robotic system: Architecture framework and deployment models\n",
      "Research on SOA-based architecture of collaborative product commerce, {eid: 84864276075, doi: 10.1109/ISRA.2012.6219120}>, <Modeling and reusing robotic software architectures: The HyperFlex toolchain, {eid: 84929193296, doi: 10.1109/ICRA.2014.6907806}>, <RoboEarth, {eid: 79959502591, doi: 10.1109/MRA.2011.941632}>, <Server-sided automatic map transformation in RoboEarth\n",
      "RoboEarth semantic mapping: A cloud enabled knowledge-based approach, {eid: 85027971073, doi: 10.1109/TASE.2014.2377791}>, <Rapyuta: A cloud robotics platform, {eid: 85027933046, doi: 10.1109/TASE.2014.2329556}>, <Cloud based centralized task control for human domain multi-robot operations, {eid: 84952982944, doi: 10.1007/s11370-015-0185-y}>, <Industry 5.0_A human-centric solution, {eid: 85070751081, doi: 10.3390/su11164371}>, <Consider the human work experience when integrating robotics in the workplace, {eid: 85063982477, doi: 10.1109/HRI.2019.8673139}>, <SmartCityWare: A service-oriented middleware for cloud and fog enabled smart city services\n",
      "ROS: An open-source robot operating system\n",
      "Society 5.0: Aiming for a new human-centered society\n",
      "The Hadoop distributed file system\n",
      "The Hadoop distributed file system: Architecture and design\n",
      "Ultra-ne grain landscape-scale quantification of dryland vegetation structure with droneacquired structure-from-motion photogrammetry\n",
      "Using game development to teach software architecture, {eid: 80052679924, doi: 10.1155/2011/920873}>, <Multi-modal visual attention for robotics active vision systems_A reference architecture\n",
      "Working on the edge\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "data = []\n",
    "while index < len(docs_filtered):\n",
    "    # re.sub('[<\\[\\]>]', '', str(docs_filtered[index].authors))\n",
    "\n",
    "    authorList = []\n",
    "    for author in docs_filtered[index].authors or []:\n",
    "        authorList.append(author.name)\n",
    "\n",
    "    if type(docs_filtered[index].id.doi) == type(None):\n",
    "        print(docs_filtered[index].title)\n",
    "        doi_paper = ''\n",
    "        doi_paper_custom = ''\n",
    "        # print(doi_paper)\n",
    "    else:\n",
    "        doi_paper = str(docs_filtered[index].id.doi) # 'https://www.doi.org/' + \n",
    "        doi_paper_custom = 'https://www.doi.org/' + str(docs_filtered[index].id.doi)\n",
    "        print(doi_paper)\n",
    "\n",
    "    data.append({'Authors': '', 'Author full names': re.sub(r'[\\[\\'\\]]', '', str(authorList)), 'Author(s) ID': '', \n",
    "    'Title': docs_filtered[index].title, 'Year': docs_filtered[index].publication_year, 'Source title': docs_filtered[index].publication_source, \n",
    "    'Volume': '', 'Issue': '', 'Art. No.': '', 'Page start': '', 'Page end': '', 'Page count': '', 'Cited by': docs_filtered[index].citation_count, \n",
    "    'DOI': doi_paper, 'Link': doi_paper_custom,  'Affiliations': re.sub(r'[\\[\\'\\]]', '', str(docs_filtered[index].affiliations)), \n",
    "    'Authors with affiliations': '', 'Abstract': docs_filtered[index].abstract, 'Author Keywords': re.sub(r'[\\[\\'\\]]', '', str(docs_filtered[index].keywords)), \n",
    "    'Index Keywords': '', 'Molecular Sequence Numbers': '',  'Chemicals/CAS': '', 'Tradenames': '', 'Manufacturers': '', \n",
    "    'Funding Details': '', 'Funding Texts': '', 'References': re.sub(r'[\\[\\'\\]]', '', str(docs_filtered[index].references)), 'Correspondence Address': '', 'Editors': '', 'Publisher': docs_filtered[index].publisher, \n",
    "    'Sponsors': '', 'Conference name': '', 'Conference date': str(docs_filtered[index].publication_date), 'Conference location': '', 'Conference code': '', 'ISSN': '', \n",
    "    'ISBN': '', 'CODEN': '', 'PubMed ID': docs_filtered[index].id.pubmed, 'Language of Original Document': docs_filtered[index].language, 'Abbreviated Source Title': '', 'Document Type': docs_filtered[index].source_type, \n",
    "    'Publication Stage': '', 'Open Access': '', 'Source': '', 'EID': docs_filtered[index].id.scopusid})\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "# Saving first group of data to a single excel file\n",
    "df = pd.DataFrame(data, columns=['Authors', 'Author full names', 'Author(s) ID', \n",
    "    'Title', 'Year', 'Source title', \n",
    "    'Volume', 'Issue', 'Art. No.', 'Page start', 'Page end', 'Page count', 'Cited by', \n",
    "    'DOI', 'Link',  'Affiliations', 'Authors with affiliations', 'Abstract', 'Author Keywords', \n",
    "    'Index Keywords', 'Molecular Sequence Numbers',  'Chemicals/CAS', 'Tradenames', 'Manufacturers', \n",
    "    'Funding Details', 'Funding Texts', 'References', 'Correspondence Address', 'Editors', 'Publisher', \n",
    "    'Sponsors', 'Conference name', 'Conference date', 'Conference location', 'Conference code', 'ISSN', \n",
    "    'ISBN', 'CODEN', 'PubMed ID', 'Language of Original Document', 'Abbreviated Source Title', 'Document Type', \n",
    "    'Publication Stage', 'Open Access', 'Source', 'EID'])\n",
    "\n",
    "# Saving first group of data to a single csv file\n",
    "df.to_csv('Snow/P12_' + filename + '.csv')\n",
    "\n",
    "# Saving first group of data to a single excel file\n",
    "# df.to_excel('Snow/P2_' + filename_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec74364e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "db69796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: utf_8\n",
      "Loaded 9 rows from the input CSV.\n",
      "Compiled OR regex pattern: re.compile('systematic|Systematic|literature|Literature|review|Review|Mapping|mapping|study|Study|SLR|SMS|survey|Survey', re.IGNORECASE)\n",
      "Number of matching rows: 7\n",
      "Row 0 skipped: Automating big data analysis based on deep learning generation by automatic service composition © 2019 IEEE.Automation of Big Data Analysis (BDA) procedure gives us a great profit in the era of Big Data and Artificial Intelligence. BDA procedure can be efficiently automated by the automatic service composition concept efficiently. Our previous work for Auto-BDA shows a great future prospect in reducing turnaround time for data analysis. Moreover, it requires consideration of the automation with a well-geared combination of the data preparation and the optimal model (deep learning) generation. This paper shows the construction of automating BDA and model generation (here deep learning) together with data preparation and parameter optimization. Automatic-service-composition, Automation, Big-Data-Analysis, CRISP-DM, Deep-learning-generation\n",
      "Row 3 skipped: A Requirements Driven Digital Twin Framework: Specification and Opportunities © 2013 IEEE.Among the tenets of Smart Manufacturing (SM) or Industry 4.0 (I4.0), digital twin (DT), which represents the capabilities of virtual representations of components and systems, has been cited as the biggest technology trend disrupting engineering and design today. DTs have been in use for years in areas such as model-based process control and predictive maintenance, however moving forward a framework is needed that will support the expected pervasiveness of DT technology in the evolution of SM or I4.0. A set of requirements for a DT framework has been derived from analysis of DT definitions, DTs in use today, expected DT applications in the near future, and longer-term DT trends and the DT vision in SM. These requirements include elements of re-usability, interoperability, interchangeability, maintainability, extensibility, and autonomy across the entire DT lifecycle. A baseline framework for DT technology has been developed that addresses many aspects of these requirements and enables the addressing of the requirements more fully through additional specification. The baseline framework includes a definition of a DT and an object-oriented (O-O) architecture for DTs that defines generalization, aggregation and instantiation of DT classes. Case studies using and extending the baseline framework illustrate its advantages in supporting DT solutions and trends in SM. Digital twin, industry 40, modeling, prediction, smart manufacturing\n",
      "Filtered rows saved to filtered_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from charset_normalizer import from_path\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"Sanitize text to handle problematic characters.\"\"\"\n",
    "    replacements = {\n",
    "        \"ŌĆ‘\": \"'\", \"ŌĆ’\": \"'\", \"ŌĆœ\": '\"', \"ŌĆ\": '\"', \"┬®\": \"®\"\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "def filter_csv_by_or_condition(input_csv, output_csv, search_conditions, columns_to_search):\n",
    "    \"\"\"\n",
    "    Filters rows from a CSV file based on OR conditions (case-insensitive).\n",
    "\n",
    "    Parameters:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to save the filtered CSV.\n",
    "        search_conditions (list): List of strings for OR conditions.\n",
    "        columns_to_search (list): List of column names to search within.\n",
    "\n",
    "    Returns:\n",
    "        None: The filtered rows are saved to the output CSV file.\n",
    "    \"\"\"\n",
    "    # Detect the file's encoding using charset-normalizer\n",
    "    detected = from_path(input_csv).best()\n",
    "    detected_encoding = detected.encoding\n",
    "    print(f\"Detected encoding: {detected_encoding}\")\n",
    "\n",
    "    # Load the CSV file into a DataFrame using the detected encoding\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv, encoding=detected_encoding)\n",
    "        print(f\"Loaded {len(df)} rows from the input CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    # Check if required columns exist\n",
    "    missing_columns = [col for col in columns_to_search if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Missing columns in the input CSV: {missing_columns}\")\n",
    "        return\n",
    "\n",
    "    # Sanitize text in relevant columns\n",
    "    df[columns_to_search] = df[columns_to_search].fillna(\"\").astype(str).applymap(sanitize_text)\n",
    "\n",
    "    # Compile a regex pattern for OR conditions (case-insensitive)\n",
    "    or_pattern = re.compile('|'.join(re.escape(term).replace(r'\\*', '.*') for term in search_conditions), re.IGNORECASE)\n",
    "    print(f\"Compiled OR regex pattern: {or_pattern}\")\n",
    "\n",
    "    # Create a boolean mask: rows must match the OR pattern in any of the specified columns\n",
    "    def row_matches(row):\n",
    "        row_text = ' '.join(row.astype(str))  # Combine the text of the specified columns\n",
    "        return bool(or_pattern.search(row_text))\n",
    "\n",
    "    mask = df[columns_to_search].apply(row_matches, axis=1)\n",
    "    print(f\"Number of matching rows: {mask.sum()}\")\n",
    "\n",
    "    # Log skipped rows for debugging\n",
    "    for idx, row in df[~mask][columns_to_search].iterrows():\n",
    "        row_text = ' '.join(row.astype(str))\n",
    "        print(f\"Row {idx} skipped: {row_text}\")\n",
    "\n",
    "    # Filter the DataFrame using the mask\n",
    "    filtered_df = df[mask]\n",
    "    if filtered_df.empty:\n",
    "        print(\"No rows matched the specified conditions.\")\n",
    "    else:\n",
    "        filtered_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Filtered rows saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv = \"Snow/P12_SOTA_SECO_CPS_2024-11-28_17-38-53.csv\"\n",
    "output_csv = \"filtered_output.csv\"\n",
    "\n",
    "# Define search conditions for OR logic\n",
    "search_conditions = [\n",
    "    \"systematic\", \"Systematic\", \"literature\", \"Literature\", \"review\", \"Review\",\n",
    "    \"Mapping\", \"mapping\", \"study\", \"Study\", \"SLR\", \"SMS\", \"survey\", \"Survey\"\n",
    "]\n",
    "\n",
    "columns_to_search = [\"Title\", \"Abstract\", \"Author Keywords\"]\n",
    "\n",
    "filter_csv_by_or_condition(input_csv, output_csv, search_conditions, columns_to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8e0771ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## LOAD Cleaned papers ###################################\n",
    "\n",
    "# with open('AAA/studies.txt', 'r') as file:\n",
    "#     data = file.read().splitlines()\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d96fe871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 359.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 papers found on Scopus\n",
      "0 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_1 = litstudy.load_scopus_csv('P1_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_1), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_1, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_1 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0ec412ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:10<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 papers found on Scopus\n",
      "2 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_2 = litstudy.load_scopus_csv('P2_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_2), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_2, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_2 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0a1abec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 papers found on Scopus\n",
      "0 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_3 = litstudy.load_scopus_csv('P3_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_3), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_3, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_3 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8685317a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:17<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 papers found on Scopus\n",
      "0 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_4 = litstudy.load_scopus_csv('P4_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_4), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_4, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_4 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "da52580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 papers found on Scopus\n",
      "0 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_6 = litstudy.load_scopus_csv('P6_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_6), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_6, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_6 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "65210b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 papers found on Scopus\n",
      "0 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_7 = litstudy.load_scopus_csv('P7_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_7), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_7, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_7 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0c752680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:06<00:00,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 papers found on Scopus\n",
      "1 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_8 = litstudy.load_scopus_csv('P8_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_8), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_8, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_8 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e775c119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:03<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 papers found on Scopus\n",
      "0 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_9 = litstudy.load_scopus_csv('P9_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_9), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_9, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_9 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "50a58b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:05<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 papers found on Scopus\n",
      "0 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_10 = litstudy.load_scopus_csv('P10_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_10), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_10, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_10 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "38349ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 papers found on Scopus\n",
      "0 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_11 = litstudy.load_scopus_csv('P11_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_11), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_11, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_11 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c5071dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 papers loaded from selected papers list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 papers found on Scopus\n",
      "1 papers were not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "docs_bib_12 = litstudy.load_scopus_csv('P12_SOTA_SECO_CPS_2024-11-28_17-38-53_snowballed.csv')\n",
    "print(len(docs_bib_12), 'papers loaded from selected papers list')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "docs_found_scopus, docs_notfound_scopus = litstudy.refine_scopus(docs_bib_12, search_title=True)\n",
    "\n",
    "print(len(docs_found_scopus), 'papers found on Scopus')\n",
    "print(len(docs_notfound_scopus), 'papers were not found')\n",
    "docs_bib_scopus_12 = docs_found_scopus | docs_notfound_scopus\n",
    "len(docs_bib_scopus_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "00dbd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bib = docs_bib_scopus_1 | docs_bib_scopus_2 | docs_bib_scopus_3 | docs_bib_scopus_4 | docs_bib_scopus_6 | docs_bib_scopus_7 | docs_bib_scopus_8 | docs_bib_scopus_9 | docs_bib_scopus_10 | docs_bib_scopus_11 | docs_bib_scopus_12\n",
    "docs_filtered = docs_bib # .filter_docs(lambda d: d.publication_year >= 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2ed0167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1109/TII.2018.2873186\n",
      "10.1002/sys.21566\n",
      "10.1109/ACCESS.2019.2953499\n",
      "10.1109/JPROC.2020.2998530\n",
      "10.1007/978-3-030-49435-3_6\n",
      "10.1109/ISSE46696.2019.8984568\n",
      "10.1109/SYSCON.2019.8836869\n",
      "10.1177/0037549719829828\n",
      "10.1080/0951192X.2019.1599433\n",
      "10.1109/ICAASE51408.2020.9380125\n",
      "10.3390/app11073014\n",
      "10.1002/jmri.27448\n",
      "10.4271/12-04-01-0003\n",
      "A case for integrated data processing in large-scale cyber-physical systems\n",
      "10.1016/j.cie.2019.106004\n",
      "10.1007/s12525-019-00362-x\n",
      "10.14488/BJOPM.2019.v16.n2.a8\n",
      "Edge cloud as an enabler for distributed AI in industrial IoT applications: The experience of the iotwins project\n",
      "10.1080/0144929X.2019.1581258\n",
      "10.1145/3365438.3410934\n",
      "10.1007/s10845-019-01512-w\n",
      "10.1016/j.cirpj.2020.02.002\n",
      "10.1145/3419394.3423666\n",
      "10.1145/3365438.3410941\n",
      "10.1007/s10270-019-00757-6\n",
      "10.1109/ICCWorkshops49005.2020.9145100\n",
      "10.3233/ATDE200188\n",
      "10.1109/CNS48642.2020.9162337\n",
      "10.1016/j.ipm.2021.102529\n",
      "10.1007/s13347-021-00450-x\n",
      "10.1515/pjbr-2021-0008\n",
      "10.1109/JSYST.2020.2993323\n",
      "10.1080/00207543.2020.1824085\n",
      "Development of manufacturing execution systems in accordance with industry 4. 0 requirements: A review of standard-and ontologybased methodologies and tools\n",
      "Explainable artificial intelligence: Concepts, taxonomies, opportunities and challenges toward responsible AI\n",
      "10.18178/ijmerr.9.2.258-263\n",
      "10.1007/s10916-020-01623-5\n",
      "10.1080/0951192X.2019.1599436\n",
      "10.1145/3386164.3387296\n",
      "10.1109/INDIN41052.2019.8972134\n",
      "10.1016/j.ijinfomgt.2019.05.020\n",
      "10.1109/SIBIRCON48586.2019.8958367\n",
      "10.1109/ICCD46524.2019.00019\n",
      "10.1007/s10845-019-01516-6\n",
      "10.1007/978-3-030-69373-2_9\n",
      "10.1007/978-3-030-48989-2_60\n",
      "10.1007/978-3-030-69373-2_6\n",
      "10.1007/978-3-030-69373-2_7\n",
      "10.1515/auto-2020-0003\n",
      "10.1109/ACCESS.2020.2998358\n",
      "10.1109/ACCESS.2020.2970143\n",
      "10.1109/IEEM45057.2020.9309951\n",
      "10.1299/jamdsm.2020jamdsm0100\n",
      "10.1109/IEEM45057.2020.9309745\n",
      "10.3390/app10248903\n",
      "10.1109/ACCESS.2020.3026541\n",
      "10.1007/978-3-030-62522-1_28\n",
      "10.1145/3417990.3421446\n",
      "10.1007/978-3-030-91279-6_16\n",
      "10.3390/app11198875\n",
      "10.1080/0951192X.2020.1775297\n",
      "10.3389/frobt.2021.758099\n",
      "10.3390/app11094276\n",
      "10.1145/3459960.3459978\n",
      "10.1016/j.jii.2021.100242\n",
      "10.1016/j.aei.2020.101225\n",
      "10.1016/j.jmsy.2020.10.015\n",
      "10.1109/ICPS49255.2021.9468224\n",
      "10.1080/0951192X.2020.1736637\n",
      "10.1016/j.ifacol.2021.08.096\n",
      "10.1016/j.compind.2021.103469\n",
      "10.1016/j.jmsy.2020.04.012\n",
      "10.1115/MSEC2021-64438\n",
      "10.1016/j.cie.2021.107241\n",
      "10.1007/s00170-021-07490-9\n",
      "10.3390/app11125633\n",
      "10.23919/DATE51398.2021.9474133\n",
      "10.1109/ACCESS.2021.3060863\n",
      "10.1109/ICPS49255.2021.9468209\n",
      "10.1109/TII.2020.3011062\n",
      "10.3390/s21248194\n",
      "10.1109/TETC.2021.3131532\n",
      "10.1109/MIC.2021.3056923\n",
      "10.1109/TII.2021.3086149\n",
      "10.1080/00207543.2021.1966118\n",
      "10.1002/9781394303564.ch14\n",
      "10.1016/j.landusepol.2021.105368\n",
      "10.1016/j.jum.2022.12.001\n",
      "10.3390/su141610028\n",
      "10.3390/su11010159\n",
      "10.1007/s43615-022-00175-9\n",
      "10.1177/87569728211070225\n",
      "10.3389/frsc.2021.663269\n",
      "10.3390/su11236631\n",
      "10.1093/eurheartj/ehaa159\n",
      "10.1007/978-3-030-41568-6_9\n",
      "10.1016/j.infsof.2018.09.006\n",
      "Modelling availability risks of IT threats in smart factory networks - A modular Petri net approach\n",
      "10.1016/j.jestch.2019.01.006\n",
      "10.1016/j.jss.2020.110697\n",
      "10.1007/978-3-030-64330-0_2\n",
      "10.1109/TASE.2020.2980726\n",
      "10.3390/electronics10202497\n",
      "10.1016/j.promfg.2020.10.163\n",
      "10.1109/TII.2020.3043802\n",
      "10.1109/TII.2021.3059676\n",
      "10.1145/3462513\n",
      "10.1109/ACCESS.2022.3172964\n",
      "10.1016/j.jss.2021.111081\n",
      "10.1007/s12652-021-03137-5\n",
      "10.1109/ISC257844.2023.10293672\n",
      "10.1109/ICSA56044.2023.00015\n",
      "Towards trustworthy autonomous systems: A survey of taxonomies and future perspectives\n",
      "10.1080/17517575.2018.1442934\n",
      "10.1109/SANER.2019.8668007\n",
      "10.1145/3323212\n",
      "10.1109/RTSS49844.2020.00012\n",
      "10.1109/ACCESS.2020.2987324\n",
      "10.1002/sys.21549\n",
      "10.1080/00207543.2019.1636321\n",
      "10.1515/eng-2019-0065\n",
      "10.1016/j.promfg.2019.03.058\n",
      "10.1109/METROI4.2019.8792918\n",
      "10.1016/j.arcontrol.2019.03.005\n",
      "10.1007/978-3-030-21451-7_1\n",
      "10.3390/jsan8020025\n",
      "10.1080/0951192X.2019.1572225\n",
      "10.1109/MetroInd4.0IoT48571.2020.9138228\n",
      "10.1007/s40684-020-00227-1\n",
      "10.1016/j.future.2020.05.012\n",
      "10.3390/s20010109\n",
      "10.1016/j.is.2020.101491\n",
      "10.1145/3381038\n",
      "10.1145/3366370\n",
      "10.13196/j.cims.2019.01.001\n",
      "10.1109/TPEL.2019.2911594\n",
      "10.7500/AEPS20190726001\n",
      "10.13335/j.1000-3673.pst.2019.0245\n",
      "10.13334/j.0258-8013.pcsee.201405\n",
      "10.13334/j.0258-8013.pcsee.211594\n",
      "10.13335/j.1000-3673.pst.2021.0661\n",
      "10.11999/JEIT220629\n",
      "10.1109/ACCESS.2019.2929296\n",
      "10.1007/978-3-030-36150-1_56\n",
      "10.3390/app10072574\n",
      "10.1145/3326066\n",
      "10.1016/j.ijpe.2019.07.033\n",
      "10.1109/aCCESS.2020.2967218\n",
      "Cooperative heterogeneous multirobot systems: A survey\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "data = []\n",
    "while index < len(docs_filtered):\n",
    "    # re.sub('[<\\[\\]>]', '', str(docs_filtered[index].authors))\n",
    "\n",
    "    authorList = []\n",
    "    for author in docs_filtered[index].authors or []:\n",
    "        authorList.append(author.name)\n",
    "\n",
    "    if type(docs_filtered[index].id.doi) == type(None):\n",
    "        print(docs_filtered[index].title)\n",
    "        doi_paper = ''\n",
    "        doi_paper_custom = ''\n",
    "        # print(doi_paper)\n",
    "    else:\n",
    "        doi_paper = str(docs_filtered[index].id.doi) # 'https://www.doi.org/' + \n",
    "        doi_paper_custom = 'https://www.doi.org/' + str(docs_filtered[index].id.doi)\n",
    "        print(doi_paper)\n",
    "\n",
    "    data.append({'Authors': '', 'Author full names': re.sub(r'[\\[\\'\\]]', '', str(authorList)), 'Author(s) ID': '', \n",
    "    'Title': docs_filtered[index].title, 'Year': docs_filtered[index].publication_year, 'Source title': docs_filtered[index].publication_source, \n",
    "    'Volume': '', 'Issue': '', 'Art. No.': '', 'Page start': '', 'Page end': '', 'Page count': '', 'Cited by': docs_filtered[index].citation_count, \n",
    "    'DOI': doi_paper, 'Link': doi_paper_custom,  'Affiliations': re.sub(r'[\\[\\'\\]]', '', str(docs_filtered[index].affiliations)), \n",
    "    'Authors with affiliations': '', 'Abstract': docs_filtered[index].abstract, 'Author Keywords': re.sub(r'[\\[\\'\\]]', '', str(docs_filtered[index].keywords)), \n",
    "    'Index Keywords': '', 'Molecular Sequence Numbers': '',  'Chemicals/CAS': '', 'Tradenames': '', 'Manufacturers': '', \n",
    "    'Funding Details': '', 'Funding Texts': '', 'References': re.sub(r'[\\[\\'\\]]', '', str(docs_filtered[index].references)), 'Correspondence Address': '', 'Editors': '', 'Publisher': docs_filtered[index].publisher, \n",
    "    'Sponsors': '', 'Conference name': '', 'Conference date': str(docs_filtered[index].publication_date), 'Conference location': '', 'Conference code': '', 'ISSN': '', \n",
    "    'ISBN': '', 'CODEN': '', 'PubMed ID': docs_filtered[index].id.pubmed, 'Language of Original Document': docs_filtered[index].language, 'Abbreviated Source Title': '', 'Document Type': docs_filtered[index].source_type, \n",
    "    'Publication Stage': '', 'Open Access': '', 'Source': '', 'EID': docs_filtered[index].id.scopusid})\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "# Saving first group of data to a single excel file\n",
    "df = pd.DataFrame(data, columns=['Authors', 'Author full names', 'Author(s) ID', \n",
    "    'Title', 'Year', 'Source title', \n",
    "    'Volume', 'Issue', 'Art. No.', 'Page start', 'Page end', 'Page count', 'Cited by', \n",
    "    'DOI', 'Link',  'Affiliations', 'Authors with affiliations', 'Abstract', 'Author Keywords', \n",
    "    'Index Keywords', 'Molecular Sequence Numbers',  'Chemicals/CAS', 'Tradenames', 'Manufacturers', \n",
    "    'Funding Details', 'Funding Texts', 'References', 'Correspondence Address', 'Editors', 'Publisher', \n",
    "    'Sponsors', 'Conference name', 'Conference date', 'Conference location', 'Conference code', 'ISSN', \n",
    "    'ISBN', 'CODEN', 'PubMed ID', 'Language of Original Document', 'Abbreviated Source Title', 'Document Type', \n",
    "    'Publication Stage', 'Open Access', 'Source', 'EID'])\n",
    "\n",
    "# Saving first group of data to a single csv file\n",
    "df.to_csv('snoballed_Final' + filename + '.csv')\n",
    "\n",
    "# Saving first group of data to a single excel file\n",
    "# df.to_excel('Snow/P2_' + filename_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90519f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
