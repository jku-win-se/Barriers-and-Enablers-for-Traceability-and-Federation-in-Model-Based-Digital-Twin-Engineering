Scopus
EXPORT DATE: 07 October 2024

@ARTICLE{Maleika2024920,
	author = {Maleika, Wojciech},
	title = {Local Polynomial Interpolation Method Optimization in the Process of Digital Terrain Model Creation Based on Data Collected From a Multibeam Echosounder},
	year = {2024},
	journal = {IEEE Journal of Oceanic Engineering},
	volume = {49},
	number = {3},
	pages = {920 – 932},
	doi = {10.1109/JOE.2024.3353271},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188518297&doi=10.1109%2fJOE.2024.3353271&partnerID=40&md5=d33278b76f0e2e758dc4f17629d0226e},
	abstract = {This article describes a local polynomial interpolation (LPI) optimization used to create digital terrain models (DTM) of the seabed based on data collected via a multibeam echosounder (MBES) during a sea survey. In the studies presented in this article, the optimal parameters of this interpolation are sought in terms of the accuracy of the created models and the calculation time. The parameters to be optimized are: the size of the area from which we select the points for interpolation (radius size), the number of points involved in the local interpolation (no), and the polynomial degree used in the interpolation (poly degree). Based on the obtained results, it was shown that the optimal values of these parameters can be selected for this type of input data, and their value depends mainly on grid resolution and the density of measurement points collected during the sea survey. Based on research using various test surfaces, it has been shown that the use of properly selected interpolation parameters enables the creation of models with slightly higher accuracy. During the research, attention was also paid to the speed of calculations, which is an important aspect of the process of creating bathymetric models. It was assumed that the new method should not significantly increase the calculation time. Finally, the author proposed using a new point selection technique (named the growing radius) during LPI, which made it possible to further increase the accuracy of the created models and the speed of calculations. The results obtained are compared with other commonly used interpolation methods using the same test data, showing the good and the bad features of the optimized LPI method. The final results of the research and the conclusions presented in this article indicate that the use of the optimized LPI together with the new technique of selecting points (the growing radius) can be a better alternative to other interpolation methods used in the process of creating bathymetric models based on data from MBES.  © 2024 IEEE.},
	author_keywords = {Digital bathymetric models; digital terrain models (DTM) creation; GIS systems; local polynomial interpolation (LPI); multibeam echosounder (MBES)},
	keywords = {Bathymetry; Hydrographic surveys; Landforms; Polynomials; Surface waters; Uncertainty analysis; Digital bathymetric model; Digital terrain model; Digital terrain model  creation; GIS systems; Local polynomial interpolation; Local polynomials; Measurement uncertainty; Model creation; Multi-beam echo sounders; Multibeam echosound; Noise measurements; Polynomial interpolation; Sea measurements; Sea surfaces; Interpolation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Meng2022,
	author = {Meng, Xiangtian and Bao, Yilin and Zhang, Xinle and Wang, Xiang and Liu, Huanjun},
	title = {Prediction of soil organic matter using different soil classification hierarchical level stratification strategies and spectral characteristic parameters},
	year = {2022},
	journal = {Geoderma},
	volume = {411},
	doi = {10.1016/j.geoderma.2022.115696},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122615630&doi=10.1016%2fj.geoderma.2022.115696&partnerID=40&md5=d8e7c9c961e88b281c37680aa97c687d},
	abstract = {Whether a finer soil classification hierarchical stratification strategy and the spectral characteristic parameters (SCPs) that describe the shape of the spectral curve can be used to improve the prediction accuracy of soil organic matter should be clarified. We measured the visible, near-infrared and shortwave infrared (VIS-NIR-SWIR, 400 – 2500 nm) spectral reflectance of 322 topsoil samples. The spectral reflectance was converted to continuum removal curves, and then, SCPs were extracted based on the curves. According to the results of the Second National Soil Survey of China, the samples were divided into 4 great groups or 8 genus, and a variety of stratification strategies were constructed based on great group (GR-S), genus (GE-S), spectral similarity (SS-S) and decision tree model (DT-S). A local random forest model was established to evaluate the performance of different stratification strategies and input variables. Our results are described as follows: (1) In different stratification strategies, the SOM prediction model based on DT-S exhibits the highest accuracy, followed by the SOM prediction models based on GE-S and SS-S; the SOM prediction model based on GR-S exhibits the lowest accuracy; (2) among the different input variables, the root mean squared error (RMSE) and coefficient of determination (R2) of the best SOM model predicted by SCPs are 5.18 g kg−1 and 0.89, respectively. Compared with the original reflectance based on the nonstratified strategy, the RMSE decreases by 4.88 g kg−1 and R2 increases by 0.32. The study results highlight the advantages of refining the soil hierarchy, which is helpful for identifying the differences in soils at the regional scale and analysing the relationship between stratification results and the characteristics of the soil environment to obtain a highly accurate prediction model. © 2022 Elsevier B.V.},
	author_keywords = {Decision tree; Density peak clustering; Soil hierarchical; Soil organic matter; Spectral characteristic parameters; Stratification strategy},
	keywords = {China; Biogeochemistry; Forecasting; Infrared devices; Infrared radiation; Mean square error; Organic compounds; Reflection; Soil surveys; Soils; Characteristics parameters; Clusterings; Density peak clustering; Model-based OPC; Prediction modelling; Soil hierarchical; Soil organic matters; Spectral characteristic parameter; Spectral characteristics; Stratification strategy; deciduous tree; forest management; soil classification; soil organic matter; spectral reflectance; stratification; topsoil; Decision trees},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Oyounalsoud2024,
	author = {Oyounalsoud, Mhamd Saifaldeen and Yilmaz, Abdullah Gokhan and Abdallah, Mohamed and Abdeljaber, Abdulrahman},
	title = {Drought prediction using artificial intelligence models based on climate data and soil moisture},
	year = {2024},
	journal = {Scientific Reports},
	volume = {14},
	number = {1},
	doi = {10.1038/s41598-024-70406-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201932754&doi=10.1038%2fs41598-024-70406-6&partnerID=40&md5=ad2345fd28ea1c37789687b671fce278},
	abstract = {Drought is deemed a major natural disaster that can lead to severe economic and social implications. Drought indices are utilized worldwide for drought management and monitoring. However, as a result of the inherent complexity of drought phenomena and hydroclimatic condition differences, no universal drought index is available for effectively monitoring drought across the world. Therefore, this study aimed to develop a new meteorological drought index to describe and forecast drought based on various artificial intelligence (AI) models: decision tree (DT), generalized linear model (GLM), support vector machine, artificial neural network, deep learning, and random forest. A comparative assessment was conducted between the developed AI-based indices and nine conventional drought indices based on their correlations with multiple drought indicators. Historical records of five drought indicators, namely runoff, along with deep, lower, root, and upper soil moisture, were utilized to evaluate the models’ performance. Different combinations of climatic datasets from Alice Springs, Australia, were utilized to develop and train the AI models. The results demonstrated that the rainfall anomaly drought index was the best conventional drought index, scoring the highest correlation (0.718) with the upper soil moisture. The highest correlation between the new and conventional indices was found between the DT-based index and the rainfall anomaly index at a value of 0.97, whereas the lowest correlation was 0.57 between the GLM and the Palmer drought severity index. The GLM-based index achieved the best performance according to its high correlations with conventional drought indicators, e.g., a correlation coefficient of 0.78 with the upper soil moisture. Overall, the developed AI-based drought indices outperformed the conventional indices, hence contributing effectively to more accurate drought forecasting and monitoring. The findings emphasized that AI can be a promising and reliable prediction approach for achieving better drought assessment and mitigation. © The Author(s) 2024.},
	author_keywords = {Drought indicators; Drought indices; Forecasting; Meteorological drought; Soft computing},
	keywords = {article; artificial intelligence; artificial neural network; climate; controlled study; correlation coefficient; decision tree; deep learning; drought; forecasting; intelligence; mitigation; natural disaster; plant root; prediction; rain; random forest; runoff; soft computing; soil moisture; support vector machine; systematic review},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Chen2024457,
	author = {Chen, Qi and Zhao, Hua and Cai, Tianjing and Gao, Yizhe and Gao, Ling and Liu, Qingjie},
	title = {Exploration of models of radiosensitive lipid metabolites of human plasma based on multiple machine learning algorithms; [基于多种机 器 学 习 算 法 的 人 血 浆 辐 射 敏 感脂质代谢物模型探索]},
	year = {2024},
	journal = {Chinese Journal of Radiological Medicine and Protection},
	volume = {44},
	number = {6},
	pages = {457 – 463},
	doi = {10.3760/cma.j.cn112271-20231228-00225},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198179655&doi=10.3760%2fcma.j.cn112271-20231228-00225&partnerID=40&md5=cab2bb6ed450edecf495c556bec4e077},
	abstract = {Objective To explore classification models for radiosensitive lipid metabolites in human peripheral blood by combining lipidomics with multiple machine learning (ML) algorithms. Methods Totally 97 peripheral blood samples were collected from 25 leukemia cases admitted to a general hospital in Beijing from March to September 2023 who were ready to undergo bone marrow transplantation, including 0 Gy blood samples before irradiation in the control group (n = 24), and 73 blood samples after irradiation at doses of 4, 8 and 12 Gy in the radiation group (n = 73), and the targeted lipidomic based on the ultra - high performance liquid chromatography-mass spectrometry (UHPLC-MS) platform method to analyze the differences of different lipids between control and radiation groups. Then, lipids responsive to radiation doses of 0 - 12 Gy were identified using linear regression. Finally, classification models were constructed using five ML algorithms based on the training set, followed by the validation and evaluation of these models using the validation set. Results Compared with the control group, the differences in the concentration changes of 62 lipids in 9 classes of lipid metabolites sensitive to radiation group were statistically significant (t = - 4. 91 to 4. 74, P < 0. 05), including sphingomyelins (SMs), cholesteryl esters(CEs), ceramides (Cers), phosphatidylinositols (PIs), hexosylceramides (HexCers), lysophosphatidylcholines (LysoPCs), phosphatidylcholines (PCOs), phosphatidylethanolamines (PEs), and lysophosphatidylethanolamines (LysoPEs) . Twenty lipids responsive to radiation doses of 0 - 12 Gy were identified, namely 11 SMs, 7 CEs, 1 Cer, and 1 PI. The five models based on ML algorithms of decision tree (DT), support vector machine (SVM), light gradient boosting machine (Light GBM), random forest (RF), and K-nearest neighbors (KNN) all exhibited high goodness of fit (F1 = 0. 69-1. 00) and high sensitivity. The evaluation and validation metrics revealed that the RF-based model yielded the optimal radiation classification discrimination (sensitivity: 1. 00; accuracy: 0. 72; F1 score: 0. 80) . Conclusions Lipid metabolites responsive to radiation and lipids responsive to radiation dose in human samples were identified using targeted lipidomics. The RF-based model can provide new ideas for exploring models of human radiosensitive lipid metabolites. © 2024 Chinese Medical Journals Publishing House Co.Ltd. All rights reserved.},
	author_keywords = {Ionizing radiation; Lipidomics; Machine learning; Random forest},
	keywords = {ceramide; cholesterol ester; lipid; lysophosphatidylcholine; phosphatidylcholine; phosphatidylethanolamine; phosphatidylinositol; sphingomyelin; Article; benchmarking; blood sampling; bone marrow transplantation; clinical article; comparative study; controlled study; decision tree; human; ionizing radiation; k nearest neighbor; learning algorithm; lipidomics; liquid chromatography-mass spectrometry; machine learning; metabolite; radiation dose; random forest; support vector machine; ultra performance liquid chromatography},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lai2021854,
	author = {Lai, Lu-Yao and Huang, Meng-Ping and Su, Song and Shu, Jian},
	title = {Liver Fibrosis Staging with Gadolinium Ethoxybenzyl Diethylenetriamine Penta-Acetic Acid-enhanced: A Systematic Review and Meta-analysis},
	year = {2021},
	journal = {Current medical imaging},
	volume = {17},
	number = {7},
	pages = {854 – 863},
	doi = {10.2174/1573405616666201130101229},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114753410&doi=10.2174%2f1573405616666201130101229&partnerID=40&md5=e8999914b05bb9a6d0161e07d72dee95},
	abstract = {OBJECTIVE: While liver biopsy is the golden standard for liver-fibrosis diagnosis, it is also invasive and has many limitations. Non-invasive techniques such as Magnetic Resonance Imaging (MRI) need to be further developed for liver fibrosis staging. This study aimed to evaluate the diagnostic accuracy of Gadolinium Ethoxybenzyl Diethylenetriamine Penta-acetic Acid (Gd-EOBDTPA)- enhanced MRI for liver fibrosis through systematic review and meta-analysis. METHODS: This study comprehensively searched relevant article in PubMed, Embase, and the Cochrane Library published from 2004 to 2018 to find studies analyzing the diagnostic accuracy of Gd-EOB-DTPA-enhanced MRI for liver fibrosis. Two reviewers independently screened the retrieved articles, extracted the required data from the included studies, and evaluated the methodological quality of the studies. The pooled sensitivity, specificity, positive likelihood ratio, negative likelihood ratio, diagnostic odds ratio, and Summary Receiver Operating Characteristics (SROC) curve were assessed. RESULTS: This study finally included 16 studies (n = 1,599) and selected a random-effects model based on the results of the I2 statistic to combine them. The areas under the SROC curve for the detection of F1 or greater, F2 or greater, F3 or greater, or F4 liver fibrosis were 0.8669, 0.8399, 0.8481, and 0.8858, respectively. CONCLUSION: Gd-EOB-DTPA-enhanced MRI showed a good diagnostic performance for staging liver fibrosis, especially for F4 liver fibrosis. Copyright© Bentham Science Publishers; For any queries, please email at epub@benthamscience.net.},
	author_keywords = {contrast media; gadolinium ethoxybenzyl DTPA.; liver cirrhosis; liver diseases; magnetic resonance imaging; Radiology},
	keywords = {Acetic Acid; Contrast Media; Gadolinium; Gadolinium DTPA; Humans; Liver Cirrhosis; Polyamines; Rubiaceae; Sensitivity and Specificity; acetic acid; contrast medium; diethylenetriamine; gadolinium; gadolinium pentetate; polyamine; human; liver cirrhosis; meta analysis; Rubiaceae; sensitivity and specificity},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Chen2022474,
	author = {Chen, Yu and Jupp, Julie R.},
	title = {Challenges to Asset Information Requirements Development Supporting Digital Twin Creation},
	year = {2022},
	journal = {IFIP Advances in Information and Communication Technology},
	volume = {639 IFIP},
	pages = {474 – 491},
	doi = {10.1007/978-3-030-94335-6_34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125238551&doi=10.1007%2f978-3-030-94335-6_34&partnerID=40&md5=f83514b8d2bb7165fee02792ace6493d},
	abstract = {The creation of a digital twin of rail infrastructure assets places greater emphasis on requirements engineering, model-based delivery methods, and digital information management to support the creation of both physical and virtual deliverables. However, requirements engineering capabilities are latent in comparison to complex discrete manufacturing. In this paper, we explore requirements engineering practices in Australian rail infrastructure projects creating digital twins for asset management and operations. An investigation of the challenges encountered by project teams during the development of asset information requirements for physical and digital deliverables was conducted using an in-depth literature review together with semi-structured interviews with rail project delivery teams. Challenges to the maturity of requirements engineering were categorised according to their main characteristics. The process, technology and supply chain issues identified provide empirical evidence of the pain points faced by delivery teams in developing asset information requirements in support of the creation of a digital twin. Findings serve as a starting point for further research into the development of requirements engineering methods distinguished by systems-based approaches. © 2022, IFIP International Federation for Information Processing.},
	author_keywords = {Asset management; Digital twin; Rail infrastructure; Requirements engineering; Systems engineering},
	keywords = {Information management; Requirements engineering; Supply chains; Assets management; Delivery methods; Digital information; Engineering modelling; Information requirement; Infrastructure assets; Model-based OPC; Rail infrastructure; Requirement development; Requirement engineering; Asset management},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Fuertes2021144395,
	author = {Fuertes, Juan José and Prada, Miguel Ángel and Rodríguez-Ossorio, José Ramón and González-Herbón, Raúl and Pérez, Daniel and Domínguez, Manuel},
	title = {Environment for Education on Industry 4.0},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {144395 – 144405},
	doi = {10.1109/ACCESS.2021.3120517},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117795683&doi=10.1109%2fACCESS.2021.3120517&partnerID=40&md5=140c8a362f4d044780afa3a270df7776},
	abstract = {A new industrial production model based on digitalization, system interconnection, virtualization and data exploitation, has emerged. Upgrade of production processes towards this Industry 4.0 model is one of the critical challenges for the industrial sector and, consequently, the training of students and professionals has to address these new demands. To carry out this task, it is essential to develop educational tools that allow students to interact with real equipment that implements, in an integrated way, new enabling technologies, such as connectivity with standard protocols, storage and data processing in the cloud, machine learning, digital twins and industrial cybersecurity measures. For that reason, in this work, we present an educational environment on Industry 4.0 that incorporates these technologies reproducing realistic industrial conditions. This environment includes cutting-edge industrial control system technologies, such as an industrial firewall and a virtual private network (VPN) to strengthen cybersecurity, an Industrial Internet of Things (IIoT) gateway to transfer process information to the cloud, where it can be stored and analyzed, and a digital twin that virtually reproduces the system. A set of hands-on tasks for an introductory automation course have been proposed, so that students acquire a practical understanding of the enabling technologies of Industry 4.0 and of its function in a real automation. This course has been taught in a master’s degree and students have assessed its usefulness by means of an anonymous survey. The results of the educational experience have been useful both from the students’ and faculty’s viewpoint. © 2021 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.},
	author_keywords = {cyber-physical system; Engineering education; IIoT; industrial Internet of Things; industry 4.0},
	keywords = {Computer system firewalls; Cyber Physical System; Cybersecurity; E-learning; Education computing; Embedded systems; Engineering education; Gateways (computer networks); Industry 4.0; Internet of things; Learning systems; Students; Teaching; Cloud-computing; Cybe-physical systems; Cyber security; Cyber-physical systems; Enabling technologies; Industrial internet of thing; Industrial production; Production models; Solid modelling; Digital storage},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23; All Open Access, Gold Open Access}
}

@ARTICLE{Han2024109,
	author = {Han, Bo and Li, Pei},
	title = {Apparel livestreaming sales forecasting models based on machine learning algorithms; [基于机器学习算法的服装直播销量预测模型]},
	year = {2024},
	journal = {Journal of Silk},
	volume = {61},
	number = {7},
	pages = {109 – 117},
	doi = {10.3969/j.issn.1001-7003.2024.07.012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199714883&doi=10.3969%2fj.issn.1001-7003.2024.07.012&partnerID=40&md5=8fdd1af7e69e6967ca7a53041f63485a},
	abstract = {With the dramatic increase in the scales of e-commerce livestreaming the number of e-commerce livestreaming users has reached 48. 8% of the overall Internet users in China. The huge supply demand requires live e-commerce stores to improve their dispatching efficiency and reduce inventory. Therefore in order to avoid retailers' profit loss it is necessary to find a more accurate method to predict livestreaming sales. The sales prediction methods mainly include traditional statistical methods and machine learning algorithms. Due to the instability of livestreaming sales and the large number of influencing factors traditional statistical methods often fail to predict the sales accurately. To complete the index system of livestreaming sales prediction and improve the accuracy of livestreaming sales prediction this paper adopted a variety of machine learning algorithms BP neural network decision tree DT random forest RF K-nearest neighbor KNN and support vector machine SVM analyzed the influencing factors of apparel livestreaming sales predicted apparel livestreaming sales and selected the best performing algorithms. The detailed research process is as follows. Firstly 17 influencing factors of livestreaming sales were selected through literature review and nine most important influencing factors were selected by using Spearman' s correlation coefficient combined with significance. Secondly different machine learning algorithms were used to establish clothing sales prediction models and the method of 5-fold cross-validation was adopted to initially screen out three algorithms RF KNN and SVM with high and stable model fit with R2 as an indicator. Finally the parameters of the three algorithms were optimized and then three prediction models were constructed. R2 MAE RMSE and MAPE were used as evaluation indexes and the optimal algorithms were selected by using the method of 5-fold cross-validation to test the performance of each model. The results of the study show that the multicollinearity between the nine factors number of fans of the anchor average number of viewers of the anchor in the last 30 days average pit output of the anchor in the last 30 days product price duration of product explanation historical sales of the product in the last 30 days number of fans of the brand historical sales of the brand in the last 30 days and discounts is weak and their correlation with the livestreaming sales is significant. Therefore these nine factors can be used as influencing factors in the prediction model. Among the influencing factors the correlation among product sales in the last 30 days the duration of product explanation and livestreaming sales is the highest. In the meanwhile the prediction algorithms KNN and RF perform better with R2 being greater than 0. 98 and MAPE within 30. 5% . Compared with the KNN algorithm the RF algorithm is more stable and its R2 RMSE and MAE perform better than those of the KNN algorithm. But the MAPE of the KNN algorithm is smaller than that of the RF algorithm for which the possible reason is that the KNN algorithm is more accurate in predicting low sales items and the relative error is smaller. According to the result of 5-fold cross-validation the RF algorithm is more stable compared with the KNN algorithm and the possible reason is that the KNN algorithm is more suitable for the dataset with more similar data features. Therefore RF can be used as the main prediction algorithm in practical applications to ensure the stability of the overall sales trend prediction. In predicting the sales of the same brand or the same category the similarity between the data is higher and then the KNN algorithm can be considered for prediction. This paper compares the performance of various prediction algorithms on livestreaming sales prediction optimizes the parameters and improves the accuracy of livestreaming sales prediction. The prediction results can help retailers make inventory planning adjust production schedules develop marketing strategies and provide data support for product purchasing pricing and promotion. Due to the fact that only some of the easily quantifiable influencing factors are explored in this paper and the sample distribution is limited future research can expand the scope of sample selection and further improve the predictive indicator system to achieve more accurate predictions. © 2024 China Silk Association. All rights reserved.},
	author_keywords = {5-fold cross-validation; K-nearest neighbors (KNN); live-streaming sales forecasting; machine learning random forest (RF); support vector machine (SVM)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Celik20242433,
	author = {Celik, Abdulkadir and Eltawil, Ahmed M.},
	title = {At the Dawn of Generative AI Era: A Tutorial-cum-Survey on New Frontiers in 6G Wireless Intelligence},
	year = {2024},
	journal = {IEEE Open Journal of the Communications Society},
	volume = {5},
	pages = {2433 – 2489},
	doi = {10.1109/OJCOMS.2024.3362271},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184802522&doi=10.1109%2fOJCOMS.2024.3362271&partnerID=40&md5=a8f7b9d1a6bd0e1d7c132783789f050a},
	abstract = {As we transition from the 5G epoch, a new horizon beckons with the advent of 6G, seeking a profound fusion with novel communication paradigms and emerging technological trends, bringing once-futuristic visions to life along with added technical intricacies. Although analytical models lay the foundations and offer systematic insights, we have recently witnessed a noticeable surge in research suggesting machine learning (ML) and artificial intelligence (AI) can efficiently deal with complex problems by complementing or replacing model-based approaches. The majority of data-driven wireless research leans heavily on discriminative AI (DAI) that requires vast real-world datasets. Unlike the DAI, Generative AI (GenAI) pertains to generative models (GMs) capable of discerning the underlying data distribution, patterns, and features of the input data. This makes GenAI a crucial asset in wireless domain wherein real-world data is often scarce, incomplete, costly to acquire, and hard to model or comprehend. With these appealing attributes, GenAI can replace or supplement DAI methods in various capacities. Accordingly, this combined tutorial-survey paper commences with preliminaries of 6G and wireless intelligence by outlining candidate 6G applications and services, presenting a taxonomy of state-of-the-art DAI models, exemplifying prominent DAI use cases, and elucidating the multifaceted ways through which GenAI enhances DAI. Subsequently, we present a tutorial on GMs by spotlighting seminal examples such as generative adversarial networks, variational autoencoders, flow-based GMs, diffusion-based GMs, generative transformers, large language models, autoregressive GMs, to name a few. Contrary to the prevailing belief that GenAI is a nascent trend, our exhaustive review of approximately 120 technical papers demonstrates the scope of research across core wireless research areas, including 1) physical layer design; 2) network optimization, organization, and management; 3) network traffic analytics; 4) cross-layer network security; and 5) localization & positioning. Furthermore, we outline the central role of GMs in pioneering areas of 6G network research, including semantic communications, integrated sensing and communications, THz communications, extremely large antenna arrays, near-field communications, digital twins, AI-generated content services, mobile edge computing and edge AI, adversarial ML, and trustworthy AI. Lastly, we shed light on the multifarious challenges ahead, suggesting potential strategies and promising remedies. Given its depth and breadth, we are confident that this tutorial-cum-survey will serve as a pivotal reference for researchers and professionals delving into this dynamic and promising domain.  © 2020 IEEE.},
	author_keywords = {5G; 6G; adversarial ML; AI-generated content; artificial intelligence (AI); autoregressive generative models; deep learning (DL); diffusion models; digital twins; discriminative AI; explainable AI; extremely large antenna arrays; generative adversarial networks; generative AI; generative models; generative pre-trained transformers; generative transformers; holographic beamforming; integrated sensing and communications; large language models; machine learning (ML); mMIMO; mmWave; near-field communication; network function virtualization; normalizing flows; open RAN; semantic communications; software defined networks; terahertz; trustworthy AI; trustworthy AI; variational autoencoders; zero-touch service management},
	keywords = {5G mobile communication systems; Beamforming; Deep learning; Near field communication; Network function virtualization; Semantics; Transfer functions; 5g; 6g; 6g mobile communication; Adversarial machine learning; Artificial intelligence; Artificial intelligence-generated content; Auto encoders; Auto-regressive; Autoregressive generative model; Communication system security; Deep learning; Diffusion model; Discriminative artificial intelligence; Explainable artificial intelligence; Extremely large antenna array; Generative artificial intelligence; Generative model; Generative pre-trained transformer; Generative transformer; Holographic beamforming; Integrated sensing; Integrated sensing and communication; Language model; Large antennas; Large language model; Machine learning; Machine-learning; Mm waves; MMIMO; Mobile communications; Near-field communication; Normalizing flow; Open RAN; Semantic communication; Service management; Software-defined networks; Tera Hertz; Trustworthy artificial intelligence; Variational autoencoder; Wireless communications; Zero-touch service management; Generative adversarial networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access}
}

@ARTICLE{Wang2021,
	author = {Wang, Yuqi and Wang, Liangxu and Sun, Yanli and Wu, Miao and Ma, Yingjie and Yang, Lingping and Meng, Chun and Zhong, Li and Hossain, Mohammad Arman and Peng, Bin},
	title = {Prediction model for the risk of osteoporosis incorporating factors of disease history and living habits in physical examination of population in Chongqing, Southwest China: based on artificial neural network},
	year = {2021},
	journal = {BMC Public Health},
	volume = {21},
	number = {1},
	doi = {10.1186/s12889-021-11002-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106925411&doi=10.1186%2fs12889-021-11002-5&partnerID=40&md5=05a65ccc01f6953b0a273db6801bbde8},
	abstract = {Background: Osteoporosis is a gradually recognized health problem with risks related to disease history and living habits. This study aims to establish the optimal prediction model by comparing the performance of four prediction models that incorporated disease history and living habits in predicting the risk of Osteoporosis in Chongqing adults. Methods: We conduct a cross-sectional survey with convenience sampling in this study. We use a questionnaire From January 2019 to December 2019 to collect data on disease history and adults’ living habits who got dual-energy X-ray absorptiometry. We established the prediction models of osteoporosis in three steps. Firstly, we performed feature selection to identify risk factors related to osteoporosis. Secondly, the qualified participants were randomly divided into a training set and a test set in the ratio of 7:3. Then the prediction models of osteoporosis were established based on Artificial Neural Network (ANN), Deep Belief Network (DBN), Support Vector Machine (SVM) and combinatorial heuristic method (Genetic Algorithm - Decision Tree (GA-DT)). Finally, we compared the prediction models’ performance through accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC) to select the optimal prediction model. Results: The univariate logistic model found that taking calcium tablet (odds ratio [OR] = 0.431), SBP (OR = 1.010), fracture (OR = 1.796), coronary heart disease (OR = 4.299), drinking alcohol (OR = 1.835), physical exercise (OR = 0.747) and other factors were related to the risk of osteoporosis. The AUCs of the training set and test set of the prediction models based on ANN, DBN, SVM and GA-DT were 0.901, 0.762; 0.622, 0.618; 0.698, 0.627; 0.744, 0.724, respectively. After evaluating four prediction models’ performance, we selected a three-layer back propagation neural network (BPNN) with 18, 4, and 1 neuron in the input layer, hidden and output layers respectively, as the optimal prediction model. When the probability was greater than 0.330, osteoporosis would occur. Conclusions: Compared with DBN, SVM and GA-DT, the established ANN model had the best prediction ability and can be used to predict the risk of osteoporosis in physical examination of the Chongqing population. The model needs to be further improved through large sample research. © 2021, The Author(s).},
	author_keywords = {Artificial neural network; Disease history; Living habits; Osteoporosis; Physical examination; Prediction model},
	keywords = {Adult; China; Cross-Sectional Studies; Habits; Humans; Neural Networks, Computer; Osteoporosis; Physical Examination; adult; China; controlled study; cross-sectional study; habit; human; osteoporosis; physical examination; randomized controlled trial},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Jiyad2022640,
	author = {Jiyad, Zainab and Plasmeijer, Elsemieke I. and Keegan, Samantha and Samarasinghe, Venura and Green, Adele C. and Akhras, Victoria},
	title = {Defining the Validity of Skin Self-Examination as a Screening Test for the Detection of Suspicious Pigmented Lesions: A Meta-Analysis of Diagnostic Test Accuracy},
	year = {2022},
	journal = {Dermatology},
	volume = {238},
	number = {4},
	pages = {640 – 648},
	doi = {10.1159/000520592},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123508856&doi=10.1159%2f000520592&partnerID=40&md5=fad4deb5cd8f8e2f3597f6b544fc2756},
	abstract = {Background: Skin self-examination (SSE) is widely promoted for the detection of suspicious pigmented lesions. However, determining screening accuracy is essential to appraising the usefulness of SSE. Objectives: The aim of this work was to pool estimates from studies of SSE diagnostic accuracy in the detection of suspicious pigmented lesions. Methods: This study was registered with PROSPERO (CRD42021246356) and conducted in accordance with PRISMA-DTA guidelines. A systematic search of Medline (PubMed) EMBASE, CINAHL, and The Cochrane Library was conducted to identify relevant studies. We included studies that examined the accuracy of SSE, either whole-body or site-specific, for detecting change in individual pigmented lesions or detecting an atypical naevus. A univariate random-effects model, based on logit-transformed data, was used to calculate a summary diagnostic odds ratio (DOR) as well as pooled sensitivity and specificity. Cochran's Q test and the I2 statistic were calculated to assess heterogeneity. A proportional hazards model was used to calculate the area under the curve (AUC) and plot the summary receiver operator characteristic curve. We used the Quality Assessment of Diagnostic Accuracy Studies-2 tool to grade study quality. Results: We identified 757 studies, of which 3 met inclusion criteria for quantitative synthesis. The pooled sensitivity and specificity based on 553 included participants was 59 and 82%, respectively. The summary DOR was 5.88 and the AUC was 0.71. There were some concerns regarding risk of bias in all 3 studies. Conclusions: SSE can detect suspicious pigmented lesions with reasonable sensitivity and relatively high specificity, with the AUC suggesting acceptable discriminatory ability.  © 2022 S. Karger AG, Basel. Copyright: All rights reserved.},
	author_keywords = {Diagnostic accuracy; Melanoma; Sensitivity; Skin self-examination; Specificity},
	keywords = {Area Under Curve; Diagnostic Tests, Routine; Humans; Self-Examination; Sensitivity and Specificity; Skin Neoplasms; adult; Article; cutaneous melanoma; diagnostic accuracy; diagnostic test accuracy study; dysplastic nevus; female; human; male; meta analysis; nevus; practice guideline; Quality Assessment of Diagnostic Accuracy Studies; screening test; self examination; sensitivity and specificity; skin examination; systematic review; validity; area under the curve; diagnostic test; pathology; self examination; skin tumor},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Green Open Access}
}

@ARTICLE{Arrighi2019,
	author = {Arrighi, Chiara and Campo, Lorenzo},
	title = {Effects of digital terrain model uncertainties on high-resolution urban flood damage assessment},
	year = {2019},
	journal = {Journal of Flood Risk Management},
	volume = {12},
	doi = {10.1111/jfr3.12530},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064528559&doi=10.1111%2fjfr3.12530&partnerID=40&md5=29e26c1b41dff63af0bb718c891f7644},
	abstract = {This work investigates the impact of high-resolution digital terrain model (DTM) uncertainties on the estimation of urban flood losses. Starting from a Light Detection And Ranging (LiDAR)-derived DTM of an urban area, four digital terrain representations (raw data, building footprints filled, buildings as waterproof blocks, and different elevation data merged) are used to generate a computational mesh to run a 2D flood model for three inundation scenarios, differing in flood volumes. The most detailed DTM is obtained by merging the DTM with elevation points based on a two-step optimal interpolation algorithm. A flood damage model based on stage-damage curves is used to estimate monetary losses to structures at the building scale. Flood maps and flood losses are then compared for each terrain representation. The application of the method to an Italian urban district shows that (a) a significant mismatch between manually surveyed elevation points and DTM can be observed, (b) different sources of elevation data can be merged to obtain an optimal representation of the terrain, (c) in dense urban settlements, important differences in flood extent and losses (up to 180%) occur depending on terrain representation. Considerations on time effort required by the increasing detail of the DTM and on the transferability of the results are presented. © 2019 The Chartered Institution of Water and Environmental Management (CIWEM) and John Wiley & Sons Ltd},
	author_keywords = {DTM; flood risk; interpolation methods; LiDAR; sensitivity},
	keywords = {algorithm; digital terrain model; flood damage; hydrological modeling; interpolation; optimization; risk assessment; urban region},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 34}
}

@CONFERENCE{Perno2020959,
	author = {Perno, M. and Hvam, L. and Haug, A.},
	title = {Enablers and barriers to the implementation of digital twins in the process industry: A systematic literature review},
	year = {2020},
	journal = {IEEE International Conference on Industrial Engineering and Engineering Management},
	volume = {2020-December},
	pages = {959 – 964},
	doi = {10.1109/IEEM45057.2020.9309745},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099750826&doi=10.1109%2fIEEM45057.2020.9309745&partnerID=40&md5=d762e9cfb15bacefa8cecf01231d9949},
	abstract = {Since its first introduction in 2002, the interest in the concept of "Digital Twins"has grown exponentially among researchers and industry practitioners. An increasing number of Digital Twin implementations are made in many industries. Given the novelty of the concept, companies from any industry type face significant challenges when implementing Digital Twins. Furthermore, only little research has been conducted in the process industry, which may be explained by the high complexity of representing and modeling the physics behind the production processes in an accurate manner. This study aims at filling this gap by providing a clear categorization of the main barriers that process companies face when implementing Digital Twins of their assets, as well as the key enabling factors and technologies that can be leveraged to overcome such challenges. Furthermore, a model based on the findings from the literature study is proposed. The results indicate a dearth in the literature focused on the process industry, therefore, key learnings from other industry sectors are gathered, and suggestions for further research are proposed.  © 2020 IEEE.},
	author_keywords = {Barrier; Digital Twin; Enabler; Literature Review; Process Industry; Simulation},
	keywords = {Engineering; Industrial engineering; High complexity; Industry sectors; Literature studies; Model-based OPC; Process industries; Production process; Systematic literature review; Digital twin},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19}
}

@ARTICLE{Yang2022,
	author = {Yang, Jaemin and Kim, Jinmo and Farshadmanesh, Pegah and Sakurahara, Tatsuya and Reihani, Seyed and Blake, Cathy and Mohaghegh, Zahra},
	title = {Uncertainty analysis on support vector machine for measuring organizational factors in probabilistic risk assessment of nuclear power plants},
	year = {2022},
	journal = {Progress in Nuclear Energy},
	volume = {153},
	doi = {10.1016/j.pnucene.2022.104411},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138793252&doi=10.1016%2fj.pnucene.2022.104411&partnerID=40&md5=24b3f457fff488b6bbffa984f7e83bd7},
	abstract = {This paper is a product of a line of research by the authors to explicitly incorporate organizational factors into the probabilistic risk assessment (PRA) of complex socio-technical systems. This explicit incorporation helps (i) assess the system risk due to organizational and managerial weaknesses, (ii) identify the critical organizational root causes of failure scenarios, aiding in effective corrective action, and (iii) avoid the potential of underestimating the system risk involving human error and organizational factors. To facilitate the measurement of organizational factors contributing to the PRA scenarios, the previous studies by the authors developed the Data-Theoretic methodology, where “data analytics” are guided by a “theory” of underlying causation to prevent misleading results from solely data-oriented approaches. The Data-Theoretic methodology consists of two submodules: (a) DT-BASE that develops a detailed causal model based on a theory-building process and is equipped with a baseline quantification utilizing analyst interpretation of information extracted from relevant references; and (b) DT-SITE that conducts data analytics (text mining using machine learning) to quantify the organizational causal elements based on industry event databases. This article investigates uncertainty analysis for the uncertainties associated with machine learning-based data mining in DT-SITE using a Support Vector Machine (SVM) classifier for industry event databases. This article conducts a literature review and proposes a categorization scheme of the SVM uncertainties to establish a theoretical foundation for identifying uncertainty sources associated with the SVM classifier. The implementation of uncertainty analysis for DT-SITE is then illustrated using a nuclear power plant case study. The potential uncertainty sources related to the SVM classifier used in DT-SITE are identified using the categorization scheme developed based on the literature review. The uncertainty analysis procedure for the SVM classifier in DT-SITE is proposed, and the linkage to the current uncertainty analysis process in PRA is discussed. The proposed uncertainty analysis procedure for the SVM classifier in DT-SITE is applied to the illustrative case study to assess the impact of one of the potential uncertainty sources (i.e., the selection of document sections included in the dataset) on the risk-informed decision-making. © 2022 Elsevier Ltd},
	author_keywords = {Machine learning; Organizational factors; Probabilistic risk assessment; Support vector machine; Uncertainty analysis},
	keywords = {Classification (of information); Data Analytics; Data mining; Learning algorithms; Nuclear energy; Nuclear fuels; Nuclear power plants; Risk assessment; Risk perception; Support vector machines; Data analytics; Machine-learning; Organisational; Organizational factors; Probabilistic Risk Assessment; Support vector machine classifiers; Support vectors machine; System risk; Uncertainty; Uncertainty sources; Uncertainty analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Bronze Open Access}
}

@ARTICLE{Hänel2021,
	author = {Hänel, Albrecht and Seidel, André and Frieß, Uwe and Teicher, Uwe and Wiemer, Hajo and Wang, Dongqian and Wenkler, Eric and Penter, Lars and Hellmich, Arvid and Ihlenfeldt, Steffen},
	title = {Digital twins for high-tech machining applications—a model-based analytics-ready approach},
	year = {2021},
	journal = {Journal of Manufacturing and Materials Processing},
	volume = {5},
	number = {3},
	doi = {10.3390/jmmp5030080},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111928107&doi=10.3390%2fjmmp5030080&partnerID=40&md5=50d04bbc839a8b01a22dfd4dee956521},
	abstract = {This paper presents a brief introduction to competition-driven digital transformation in the machining sector. On this basis, the creation of a digital twin for machining processes is approached firstly using a basic digital twin structure. The latter is sub-grouped into information and data models, specific calculation and process models, all seen from an application-oriented perspective. Moreover, digital shadow and digital twin are embedded in this framework, being discussed in the context of a state-of-the-art literature review. The main part of this paper addresses models for machine and path inaccuracies, material removal and tool engagement, cutting force, process stability, thermal behavior, workpiece and surface properties. Furthermore, these models are superimposed towards an integral digital twin. In addition, the overall context is expanded towards an integral software architecture of a digital twin providing information system. The information system, in turn, ties in with existing forward-oriented planning from operational practice, leading to a significant expansion of the initially presented basic structure for a digital twin. Consequently, a time-stratified data layer platform is introduced to prepare for the resulting shadow-twin transformation loop. Finally, subtasks are defined to assure functional interfaces, model integrability and feedback measures. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Cyber-physical production; Digital manufacturing system; Digital shadow; Digital twin; Digital twin integration; Information model; Machining processes},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 34; All Open Access, Gold Open Access}
}

@CONFERENCE{Marouf2019,
	author = {Marouf, Ahmed Al and Hossian, Rafayet},
	title = {Lyricist Identification using Stylometric Features utilizing BanglaMusicStylo Dataset},
	year = {2019},
	journal = {2019 International Conference on Bangla Speech and Language Processing, ICBSLP 2019},
	doi = {10.1109/ICBSLP47725.2019.201534},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084978601&doi=10.1109%2fICBSLP47725.2019.201534&partnerID=40&md5=d878cda8e56eaac952351d5990eb232a},
	abstract = {This paper presents a profile-based approach utilizing supervised learning methods to identify the lyricist of Bangla songs written by two legendary poets novelist Kazi Nazrul Islam and Rabindranath Tagore. The problem statement for this paper could be considered as authorship attribution using stylometric features on Bangla lyrics. We have utilized the BanglaMusicStylo dataset, which consists of 856 and 620 songs of Rabindranath Tagore and Kazi Nazrul Islam, respectively. The traditional authorship attribution works found in the literature are based on the novels written by the authors, not Bangla song lyrics. Using the Bangla song lyrics made it a challenging task, as the word choices made by the authors in songs depends on the rhythms, completeness, situation and many more. In this paper, we have tried to fusion different types of stylometric features, such as lexical, structural, stylistic etc. For experimentation, we have designed the prediction model based on supervised learning exploiting Naïve Bayes (NB), Simple Logistic Regression (SLR), Decision Tree (DT), Support Vector Machine (SVM), and Multilayer Perceptron (MLP). The experimental model consists of several steps including data pre-processing, feature extraction, data processing, and classification model. After performance evaluation, we have got approximately 86.29% accuracy from SLR, which is quite satisfactory. © 2019 IEEE.},
	author_keywords = {Authorship Attribution; BanglaMusicStylo Dataset; Linguistic Feature; Stylometric Features; Supervised Learning},
	keywords = {Data handling; Decision trees; Logistic regression; Support vector machines; Support vector regression; Authorship attribution; Classification models; Data preprocessing; Experimental modeling; Multi layer perceptron; Simple logistic regressions; Stylometric features; Supervised learning methods; Learning systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Levine2022,
	author = {Levine, Nathaniel M. and Spencer, Billie F.},
	title = {Post-Earthquake Building Evaluation Using UAVs: A BIM-Based Digital Twin Framework},
	year = {2022},
	journal = {Sensors},
	volume = {22},
	number = {3},
	doi = {10.3390/s22030873},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123306163&doi=10.3390%2fs22030873&partnerID=40&md5=39073e7ae5f57461be62bbeb66829e10},
	abstract = {Computer vision has shown potential for assisting post-earthquake inspection of buildings through automatic damage detection in images. However, assessing the safety of an earthquake-damaged building requires considering this damage in the context of its global impact on the structural system. Thus, an inspection must consider the expected damage progression of the associated component and the component’s contribution to structural system performance. To address this issue, a digital twin framework is proposed for post-earthquake building evaluation that integrates unmanned aerial vehicle (UAV) imagery, component identification, and damage evaluation using a Building Information Model (BIM) as a reference platform. The BIM guides selection of optimal sets of images for each building component. Then, if damage is identified, each image pixel is assigned to a specific BIM component, using a GrabCut-based segmentation method. In addition, 3D point cloud change detection is employed to identify nonstructural damage and associate that damage with specific BIM components. Two example applications are presented. The first develops a digital twin for an existing reinforced concrete moment frame building and demonstrates BIM-guided image selection and component identification. The second uses a synthetic graphics environment to demonstrate 3D point cloud change detection for identifying damaged nonstructural masonry walls. In both examples, observed damage is tied to BIM components, enabling damage to be considered in the context of each component’s known design and expected earthquake performance. The goal of this framework is to combine component-wise damage estimates with a pre-earthquake structural analysis of the building to predict a building’s post-earthquake safety based on an external UAV survey. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Automated inspection; Building information modeling; Computer vision; Digital twin; Post-earthquake evaluation; Unmanned aerial vehicles},
	keywords = {Antennas; Architectural design; Computer vision; Damage detection; Earthquakes; Image segmentation; Reinforced concrete; Walls (structural partitions); 3D point cloud; Automated inspection; Building evaluations; Building Information Modelling; Change detection; Component identification; Model-based OPC; Modeling component; Post-earthquake evaluation; Structural systems; Unmanned aerial vehicles (UAV)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 54; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Raitviir2024,
	author = {Raitviir, Christopher-Robin and Lill, Irene},
	title = {Conceptual Framework of Information Flow Synchronization Throughout the Building Lifecycle},
	year = {2024},
	journal = {Buildings},
	volume = {14},
	number = {7},
	doi = {10.3390/buildings14072207},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199596287&doi=10.3390%2fbuildings14072207&partnerID=40&md5=11e5863e233d6f779f43935ccfabd411},
	abstract = {The construction industry’s reliance on traditional methods and fragmented workflows results in significant information loss, inefficiencies, increased costs, and errors. This study addresses these issues by integrating comprehensive urban planning with building information modeling (BIM) to create a seamless information flow throughout the building lifecycle. We propose a holistic framework that synchronizes data from planning to demolition, incorporating national and municipal digital twins. An imperative literature review and analysis of international best practices were conducted to develop a conceptual framework aimed at improving data accuracy and interoperability. Our findings underscore the importance of adopting open standards such as Industry Foundation Classes (IFC) and CityGML for effective information exchange. By implementing an information model (IM)-based approach in urban planning and public sector permit processes, project timelines can be streamlined, and regulatory compliance enhanced. This study concludes that continuous, integrated information flow facilitates more efficient, cost-effective construction practices and improved decision-making. Furthermore, this research illustrates the potential of digital twin technology to revolutionize the construction industry by enabling real-time data integration and fostering stakeholder collaboration, ultimately offering a robust framework for practitioners, and significantly enhancing the efficiency and accuracy of construction processes. © 2024 by the authors.},
	author_keywords = {building information modeling; building lifecycle data; building permits; city information model; digital twins; GeoBIM; information model-based urban planning},
	keywords = {Architectural design; Construction industry; Cost effectiveness; Data integration; Decision making; Information theory; Life cycle; Urban planning; Building Information Modelling; Building life cycle; Building lifecycle data; Building permits; City information model; GeoBIM; Information model-based urban planning; Information Modeling; Model-based OPC; Regulatory compliance},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@CONFERENCE{Schabany2023946,
	author = {Schabany, Dariush and Hülsmann, Tom-Hendrik and Schmetz, Arno},
	title = {Development of a Maturity Assessment Model for Digital Twins in Battery Cell Industry},
	year = {2023},
	journal = {Procedia CIRP},
	volume = {120},
	pages = {946 – 951},
	doi = {10.1016/j.procir.2023.09.105},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184595832&doi=10.1016%2fj.procir.2023.09.105&partnerID=40&md5=3ad0e27f5854fbde5d6573a76cfa1bbf},
	abstract = {Due to the increasing demand for batteries, fueled by the transition to sustainable energy sources and electric vehicles, battery factories are currently being built all around the world. To meet the resulting challenges, digital technologies are a key component in the context of competitive battery cell manufacturing. For a requirements-oriented implementation of an appropriate Industry 4.0 environment, three Digital Twins are implemented to meet the digital needs in the battery cell manufacturing environment: The Product Twin, the Plant Twin, and the Building Twin. Addressing the challenges of battery cell manufacturing requires holistic Digital Twins that encompass the entire lifecycle. Existing Maturity Models do not provide a sufficient basis for a systematic assessment and holistic development of the Product-, the Plant-, and the Building Twin in battery cell manufacturing. Therefore, a novel Maturity Model considering the specific requirements has been developed. The Maturity Model is developed using an action model based on the Design Science Research methodology. The content components are carried out with qualitative empiricism - literature review, expert workshops, expert interviews - within five iterations. This is followed by expert evaluation of the model using a case study. The result of the empiricism is a holistic Maturity Assessment Model for Digital Twins in Battery Cell Industry. The Maturity Model includes five maturity levels, twelve maturity dimensions, 50 maturity sub-dimensions and 250 maturity indicators. Fuzzy Logic is used to perform the quantitative maturity assessments of the Digital Twins by means of the maturity indicators. The evaluation results show that the model can offer consistent and reproducible maturity assessments. Once the model has been tested and verified with further case studies it is plausible that the Maturity Model can be applied to a wide range of Digital Twins in the manufacturing industry. © 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of the 56th CIRP International Conference on Manufacturing Systems 2023.},
	author_keywords = {Battery Cell Manufacturing; Digital Twins; Fuzzy Logic; Industry 4.0; Maturity Models},
	keywords = {Computer circuits; Industry 4.0; Life cycle; Secondary batteries; Assessment models; Battery cell manufacturing; Battery cells; Case-studies; Cell manufacturing; Electric vehicle batteries; Fuzzy-Logic; Maturity assessments; Maturity model; Sustainable energy sources; Fuzzy logic},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Li2024,
	author = {Li, Wenjia and Guo, Jingyu and Liu, Wanting and Tu, Jason and Tang, Qinghe},
	title = {Effect of older adults willingness on telemedicine usage: an integrated approach based on technology acceptance and decomposed theory of planned behavior model},
	year = {2024},
	journal = {BMC Geriatrics},
	volume = {24},
	number = {1},
	doi = {10.1186/s12877-024-05361-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204287928&doi=10.1186%2fs12877-024-05361-y&partnerID=40&md5=4a8bbc2c5fc83294b82b53aaece1d8fa},
	abstract = {Background: Telemedicine, as a novel method of health management system, has demonstrated to have a significant impact on health levels. However, a challenge persists in the form of low usage rates and acceptance among older adults in China. There are accumulating evidence that willingness will affect the telemedicine usage among older adults. This study investigates factors influencing older users’ trust in adopting telemedicine technology, thereby promoting actual use. Methods: A questionnaire survey was conducted with 400 urban seniors aged 60 and above. Drawing from the Technology Acceptance Model (TAM) and the Decomposed Theory of Planned Behavior (DTPB), the author combines elements such as Perceived Usefulness, Perceived Ease of Use, Subjective Norms, Service Environment, Self-Efficacy, Behavioral Intention to Use, and Usage Behavior. The aim is to explore the interrelationships between these factors. Results: Perceived Usefulness (PU) and Service Environment (SE) significantly and positively impact Behavioral Intention (BI) to use telemedicine, with Trust (TR) identified as a crucial mediating variable. Enhancing trust can substantially increase older adults’ intention to use telemedicine services. Furthermore, the study reveals a significant relationship between older adults’ trust in telemedicine and factors such as Perceived Usefulness (PU), Service Environment (SE), Subjective Norms (SR), as well as Emotional Risk (ER) and Cost Risk (CR), the latter two tending to decrease Trust(TR). Conclusions: This paper constructs and validates a combined model based on TAM and DTPB, comprehensively exploring the potential factors influencing the older adults’ intention to use telemedicine. The findings suggest that telemedicine services for older adults should prioritize improving user perception and enhancing trust throughout the service process to effectively increase their willingness to use these services. © The Author(s) 2024.},
	author_keywords = {Older adults; Telemedicine; Trust; Willingness to use},
	keywords = {Aged; Aged, 80 and over; Attitude to Computers; Female; Humans; Male; Middle Aged; Patient Acceptance of Health Care; Surveys and Questionnaires; Telemedicine; Theory of Planned Behavior; adult; aged; article; female; human; male; questionnaire; self concept; telemedicine; Theory of Planned Behavior; attitude to computers; middle aged; patient attitude; psychology; Theory of Planned Behavior; very elderly},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Fanos2020,
	author = {Fanos, Ali Mutar and Pradhan, Biswajeet and Alamri, Abdullah and Lee, Chang-Wook},
	title = {Machine learning-based and 3d kinematic models for rockfall hazard assessment using LiDAR Data and GIS},
	year = {2020},
	journal = {Remote Sensing},
	volume = {12},
	number = {11},
	doi = {10.3390/rs12111755},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086446659&doi=10.3390%2frs12111755&partnerID=40&md5=3f1dc70ac3640b8c32678d5746e200ab},
	abstract = {Rockfall is one of the most hazardous phenomena in mountainous and hilly regions with high and steep terrain. Such incidents can cause massive damage to people, properties, and infrastructure. Therefore, proper rockfall hazard assessment methods are required to save lives and provide a guide for the development of an area. The aim of this research is to develop a method for rockfall hazard assessment at two different scales (regional and local). A high-resolution airborne laser scanning (ALS) technique was utilized to derive an accurate digital terrain model (DTM); next, a terrestrial laser scanner (TLS) was used to capture the topography of the two most critical areas within the study area. A staking machine-learning model based on different classifiers, namely logistic regression (LR), random forest (RF), artificial neural network (ANN), support vector machine (SVM), and k-nearest neighbor (KNN), was optimized and employed to determine rockfall probability by utilizing various rockfall conditioning factors. A developed 3D rockfall kinematic model was used to obtain rockfall trajectories, velocity, frequency, bouncing height, kinetic energy, and impact location. Next, a spatial model combined with a fuzzy analytical hierarchy process (fuzzy-AHP) integrated in the Geographic Information System (GIS) was developed to assess rockfall hazard in two different areas in Ipoh, Malaysia. Additionally, mitigation processes were suggested and assessed to provide a comprehensive information for urban planning management. The results show that, the stacking random forest-k-nearest neighbor (RF-KNN) model is the best hybrid model compared to other tested models with an accuracy of 89%, 86%, and 87% based on training, validation, and cross-validation datasets, respectively. The three-dimension rockfall kinematic model was calibrated with an accuracy of 93% and 95% for the two study areas and subsequently the rockfall trajectories and their characteristics were derived. The assessment of the suggested mitigation processes proves that the proposed methods can reduce or eliminate rockfall hazard in these areas. According to the results, the proposed method can be generalized and applied in other geographical places to provide decision-makers with a comprehensive rockfall hazard assessment. © 2020 by the authors.},
	author_keywords = {3D kinematic modeling; GIS model; LiDAR; Machine learning; Rockfall hazard},
	keywords = {Decision making; Decision trees; Geographic information systems; Kinematics; Kinetic energy; Kinetics; Laser applications; Logistic regression; Motion compensation; Nearest neighbor search; Neural networks; Random forests; Rock bursts; Support vector machines; Support vector regression; Surveying instruments; Temperature control; Topography; Airborne Laser scanning; Comprehensive information; Digital terrain model; Fuzzy analytical hierarchy process; K nearest neighbor (KNN); Machine learning models; Terrestrial laser scanners; Urban planning managements; Learning systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28; All Open Access, Gold Open Access}
}

@ARTICLE{Milošević202295,
	author = {Milošević, Marko D. G. and Pålsson, Björn A. and Nissen, Arne and Nielsen, Jens C. O. and Johansson, Håkan},
	title = {Demonstration of a Digital Twin Framework for Model-Based Operational Condition Monitoring of Crossing Panels},
	year = {2022},
	journal = {Lecture Notes in Mechanical Engineering},
	pages = {95 – 105},
	doi = {10.1007/978-3-031-07305-2_11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136939837&doi=10.1007%2f978-3-031-07305-2_11&partnerID=40&md5=d1eeffd4fb59b2dea23875c68ba5a732},
	abstract = {The wheel transition area in railway crossings is subjected to impact loads that cause an accumulation of structural degradation in crossing panels over time. This degradation leads to high maintenance costs and possibly traffic disturbances. There is therefore a demand from infrastructure managers to monitor the condition and predict maintenance needs for these assets without the need for regular on-site inspections. One solution for operational condition monitoring is to observe the structural response of the crossing under traffic loading via embedded accelerometers. From these measurements, relative changes in track dynamics over time can be observed. To derive a condition or predict maintenance needs, however, these measured accelerations need to be related to the status of the asset. A framework for this where measurement data, simulation models and maintenance history are combined to build an online model that can assess the status and predict future maintenance needs for a material asset is often called a Digital Twin. This paper will present a Digital Twin framework that uses measured accelerations, climate data, scanned running surface geometry and a multi-body simulation (MBS) model to estimate the status and degradation rate of crossing panels. Method developments for this framework are demonstrated for two in situ crossings. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author_keywords = {Condition monitoring; Crossing panel; Digital Twin demonstrator; Multi-body simulations},
	keywords = {Climate models; Condition based maintenance; Degradation; Forecasting; Traffic surveys; Condition; Crossing panel; Digital twin demonstrator; Impact loads; Maintenance cost; Model-based OPC; Multibody simulations; Operational conditions; Simulation model; Structural degradation; Condition monitoring},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Pascual2023,
	author = {Pascual, Adrián and Guerra-Hernandez, Juan},
	title = {Correction of phenology-induced effects in forest canopy height models based on airborne laser scanning data. Insights from the deciduous mountain forests in Picos de Europa National Park in Spain},
	year = {2023},
	journal = {Ecological Informatics},
	volume = {75},
	doi = {10.1016/j.ecoinf.2023.102092},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151647797&doi=10.1016%2fj.ecoinf.2023.102092&partnerID=40&md5=e8899829149ada82b86ccbe3d2b0d87e},
	abstract = {The comparison of ALS time-series is an important element of modern landscape conservation planning, especially to monitor forest ecosystems. Modellers must evaluate phenology when comparing ALS-based maps of ground elevation or canopy height between years. We showcase the scenario using a National Park in the North-West of Spain where bi-temporal ALS has been used to map deciduous mountain forests. We compare the 2010 and 2021 surveys using the same algorithms to interpret ALS data and to generate actionable products for managers, mainly digital terrain models and canopy height models. We implemented a hybrid approach to correct the discrepancies between the surveys, showing the problems arising from differences in phenology or the selection of the scaling. We quantified around 5% of the area suffered from >1 m difference in DTM and higher impact on CHM values. With the hybrid method, modellers can highly reduce the uncertainty when comparing two ALS surveys and derivable products. We provided a solid graphical and analytical diagnosis of these emerging problems in the context of multi-temporal ALS surveys testing the hybrid approach at two resolutions: 1 and 2 m, fine-grained scales. The assessment of phenology-induced effects is important under the context of nationwide ALS survey programmes currently in operation and on high-demand. Finally, we discuss and frame the hybrid-approach as a well-suited vector of canopy gap detection methods to support conservation planning and enforce species-specific habitat improvements. © 2023 Elsevier B.V.},
	author_keywords = {Forest inventory; Forest surveying; Lidar; Remote sensing; Temperate forests},
	keywords = {Cantabria; Cantabrian Mountains; Picos de Europa; Spain; algorithm; canopy gap; conservation planning; ecological modeling; forest canopy; forest inventory; lidar; phenology; remote sensing; surveying; temperate forest},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Schlör2021145,
	author = {Schlör, Holger and Fischer, Wiltrud and Venghaus, Sandra},
	title = {The German capability index—An operationalization of sen’s capability approach},
	year = {2021},
	journal = {Environmental Footprints and Eco-Design of Products and Processes},
	pages = {145 – 189},
	doi = {10.1007/978-981-16-0239-9_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105475108&doi=10.1007%2f978-981-16-0239-9_6&partnerID=40&md5=24a54eddedc5fce1434cf06125a744c5},
	abstract = {When reflecting on the global call for sustainable development (Gutteres in Davos speech 2019, January 24. World Economic Forum, Davos, 2019 [29]), a question posed by Amartya Sen actuates further thought—namely that for who has to be sustained (Anand and Sen in World Dev 28:2029–2049, 2000a [4])? This question was answered by himself, proposing “it is not so much that humanity is trying to sustain the natural world but rather that humanity is trying to sustain itself (Sen in J Hum Dev Capabil 14:6–20, 2013 [66])”. Against this background, in our research we addressed this question of who has to be sustained using Sen’s capability approach for the case study of Germany. For this purpose, the German household survey (Einkommens- und Verbrauchsstichprobe (EVS)) of the German Federal Statistical Office is used to analyse five social household groups (all households, single households, single parents, couples without children, couples with children) according to their income (nine income classes) (Federal Statistical Office Germany in Wirtschaftsrechnungen. Einkommens- und Verbrauchsstichprobe Einnahmen und Ausgaben privater Haushalte 2018, 2020a [19]; Federal Statistical Office Germany in Wirtschaftsrechnungen. Einkommens- und Verbrauchsstichprobe Konsumausgaben privater Haushalte 2018, 2020b [20]). Based on this survey, the question of who needs to be sustained will be analysed as follows: 1.First, the “worldwide reach (Rosa in Resonanz: Eine Soziologie der Weltbeziehung. Suhrkamp, Berlin, 2019 [62])” of the consumption patterns of the German household groups will be measured by their ecological and water footprints.2.Second, an operationalization is derived for Amartya Sen’s sustainable development definition using the food–energy–water nexus as the core of sustainable development (United Nations (UN Water) in Water, food and energy. United Nations, 2020 [73]), and to reveal botha.potential contradictions between the FEW sectors and other sectors of the German society which can negatively affect sustainable development in Germany, andb.the degree of inequality in functionings and capabilities among German households.3.Finally, those household groups will be identified which “have to be sustained” the most according to Sen’s definition. In the presented model, based on ul Haq (Fukuda-Parr in Fem Econ 9:301–317, 2003 [24]; Haq in Human development in a changing world. UNDP, New York, 1992 [30]; Haq in Reflections on human development. Oxford University Press, Oxford, 1995 [31]) and Sen (Anand and Sen in Human development index: methodology and measurement. Occasional papers. UNDP, New York, 1994 [3]; Sen in Ökonomie für den Menschen [Economy for the people], 2nd edn. dtv, Munch, 2003 [59]), the German capability index (GCI) is used to reveal both the capabilities and functionings of German society and the underlying justice structure of the German society. The capabilities (realization opportunities) in this context describe people’s opportunities of using their functionings (abilities) to achieve a place in society (Venkatapuram in Health justice. Polity Press, Cambridge, 2011 [81]) given their personal capabilities, while simultaneously maintaining a sustainable use of the food–energy–water resources. The aim of the German capability index (GCI) is to make the capabilities of the various German households comparable and establish the GCI as an index of revealed capabilities. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2021.},
	author_keywords = {Capability approach; Food–energy–water nexus; Germany; Sustainable development},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Papadonikolaki202381,
	author = {Papadonikolaki, Eleni and Anumba, Chimay},
	title = {How Can Digital Twins Support the Net Zero Vision?},
	year = {2023},
	journal = {Lecture Notes in Civil Engineering},
	volume = {358 LNCE},
	pages = {81 – 97},
	doi = {10.1007/978-3-031-32515-1_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171565075&doi=10.1007%2f978-3-031-32515-1_7&partnerID=40&md5=74f05227df33c8acfe72c1dfb5afbdc4},
	abstract = {Net Zero relates to decarbonisation efforts to tackle climate change by not adding new emissions to the atmosphere. Reaching Net Zero depends on datafication and digitisation as data on carbon emissions collected across assets’ lifecycle are important for neutralising them. This study aims to understand how technological ecosystems such as Digital Twins – that connect physical and digital artefacts – can support a Net Zero vision in the built environment, in an industry-agnostic way to transfer any relevant lessons from other sectors. The method is a systematic literature review that structures new knowledge on the topic. The study showed that environment sustainability has been an overused idea in scientific literature, yet rarely operationalised. A scarcity of studies utilising the potential of digital twins for Net Zero was found. The emergent themes are: overreliance on technocratic solutions at the expense of systems thinking, proliferation of renewable sources and misunderstandings regarding visualisation necessity, and showing pathways for future DT system design for Net Zero. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author_keywords = {Digital twins; model-based systems engineering; net zero; review; sustainability},
	keywords = {Climate change; Life cycle; Systems thinking; Asset lifecycle; Built environment; Carbon emissions; Decarbonisation; Digital artifacts; Digitisation; Model-based system engineerings; Net zero; Physical artifacts; Systematic literature review; Sustainable development},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Sofia2020,
	author = {Sofia, Hakdaoui and Anas, Emran and Faiz, Oumghar},
	title = {Mobile mapping, machine learning and digital twin for road infrastructure monitoring and maintenance: Case study of mohammed VI bridge in Morocco},
	year = {2020},
	journal = {Proceedings - 2020 IEEE International Conference of Moroccan Geomatics, MORGEO 2020},
	doi = {10.1109/Morgeo49228.2020.9121882},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087967317&doi=10.1109%2fMorgeo49228.2020.9121882&partnerID=40&md5=f64c9686dd6e9bcc47bedf0562162dde},
	abstract = {The concepts of Digital Twin has been recently introduced, it refers to functional connections between a complex physical system and its high-fidelity digital replica. Digital Twin process workflow is proposed in case of Mohammed VI Bridge modeling in Morocco. The current maintenance of a road infrastructure is based on a manual inspection and a system based on traditional tools. Aging infrastructures require a new approach to maintenance in terms of inspection, bridge maintenance system, simulation and systematic evaluation. This system now exists and is called the Digital Twin. Digital Twin can be thought of as a virtual prototype in service that changes dynamically in near real time as its physical twin changes. An urban infrastructure digital twin is a virtual instance of his physical twin that is continuously updated with multisource, multisensor and multitemporal data that can be used for monitoring, simulating and forecasting any potential problem that may appear in the structure and proposing planning for repair and maintenance of health status throughout the life cycle of this infrastructure. This work presents a general vision and a justification for integrating DT technology with geospatial data. The paper examines the benefits of integrating 3D GIS data acquired by automated mobile mapping (MMS) workflows for modeling the reality of a major bridge infrastructure in Morocco. This allowed to study the future performance of this bridge structure on virtual twin structures under different environmental conditions. Cloud point data are acquired by a Mobile Mapping System on Mohammed VI Bridge and converted in BIM model by a scan to BIM process and is integrated in a GIS and BIM virtual environment and shows the efficiency of volumetric auscultation in terms of surface flatness and distortion inspection. This project provides a new bridge maintenance system using the concept of a Digital Twin. This digital model is a platform that allows to collect, organize and share the maintenance history of this important road infrastructure in Morocco. © 2020 IEEE.},
	author_keywords = {3D model-based; Digital Twin; Bridge; Cloud point; Mobile mapping systems (MMS); Simulation; Virtual prototype; Volumetric auscultation.},
	keywords = {Data integration; Digital twin; E-learning; Geographic information systems; Inspection; Life cycle; Machine learning; Maintenance; Mapping; Object oriented programming; Roads and streets; Structural health monitoring; Surveying; Bridge maintenance systems; Complex physical systems; Environmental conditions; Functional connection; Mobile mapping systems; Multisensor and multitemporal; Repair and maintenance; Systematic evaluation; Bridges},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 40}
}

@ARTICLE{Santos2024,
	author = {Santos, Francisco J. and Guzmán, Carmen and Ahumada, Pedro},
	title = {Assessing the digital transformation in agri-food cooperatives and its determinants},
	year = {2024},
	journal = {Journal of Rural Studies},
	volume = {105},
	doi = {10.1016/j.jrurstud.2023.103168},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179788244&doi=10.1016%2fj.jrurstud.2023.103168&partnerID=40&md5=1f2a0c3c4968c12aeb4832f2698f2a3d},
	abstract = {The digital transformation (DT) of companies implies the emergence of new business models based on the widespread use of digital technologies. Digital transformation is necessary to improve efficiency, productivity, and market access in a context of increasing competition. In the case of the agri-food sector, DT is also required to address the challenges of food safety, food waste, and sustainability. This research aims to build a theoretical framework and a methodology to assess the global level of DT and its dimensions and factors that influence it in a specific type of agri-food company, namely agri-food cooperatives. These prominent social economy entities follow cooperative principles and play a significant role in rural development, especially in the most backward regions. An empirical analysis has been conducted to validate the methodology and meet the objectives. To this end, a Global Index of DT was built, and data was obtained from a survey of agri-food cooperatives in Andalusia, a southern Spanish region with a long-standing tradition of agri-food cooperatives, which also presents a low per-capita income in the European context. Empirical results effectively reveal that the proposed theoretical framework and methodology expediate the assessment of the global level of DT of agri-food cooperatives and the relevance of certain influential critical factors. In this way, relevant information for cooperatives and policy-makers can be collected to facilitate the implementation of DT. © 2023 The Authors},
	author_keywords = {Agri-food cooperatives; Digital maturity model; Digital transformation; Rural development; Theoretical framework},
	keywords = {Andalucia; Europe; Spain; accessibility; agricultural market; agricultural technology; agroindustry; cooperative sector; digitization; efficiency measurement; food production; food safety; food waste; information and communication technology; rural development; technology adoption; theoretical study},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Hybrid Gold Open Access}
}

@CONFERENCE{Merkle2019,
	author = {Merkle, Lukas},
	title = {Cloud-Based Battery Digital Twin Middleware Using Model-Based Development},
	year = {2019},
	journal = {ACM International Conference Proceeding Series},
	doi = {10.1145/3386164.3387296},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123040661&doi=10.1145%2f3386164.3387296&partnerID=40&md5=1744a5d651639e5eb78e71fd7f7d92c5},
	abstract = {Following the trends of electrification, the energy storage of vehicles is gaining importance as the most expensive part of an electric car. Since lithium-ion batteries are perishable goods and underlie e. g. aging effects, environmental and operating conditions during manufacturing and car usage need close supervision. With regard to the paradigm of digital twins, data from various life cycle phases needs to be collected and processed to improve the general quality of the system. To achieve this complex task, a suitable framework is needed in order to operate the fleet of digital twins during manufacturing processes, the automotive usage and a potential second life. Based on a literature review, we formulate requirements for a digital twin framework in the field of battery systems. We propose a framework to develop and operate a fleet of digital twins during all life cycle phases. Results feature a case study in which we implement the stated framework in a cloud-computing environment using early stages of battery system production as test a bed. With the help of a self-discharge model of li-ion cells, the system can estimate the SOC of battery modules and provide this information to the arrival testing procedures. © 2019 ACM.},
	author_keywords = {Battery System; Control Middleware; Digital Twin; IoT; Self-Discharge},
	keywords = {Automotive industry; Digital storage; Digital twin; Life cycle; Manufacture; Middleware; Cloud computing environments; Expensive parts; Literature reviews; Manufacturing process; Model based development; Operating condition; Perishable goods; Testing procedure; Lithium-ion batteries},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Keskin2022,
	author = {Keskin, Basak and Salman, Baris and Koseoglu, Ozan},
	title = {Architecting a BIM-Based Digital Twin Platform for Airport Asset Management: A Model-Based System Engineering with SysML Approach},
	year = {2022},
	journal = {Journal of Construction Engineering and Management},
	volume = {148},
	number = {5},
	doi = {10.1061/(ASCE)CO.1943-7862.0002271},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127072020&doi=10.1061%2f%28ASCE%29CO.1943-7862.0002271&partnerID=40&md5=1932513bfbd72d548cacde9f14737251},
	abstract = {Airports play an essential role within the transportation and infrastructure sectors. As modernizing and expanding airport infrastructures receive a higher priority, addressing inefficiencies in capital expenditures (CapEx) and operational expenses (OpEx) through transformative digital ecosystems is gaining an increasing level of interest. Building information modeling (BIM) can form a basis to enable an end-to-end digital delivery platform, where airport life cycle data can be utilized for actionable insights. This study proposes a scalable, novel BIM-based modularized platform architecture based on model-based system engineering (MBSE) principles with systems modeling language (SysML) for optimal collection, integration, management, and utilization of airport critical asset data. This platform architecture aims at acting as a metaframework that enables an airport digital twin through cohesive and structured view of asset life cycle information for insightful actions by major airport stakeholders. Research methodology features an online survey and focus group discussions for data collection; mixed method analysis for data analysis; MBSE with SysML for data mapping; and prototype demonstration and an expert opinion survey for validation. Adopting a scalable BIM-based digital platform is anticipated to result in boosts in business value of airport infrastructures by sustaining value generation in operations and decreasing upfront costs for technology implementation.  © 2022 American Society of Civil Engineers.},
	author_keywords = {Asset lifecycle management; Building information modeling (BIM); Digital platform architecture; Digital twin},
	keywords = {Airports; Information management; Information theory; Life cycle; Modeling languages; Surveys; Systems engineering; Asset lifecycle; Asset lifecycle management; Building information modeling; Building Information Modelling; Digital platform architecture; Digital platforms; Lifecycle management; Model-based OPC; Model-based system engineerings; Platform architecture; Architectural design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23}
}

@ARTICLE{Naumchev2019351,
	author = {Naumchev, Alexandr and Sadovykh, Andrey and Ivanov, Vladimir},
	title = {VERCORS: Hardware and Software Complex for Intelligent Round-Trip Formalized Verification of Dependable Cyber-Physical Systems in a Digital Twin Environment (Position Paper)},
	year = {2019},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {11771 LNCS},
	pages = {351 – 363},
	doi = {10.1007/978-3-030-29852-4_30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075682305&doi=10.1007%2f978-3-030-29852-4_30&partnerID=40&md5=ddfd9800bc60ff99a1848056e7a50e0b},
	abstract = {Formal specification, model checking and model-based testing are recommended techniques for engineering of mission-critical systems. In the meantime, those techniques struggle to obtain wide adoption due to inherent learning barrier, i.e. it is considered difficult to use those methods. There is also a common difficulty in translating the specifications in natural language, a common practice nowadays, to formal specifications. In this position paper we discuss the concept of an end-to-end methodology that helps identify specifications from various sources, automatically create formal specifications and apply them to verification of cyber-physical systems. Thus, we intent to address the challenges of creation of formal specifications in an efficient automated and tool-supported manner. The novelty of the approach is analyzed through a survey of state of the art. It is currently planned to implement this concept and evaluate it with industrial case studies. © 2019, Springer Nature Switzerland AG.},
	author_keywords = {Co-simulation; Cyber-physical systems (CPS); Digital twin; Formal specification; Language processing; Model-based testing; Multi-modelling; Natural; Traceability; Verification},
	keywords = {Computer simulation languages; Cyber Physical System; Embedded systems; Model checking; Modeling languages; Natural language processing systems; Verification; Cosimulation; Cyber-physical systems (CPS); Digital twin; Language processing; Model based testing; Multi-modelling; Natural; Traceability; Formal specification},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Li2023183,
	author = {Li, Wenzhuo and Li, Bei and Gao, Haining and Peng, Cong and Zhang, Wei and Li, Yalei and Ji, Xin},
	title = {Survey and Practice on Architecture and Deployment Method of Digital Twin System for Intelligent Substation},
	year = {2023},
	journal = {Proceedings - 2023 7th International Symposium on Computer Science and Intelligent Control, ISCSIC 2023},
	pages = {183 – 190},
	doi = {10.1109/ISCSIC60498.2023.00046},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184999334&doi=10.1109%2fISCSIC60498.2023.00046&partnerID=40&md5=ef4bb06bca625398933dba99315ae48d},
	abstract = {This article studies the architecture and deployment of a digital twin system for intelligent substations. Firstly, the development status of smart grid digital twins was investigated and summarized from typical architectures, key technologies, and application directions. Then, based on modern information technology, we proposed the architecture and deployment method of the intelligent substation digital twin system. The proposed system consists of a data collection layer, a model driven layer, and a twin application layer, which respectively elaborate on the specific content and advantages, and supplement the complete deployment method of the system. The intelligent substation digital twin system effectively promotes the digitization and intelligence of substations, providing a new construction and management mode, and has potential application prospects in promoting equipment status perception, deterioration trend analysis, and fault handling in the power industry.  © 2023 IEEE.},
	author_keywords = {digital twin; equipment status perception; intelligent substations; modern information technology},
	keywords = {Architecture; Construction equipment; Deterioration; Electric substations; Data collection; Deployment methods; Development status; Equipment status perception; Intelligent substation; Key technologies; Model-driven; Modern information technologies; Smart grid; Technologies and applications; Computer architecture},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Xu2021150,
	author = {Xu, Hongming and Zhou, Quan},
	title = {Artificial intelligence technologies for engine control development: State-of-the-art review and outlook; [人工智能在发动机控制开发中的应用及前景]},
	year = {2021},
	journal = {Journal of Automotive Safety and Energy},
	volume = {12},
	number = {2},
	pages = {150 – 162},
	doi = {10.3969/j.issn.1674-8484.2021.02.002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125705011&doi=10.3969%2fj.issn.1674-8484.2021.02.002&partnerID=40&md5=962c3e8a05a34ece3d8071e7efe1d732},
	abstract = {Connection, automation, sharing and electrification (CASE) are the future of vehicle and mobility systems. International Energy Agency (IEA) predicts that electric vehicles including plug-in hybrid vehicles will count for 97% of the market by 2050. The increasingly stringent emission legislations for CO2 reduction, especially when involving real driving emissions (RDE) testing are the main challenges to the control system of the vehicle powertrains. This paper focus on artificial intelligence (AI) technologies for engine control developments that follow the standard model-based routine. By reviewing state-of-the-art AI technologies for feedforward control, feedback control, and global optimization at system level, the advantage and disadvantage of the AI technologies are compared and summarized. An outlook is provided based on the literature survey. It indicates that AI will promote the fusion of technologies in three representative domains, 1) fusion of cyber systems and physical systems, e.g., digital twin of engine; 2) fusion of machine learning systems and classical control systems, e.g., AI-based calibration of engine controllers; and 3) fusion of information from multiple sources, e.g., powertrain domain control network. The technology fusions in these three domains are expected to promote the development of advanced engines which aims to achieve zero emissions. © 2021 Tsinghua University Press. All Rights Reserved.},
	author_keywords = {Artificial intelligence (AI); Controller calibration; Engine control; Model-based development; System optimization},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Giannetti2020,
	author = {Giannetti, Francesca and Puletti, Nicola and Puliti, Stefano and Travaglini, Davide and Chirici, Gherardo},
	title = {Assessment of UAV photogrammetric DTM-independent variables for modelling and mapping forest structural indices in mixed temperate forests},
	year = {2020},
	journal = {Ecological Indicators},
	volume = {117},
	doi = {10.1016/j.ecolind.2020.106513},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084949614&doi=10.1016%2fj.ecolind.2020.106513&partnerID=40&md5=a83dcc9d7b6a4e9251ca4801c15ca9c7},
	abstract = {In the EU 2020 biodiversity strategy, maintaining and enhancing forest biodiversity is essential. Forest managers and technicians should include biodiversity monitoring as support for sustainible forest management and conservation issues, through the adoption of forest biodiversity indices. The present study investigates the potential of a new type of Structure from Motion (SfM) photogrammetry derived variables for modelling forest structure indicies, which do not require the availability of a digital terrain model (DTM) such as those obtainable from Airborne Laser Scanning (ALS) surveys. The DTM-independent variables were calculated using raw 3D UAV photogrammetric data for modeling eight forest structure indices which are commonly used for forest biodiversity monitoring, namely: basal area (G); quadratic mean diameter (DBHmean); the standard deviation of Diameter at Breast Height (DBHσ); DBH Gini coefficient (Gini); the standard deviation of tree heights (Hσ); dominant tree height (Hdom); Lorey's height (Hl); and growing stock volume (V). The study included two mixed temperate forests areas with a different type of management, with one area, left unmanaged for the past 50 years while the other being actively managed. A total of 30 field sample plots were measured in the unmanaged forest, and 50 field plots were measured in the actively managed forest. The accuracy of UAV DTM-independent predictions was compared with a benchmark approach based on traditional explanatory variables calculated from ALS data. Finally, DTM-independent variables were used to produce wall-to-wall maps of the forest structure indices in the two test areas and to estimate the mean value and its uncertainty according to a model-assisted regression estimators. DTM-independent variables led to similar predictive accuracy in terms of root mean square error compared to ALS in both study areas for the eight structure indices (DTM-independent average RMSE% = 20.5 and ALS average RMSE% = 19.8). Moreover, we found that the model-assisted estimation, with both DTM-independet and ALS, obtained lower standar errors (SE) compared to the one obtained by model-based estimation using only field plots. Relative efficiency coefficient (RE) revealed that ALS-based estimates were, on average, more efficient (average RE ALS = 3.7) than DTM-independent, (average RE DTM-independent = 3.3). However, the RE for the DTM-independent models was consistently larger than the one from the ALS models for the DBH-related variables (i.e. G, DBHmean, and DBHσ) and for V. This highlights the potential of DTM-independent variables, which not only can be used virtually on any forests (i.e., no need of a DTM), but also can produce as precise estimates as those from ALS data for key forest structural variables and substantially improve the efficiency of forest inventories. © 2020 Elsevier Ltd},
	author_keywords = {Airborne laser scanning; Biodiversity; Drone; DTM-independent; Forest inventory; Forest structure; Precision forestry; Structure from motion},
	keywords = {Biodiversity; Efficiency; Indexing (materials working); Mean square error; Photogrammetry; Statistics; Temperature control; Uncertainty analysis; Unmanned aerial vehicles (UAV); Airborne Laser scanning; Biodiversity monitoring; Diameter-at-breast heights; Digital terrain model; Management and conservations; Model-based estimation; Quadratic mean diameter; Root mean square errors; assessment method; basal area; biodiversity; conservation status; digital terrain model; ecological modeling; forest inventory; forest management; mixed forest; photogrammetry; satellite data; temperate forest; tree; Forestry},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 22; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Huang20242161,
	author = {Huang, Yishu and Zang, Yufu and Jiang, Qihan and Mi, Wenhan},
	title = {Extraction of Building Outlines from Airborne LiDAR Point Clouds Using Line-CNN Based on Deep Network; [采用 Line-CNN 深度学习网络的机载点云建筑轮廓线提取]},
	year = {2024},
	journal = {Chuan Bo Li Xue/Journal of Ship Mechanics},
	volume = {26},
	number = {9},
	pages = {2161 – 2176},
	doi = {10.12082/dqxxkx.2024.230503},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204961293&doi=10.12082%2fdqxxkx.2024.230503&partnerID=40&md5=595963d61b7f26e4b33476343bbf3072},
	abstract = {Urban 3D modeling is indispensable for digital twinning and the development of smart cities. The effective extraction of building outlines is a critical step in achieving high-precision urban modeling and 3D mapping. At present, the extraction of building outlines from airborne point cloud data still faces challenges, such as low efficiency and accuracy with conventional methods and limited calibration samples. In response to these challenges, this paper introduces a deep learning method for extracting building outlines from 3D airborne point clouds. The airborne LiDAR point clouds are the primary data input. First, through vertical projection to the XOY plane, point clouds of buildings with the application of progressive morphological filtering are converted to rasterized elevation that characterizes spatial variation of terrain and visible light raster images that depict texture differences. Then, the deep learning model based on Lines-Convolutional Neural Networks (Line-CNN) is employed to preliminarily extract line features from raster images, encompassing stages of feature extraction, node prediction, route generation, and others. To enhance the quality of the primary straight-line extraction, an optimization strategy is introduced, which incorporates a range of comprehensive trimming and completion operations, aligning with information extracted from both the elevation and visible light raster images. Simultaneously, false line segments are eliminated, and missing lines are added, resulting in the regular and complete building outline features. To verify the proposed model, the airborne point cloud data from NUIST campus and the ISPRS H3D 2019 datasets are utilized in the experiment. Our results show that the proposed method accurately and comprehensively extracts building outline features from LiDAR images, achieving an impressive average accuracy and completeness rate, both up to 90%. Furthermore, the proposed method is highly efficient and effectively addresses the challenge of insufficient 3D calibration samples in traditional methods, making it suitable for various applications, particularly large-scale urban 3D modeling and cadastral surveying. To sum up, the proposed method constitutes a significant stride in advancing urban modeling and 3D mapping. It provides a novel solution to address the challenges associated with building outline extraction, particularly within the context of smart cities and digital twins. Due to the model's high accuracy, completeness, and efficiency, our method is highly helpful for a wide range of applications in the urban planning and geospatial information fields. © 2024 China Ship Scientific Research Center. All rights reserved.},
	author_keywords = {building contour line; colored airborne point cloud; deep learning; elevation raster image; high precision urban modeling; pruning and completion optimization; visible light raster image},
	keywords = {Benchmarking; Deep neural networks; Geological surveys; Image segmentation; Image texture; Large datasets; Oil tankers; Photomapping; Rasterization; Trimming; Building contour line; Colored airborne point cloud; Contour line; Deep learning; Elevation raster image; High precision urban modeling; High-precision; Optimisations; Point-clouds; Pruning and completion optimization; Raster image; Urban modeling; Visible light; Visible light raster image; Convolutional neural networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hosamo2023,
	author = {Hosamo, Haidar Hosamo and Nielsen, Henrik Kofoed and Kraniotis, Dimitrios and Svennevig, Paul Ragnar and Svidt, Kjeld},
	title = {Digital Twin framework for automated fault source detection and prediction for comfort performance evaluation of existing non-residential Norwegian buildings},
	year = {2023},
	journal = {Energy and Buildings},
	volume = {281},
	doi = {10.1016/j.enbuild.2022.112732},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145262458&doi=10.1016%2fj.enbuild.2022.112732&partnerID=40&md5=4a9930c006a4e7aa1de85d051c92a168},
	abstract = {Numerous buildings fall short of expectations regarding occupant satisfaction, sustainability, or energy efficiency. In this paper, the performance of buildings in terms of occupant comfort is evaluated using a probabilistic model based on Bayesian networks (BNs). The BN model is founded on an in-depth analysis of satisfaction survey responses and a thorough study of building performance parameters. This study also presents a user-friendly visualization compatible with BIM to simplify data collecting in two case studies from Norway with data from 2019 to 2022. This paper proposes a novel Digital Twin approach for incorporating building information modeling (BIM) with real-time sensor data, occupants’ feedback, a probabilistic model of occupants’ comfort, and HVAC faults detection and prediction that may affect occupants’ comfort. New methods for using BIM as a visualization platform, as well as a predictive maintenance method to detect and anticipate problems in the HVAC system, are also presented. These methods will help decision-makers improve the occupants’ comfort conditions in buildings. However, due to the intricate interaction between numerous equipment and the absence of data integration among FM systems, CMMS, BMS, and BIM data are integrated in this paper into a framework utilizing ontology graphs to generalize the Digital Twin framework so it can be applied to many buildings. The results of this study can aid decision-makers in the facility management sector by offering insight into the aspects that influence occupant comfort, speeding up the process of identifying equipment malfunctions, and pointing toward possible solutions. © 2022 The Author(s)},
	author_keywords = {Building information modelling (BIM); Decision-making; Digital twin; Facility management; Fault detection; Predictive maintenance},
	keywords = {Architectural design; Bayesian networks; Climate control; Data integration; Data visualization; Energy efficiency; Fault detection; Information theory; Office buildings; Visualization; Building information modeling; Building Information Modelling; Decision makers; Decisions makings; Facilities management; Faults detection; Occupant comforts; Predictive maintenance; Probabilistic models; Source detection; Decision making},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 38; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Pimenova2022,
	author = {Pimenova, Olga and Roberts, Craig and Rizos, Chris},
	title = {Regional “Bare-Earth” Digital Terrain Model for Costa Rica Based on NASADEM Corrected for Vegetation Bias},
	year = {2022},
	journal = {Remote Sensing},
	volume = {14},
	number = {10},
	doi = {10.3390/rs14102421},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131015316&doi=10.3390%2frs14102421&partnerID=40&md5=b5f6580e2a16d07e9626b6cb8a132644},
	abstract = {A large percentage of the Costa Rican territory is covered with high evergreen forests. In order to compute a 1′′ Bare-Earth Digital Terrain Model (DTM) for Costa Rica CRDTM2020, stochastic Vegetation Bias (VB) was reduced from the 1′′ NASADEM, Digital Elevation Model (DEM) based on the Shuttle Radar Topography Mission (SRTM) data. Several global models such as: canopy heights from the Global Forest Canopy Height 2019 model, canopy heights for the year 2000 from the Forest Canopy Height Map, and canopy density from the Global Forest Change model 2000 to 2019, were used to represent the vegetation in the year of SRTM data collection. Four analytical VB models based on canopy heights and canopy density were evaluated and validated using bare-earth observations and canopy heights from the Laser Vegetation Imaging Sensor (LVIS) surveys from 1998, 2005, and 2019 and a levelling dataset. The results show that differences between CRDTM2020 and bare-earth elevations from LVIS2019 in terms of the mean, median, standard deviation, and median absolute difference (0.9, 0.8, 7.9 and 3.7 m, respectively) are smaller than for any other of the nine evaluated global DEMs. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {bare-earth digital elevation model; CRDTM2020; digital terrain model; LVIS; NASADEM; vegetation bias},
	keywords = {Digital instruments; Forestry; Geomorphology; Landforms; Stochastic models; Stochastic systems; Surveying; Topography; Tracking radar; Bare-earth digital elevation model; Canopy heights; Costa Rica; CRDTM2020; Digital elevation model; Digital terrain model; Laser vegetation imaging sensors; Model-based OPC; NASADEM; Vegetation bias; Vegetation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Treffinger202228,
	author = {Treffinger, Peter and Canz, Michael and Glembin, Jens},
	title = {Investigations on the Students’ Perception of an Online-Based Laboratory with a Digital Twin in the Main Course of Studies in Mechanical Engineering},
	year = {2022},
	journal = {International journal of online and biomedical engineering},
	volume = {18},
	number = {14},
	pages = {28 – 41},
	doi = {10.3991/ijoe.v18i14.35081},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143787953&doi=10.3991%2fijoe.v18i14.35081&partnerID=40&md5=15ffc1d3013d410f2430c1d9afa25e6e},
	abstract = {During the coronavirus crisis, labs had to be offered in digital form in mechanical engineering at short notice. For this purpose, digital twins of more complex test benches in the field of fluid energy machines were used in the mechanical engineering course, with which the students were able to interact remotely to obtain measurement data. The concept of the respective lab was revised with regard to its implementation as a remote laboratory. Fortunately, real-world labs were able to be fully replaced by remote labs. Student perceptions of remote labs were mostly positive. This paper explains the concept and design of the digital twins and the lab as well as the layout, procedure, and finally the results of the accompanying evaluation. However, the implementation of the digital twins to date does not yet include features which address the tactile experience of working in real-world labs. © 2022,International journal of online and biomedical engineering. All Rights Reserved.},
	author_keywords = {Digital twin; Evaluation survey; Laboratory concept; Model-based laboratory; Remote; Virtual lab},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{wang2022603,
	author = {wang, Bo and Deng, Nana and Zhao, Wenhui and Wang, Zhaohua},
	title = {Residential power demand side management optimization based on fine-grained mixed frequency data},
	year = {2022},
	journal = {Annals of Operations Research},
	volume = {316},
	number = {1},
	pages = {603 – 622},
	doi = {10.1007/s10479-021-04119-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106428999&doi=10.1007%2fs10479-021-04119-8&partnerID=40&md5=0f015727529397e343073c53135e4ff5},
	abstract = {Demand-side response (DSR) measures, which facilitate the management of system reliability and maintain system resource adequacy of power grids, are gaining prominence. Traditional optimization methods for residential electricity usage in peak-demand hours often lack flexibility and effectiveness in solving practical problems due to a large amount of data and a high degree of data uncertainty. Our study aims to obtain a high degree of responsiveness for numerous and highly dispersed residential customers by integrating data-driven and model-driven methods. In this study, we conducted large-scale DSR controlled trials, collected 15-min intervals of electricity consumption data, and matched the survey data. A dynamic time-warping clustering-based difference-in-difference method was proposed for empirical analysis. The results indicate that monetary incentives induced a statistically significant reduction in electricity usage during peak periods in summer by 18.6%, while the response effect was not significant in winter. The heterogeneity analysis suggests that the reduction was mainly contributed by the middle-income group and the elderly group. Our study provides a basis for policymakers to formulate tailored policies to optimize residents’ electricity consumption during peak hours. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {DSR management; DTW-clustering; Large-scale randomized controlled trials; Monetary incentives},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{El – Zahlanieh2022,
	author = {El – Zahlanieh, Stephanie and Sivabalan, Shyarinya and Dos Santos, Idalba Souza and Tribouilloy, Benoit and Brunello, David and Vignes, Alexis and Dufaud, Olivier},
	title = {A step toward lifting the fog off mist explosions: Comparative study of three fuels},
	year = {2022},
	journal = {Journal of Loss Prevention in the Process Industries},
	volume = {74},
	doi = {10.1016/j.jlp.2021.104656},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116891749&doi=10.1016%2fj.jlp.2021.104656&partnerID=40&md5=4049e824eb56e6928cfe743f20a3d8b7},
	abstract = {Gases, vapors, and dusts are all potential explosion threats; however, mists should also be taken into account. Indeed, dozens of accidents involving hydrocarbon mists were identified in incident surveys. Mist explosions continue to occur, highlighting the need to evaluate and assess the validity of present approaches for assessing mist ATEX risks and to establish reliable standardized safety parameters for fuel mists. In a modified apparatus based on the 20 L explosion sphere, three fluids of industrial interest were investigated. A new siphon injection system comprising a Venturi junction was installed, offering a wide range of dispersion performances. This system was controlled by a specifically developed program, ensuring the apparatus's versatility and adaptability to various tested liquids. It enables precise control of the gas carrier flow, liquid flow, and injection and ignition durations, allowing modification of the dilution rate of a particular droplet size distribution (DSD). The mist cloud dispersed in the 20 L sphere was characterized by determining its DSD using an in-situ laser diffraction sensor and by performing Particle Image Velocimetry (PIV). Mists of kerosene, diesel and ethanol were then subjected to tests to assess their lower explosive limit (LELmist), minimum ignition energy (MIE), maximum explosion pressure (Pmax), and rate of pressure rise (dP/dtmax). For instance, it was found that the LELmist of ethanol, kerosene Jet A1, and diesel fuel for a DSD averaged at 8–10 μm reach 77, 94, and 93 g/m3 respectively. This LELmist was also shown to increase with increasing DSD in the case of Jet A1 mists. A sensitivity study was also performed to emphasize the impact of parameters such as the fuel type, the DSD, and the mist temperature. Findings showed that the explosion severity is strongly influenced by the chemical nature and the volatility of the dispersed fuel. Moreover, controlling the sphere temperature was proven to be a crucial step when using such apparatus for the evaluation of the explosibility of mists. An evaporation model based on the d2 law was also developed to visualize the vapor-liquid ratio before ignition. These findings have already led to the development of a new procedure for determining safety standards for hydrocarbon mists, as well as tools to assess mist explosion risks. They have proven that it is possible to evaluate the ignition sensitivity and explosion severity of fuel mists using a single well-known apparatus. © 2021 Elsevier Ltd},
	author_keywords = {Explosion; Fuels; Hazardous areas; Hydrocarbon aerosols; Process safety; Risk assessment},
	keywords = {Aerosols; Diesel engines; Ethanol; Hydrocarbons; Liquids; Risk assessment; Software testing; Velocity measurement; Comparatives studies; Droplet size distributions; Hazardous area; Hydrocarbon aerosol; Injection systems; Potential explosion; Process safety; Risks assessments; Safety parameters; Venturi; Explosions},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Bronze Open Access}
}

@ARTICLE{Wu2024,
	author = {Wu, Wenyu and Sun, Xuan and He, Ziwei and Rong, Weiran and Fan, Hao and Xue, Chengqi},
	title = {Establishing a user demand hierarchy model driven by a mental model for complex operating systems},
	year = {2024},
	journal = {International Journal of Industrial Ergonomics},
	volume = {103},
	doi = {10.1016/j.ergon.2024.103634},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202349935&doi=10.1016%2fj.ergon.2024.103634&partnerID=40&md5=d15228f6c47fc85c951d9a4ceec1b86e},
	abstract = {In complex human-computer interactions, issues such as task failures, system malfunctions, and frequent accidents caused by user errors are common. Therefore, it is necessary to study complex system interactions to enhance overall efficiency. This study focuses on the task interface of a digital twin system for computer numerical control (CNC) machine tools and examines the relationship between the user mental model and interface design elements. Key mental information is obtained through questionnaires and interviews, forming the basis for establishing a user mental model. High-frequency information words are extracted, experimental samples are designed, and importance rating surveys are conducted. Quantitative analysis methods, including factor analysis and weight calculation, are utilized to analyze the needs of target users. Consequently, a user demand hierarchy model is constructed. This approach aims to effectively reduce user errors in the human-computer interaction process within complex systems and enhance cognitive efficiency. © 2024 Elsevier B.V.},
	author_keywords = {Complex system; Human-computer interaction; Mental model; Visual interface},
	keywords = {Computer interaction; Failure systems; Hierarchy models; Mental model; Model-driven; Overall efficiency; System interactions; Task failures; User demands; Visual Interface; adult; Article; cognition; cognitive model; data analysis; female; health survey; human; human computer interaction; male; mathematical model; middle aged; quantitative analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Wang2024,
	author = {Wang, Chen and Liu, Yunlong and Zhang, Qichun and Lin, Xinyu and Ma, Jingyao},
	title = {A Novel Multi-Class Product Rating Prediction Model based on Enhanced Textrank Text Encoding and Human Psychology Simulation},
	year = {2024},
	journal = {Proceedings of the IEEE International Conference on Industrial Technology},
	doi = {10.1109/ICIT58233.2024.10540727},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195809631&doi=10.1109%2fICIT58233.2024.10540727&partnerID=40&md5=040e8ba9cccf10fc84778ca208d28518},
	abstract = {In recent years, with the booming development of the internet and e-commerce, user comments and ratings on a particular product often determine the marketing strategies of businesses. It is believed that if businesses can collect user feedback (such as through surveys) before they rate a product, they can analyze people's product experiences at minimal cost. However, most comments are in free-form, slang, or internet language, and manually categorizing and annotating them would require significant resources, which is not feasible for small and medium -sized businesses. To address this issue, some deep learning frameworks have been used for sentiment analysis and rating prediction in text. However, these models often overlook the actual application scenario when predicting ratings. In reality, users' ratings are not solely based on their own experiences but are also influenced by the historical comments on the product. Therefore, we propose a supervised product rating prediction model. In the first stage, the improved dynamic TextRank submodule (DTsub) and a lightweight Feature Fusion submodule (FFsub) are used to construct text vectors, utilizing a sentiment word library. In the second stage, a Long Short-Term Memory (LSTM) neural network and a Feed Forward neural network (FFNN) are utilized to predict product ratings (multiclass) based on the actual application scenario using ensemble learning and voting. Additionally, we also introduced a weighted cross-entropy loss tailored to businesses product ratings in our model. Experimental results on an Amazon product review dataset show significant improvements in accuracy, F1 score, and F2 score of our proposed DTFS-LFNN model.  © 2024 IEEE.},
	author_keywords = {Human psychology; Machine Learning; NLP; Review rating prediction; Sentiment analysis},
	keywords = {Feedforward neural networks; Learning systems; Long short-term memory; Marketing; Sentiment analysis; Signal encoding; Application scenario; Human psychology; Machine-learning; Model-based OPC; Prediction modelling; Product ratings; Review rating prediction; Sentiment analysis; Submodules; Text encoding; Forecasting},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Renold202443984,
	author = {Renold, A. Pravin and Kathayat, Neeraj Singh},
	title = {Comprehensive Review of Machine Learning, Deep Learning, and Digital Twin Data-Driven Approaches in Battery Health Prediction of Electric Vehicles},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {43984 – 43999},
	doi = {10.1109/ACCESS.2024.3380452},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188911810&doi=10.1109%2fACCESS.2024.3380452&partnerID=40&md5=63ccdd2dc4a3d46cecb2f8e0cf328861},
	abstract = {This paper presents a comprehensive survey of machine learning, deep learning, and digital twin technology methods for predicting and managing the battery state of health in electric vehicles. Battery state of health estimation is essential for optimizing the battery usage, performance, safety, and cost-effectiveness of electric vehicles. Estimating the state of health of a battery is a complex undertaking due to its dependency on multiple factors. These factors include battery characteristics such as type, chemistry, size, temperature, current, voltage, impedance, cycle number, and driving pattern. There are drawbacks to traditional methods, such as experimental and model-based approaches, in terms of accuracy, complexity, expense, and viability for real-time applications. By employing a variety of algorithms to discover the nonlinear and dynamic link between the battery parameters and the state of health, data-driven techniques like machine learning, deep learning, and data-driven digital twin technologies can get beyond these restrictions. Data-driven methods can also incorporate physics and domain knowledge to improve the explainability and interpretability of the results. This paper reviews the latest advancements and challenges of using data-driven techniques for battery state of health estimation and management in electric vehicles. The paper also discusses the future directions and opportunities for further research and development in this field. The survey scope spans publications from the year 2021 to 2023. © 2013 IEEE.},
	author_keywords = {battery health prediction; data-driven methods; deep learning models; digital twin technology; electric vehicles; lithium-ion batteries; Machine learning models},
	keywords = {Battery management systems; Charging (batteries); Cost effectiveness; Deep learning; E-learning; Electric vehicles; Engineering education; Battery; Data-driven methods; Deep learning; Deep learning model; Digital twin technology; Learning models; Machine learning models; Machine-learning; Predictive models; State of health; Lithium-ion batteries},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access}
}

@CONFERENCE{Salehi2023,
	author = {Salehi, Vahid},
	title = {APPLICATION OF MUNICH AGILE CONCEPT FOR MBSE BASED DEVELOPMENT OF AUTOMATED GUIDED ROBOT BASED ON DIGITAL TWIN-DATA},
	year = {2023},
	journal = {Proceedings of the ASME Design Engineering Technical Conference},
	volume = {2},
	doi = {10.1115/DETC2023110983},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178512518&doi=10.1115%2fDETC2023110983&partnerID=40&md5=2e3fde5b1bf02d32f04b66b67f386828},
	abstract = {Simulation and Virtualization of robotic systems and Autonomous Guides Vehicle (AGV) is one of the most important aspects in the future development of such systems. This paper will contribute a framework and an entire toolchain to present a holistic and systematic approach for development of future robotic AGV systems. In the first section this paper will present the first descriptive study which is the literature survey and the definition of the scientific gaps and research questions related to systematic approaches based on Systems Engineering and Model Based System Engineering. In section number two this paper will present a new approach and framework called “Munich Agile Concept for Modell based Systems Engineering (MAGIC)” to develop entire virtual tool chains for virtualization and simulation of different functions in a virtual environment. This aspect is one of the important contributions of this paper. MBSE is based on three important core pillar which is 1) Methods/Processes, 2) Language and 3) Systems. The purpose of the new developed Munich Agile Concept Approach is to handle the complexity over the entire holistic information framework for simulation and virtualization of AGV function development from the system requirement definition process up to the test and validation of the system. Furthermore, with MAGIC there is a holistic approach and method to carry out the entire virtual development process by means of capable simulation and virtualization tools based on MAGICLOOP. The Munich Agile Concept contains six different levels which are System Requirement-, System Functional-, System Architecture-, System Validation-, System Test and the System-Usage-Level. For defining the first three-level, a graphical language called System Modelling Language (SysML) has been applied. The third chapter will apply the developed approach in an industrial Use Case which is based on simulation and virtualization of AGVs in a manufacturing environment. The last chapter will summaries the results of the applied Method. A lot of verification and testing are required robot and AGV development. ore the autonomous driving system can be deployed. However, testing in a real world will consume a lot of resources and may also bring security risks. Therefore, simulation is particularly important for autonomous driving systems. Copyright © 2023 by ASME.},
	author_keywords = {AGV; autonomous driving; MAGIC; simulation; visualization},
	keywords = {Agile manufacturing systems; Autonomous vehicles; Data visualization; Iterative methods; Requirements engineering; Virtual reality; Virtualization; Autonomous driving; Autonomous guide vehicle; Descriptive studies; Driving systems; MAGIC; Robotic systems; Simulation; System requirements; Vehicle system; Virtualizations; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhu2020,
	author = {Zhu, Erzhou and Ju, Yinyin and Chen, Zhile and Liu, Feng and Fang, Xianyong},
	title = {DTOF-ANN: An Artificial Neural Network phishing detection model based on Decision Tree and Optimal Features},
	year = {2020},
	journal = {Applied Soft Computing Journal},
	volume = {95},
	doi = {10.1016/j.asoc.2020.106505},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087199187&doi=10.1016%2fj.asoc.2020.106505&partnerID=40&md5=b9ad22d03a6d910d25099bbc505c8f5e},
	abstract = {Recently, phishing emerges as one of the biggest threats to human's daily networking environments. Phishing attackers disguise illegal URLs as normal ones to steal user's private information with the social engineering techniques, such as emails and SMS, which calls for an effective method of preventing phishing attacks to relieve the loss by them. Neural networks can be used to detect and prevent phishing attacks because of their strong active learning abilities from massive datasets and high accuracy in data classification. However, duplicate points in the public datasets and negative and useless features in the feature vectors will trap the training of the neural networks into the problem of over-fitting, which will make the trained classifier weak when detect phishing websites. This paper proposes DTOF-ANN (Decision Tree and Optimal Features based Artificial Neural Network) to tackle this shortcoming, which is a neural-network phishing detection model based on decision tree and optimal feature selection. First, the traditional K-medoids clustering algorithm is improved with an incremental selection of initial centers to remove the duplicate points from the public datasets. Then, an optimal feature selection algorithm based on the new defined feature evaluation index, decision tree and local search method is designed to prune out the negative and useless features. Finally, the optimal structure of the neural network classifier is constructed through properly adjusting parameters and trained by the selected optimal features. Experimental results have demonstrated that DTOF-ANN exhibits higher performance than many of the existing methods. © 2020 Elsevier B.V.},
	author_keywords = {Feature selection; K-medoids clustering; Neural network; Phishing detection},
	keywords = {Classification (of information); Clustering algorithms; Computer crime; Decision trees; Neural networks; Structural optimization; Trees (mathematics); Adjusting parameters; Data classification; K-medoids clustering; Local search method; Networking environment; Neural network classifier; Optimal feature selections; Phishing detections; Feature extraction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 58}
}

@CONFERENCE{Teng2022,
	author = {Teng, Zhan and Liu, Wanqing and Yi, Yaowu},
	title = {Research on the Application of Small Area 3D Modeling Based on Consumer UAV - Take DJI Yu-2 UAV as an Example},
	year = {2022},
	journal = {Journal of Physics: Conference Series},
	volume = {2410},
	number = {1},
	doi = {10.1088/1742-6596/2410/1/012015},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145202773&doi=10.1088%2f1742-6596%2f2410%2f1%2f012015&partnerID=40&md5=565d6decd4892e26edfbb0ed3da48bb2},
	abstract = {The use of UAVs for geographic information mapping is a relatively mature technology. It has the advantages of a large measurement range, fast data collection speed and high efficiency. It is gradually becoming an important technical means that can partially replace traditional field measurement. 3D modeling of UAV image data is an important aspect of UAV surveying and mapping applications. Under the current background of "real China"and "digital twin", UAV modeling will have very broad application prospects. The current UAV modeling is mainly based on large fixed-wing UAVs or professional rotary-wing UAVs, but their prices are relatively high, so this paper tries to use a relatively low-priced consumer-grade UAV-DJI Yu 2, the professional version of the UAV performs image control-free 3D modeling without ground control points and analyzes the accuracy of the results to verify its applicability in geographic information data collection.  © Published under licence by IOP Publishing Ltd.},
	keywords = {3D modeling; Data acquisition; Fixed wings; Mapping; Rock mechanics; Three dimensional computer graphics; 'current; 3D models; 3d-modeling; Data collection; Geographic information; Higher efficiency; Information mapping; Measurement range; Model-based OPC; Small area; Unmanned aerial vehicles (UAV)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Gratius2022,
	author = {Gratius, Nicolas and Hou, Yu and Bergés, Mario and Akinci, Burcu},
	title = {Lessons learned on the implementation of probabilistic graphical model-based digital twins: A space habitat study},
	year = {2022},
	journal = {Proceedings of the International Astronautical Congress, IAC},
	volume = {2022-September},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167580051&partnerID=40&md5=5ba26d4ecf4eca0b2be94acfb579d355},
	abstract = {Habitats for future human spaceflights will require more resilient environmental control and life support systems (ECLSS). To that end, it is important to facilitate decision making in case of unexpected failure by quantifying the uncertain and dynamic nature of the physical phenomena involved. Combining probabilistic and deterministic models is a particularly promising approach to address this issue. In particular, Probabilistic Graphical Model (PGM) based digital twins are relevant as they embed random variables evolving overtime. Previous research used this modelling method for several applications such as monitoring structural health or manufacturing processes. We envision that the space exploration sector can also benefit from this approach by using the insight gained on specific sub-systems. In this study, we propose lessons learned on the implementation process of PGM-based Digital Twin to quantify uncertainties for temperature prognosis in ECLSS. These findings are introduced as a step-by-step guideline which result in developing a probabilistic model applicable to space habitats. We focused on directed acyclic graphs as this type of PGM can integrate expert's knowledge with data which has been proven to enhance accuracy. A literature review was conducted to identify the state-of-the-art practices and the proposed lessons learned were derived from the study of a physical infrastructure meant to predict the behavior of a space habitat. A temperature control failure scenario was considered, and the Digital Twin estimated the time available before the temperature would become critical. Experiments were conducted on three office rooms to simulate the behavior of an ECLSS. The model was trained offline using historical sensor data and performed inference online by computing the conditional probability of a multivariate normal density. We found that a successful implementation process requires to iteratively go through four stages: outline, design, calibrate and evaluate. It involves selecting ECLSS relevant functionalities and an associated decision-making problem that relies on habitability criteria. Observable variables must be chosen according to a sensors architecture that is compatible with a typical habitat infrastructure. As real space systems are not easily available for model validation, we suggest evaluating early designs on high fidelity analogs. In future work, we envisage to further assess the impact of the design stage on the model's performance by considering computational cost and inference capability. Copyright © 2022 by the International Astronautical Federation (IAF). All rights reserved.},
	author_keywords = {Digital twin; ECLSS; Probabilistic graphical model},
	keywords = {Decision making; Directed graphs; Ecosystems; Environmental management; Industrial research; Iterative methods; Space research; Decisions makings; Dynamic nature; Environmental control and life support system; Human spaceflights; Implementation process; Model-based OPC; Probabilistic graphical models; Probabilistic models; Space habitat; Unexpected Failures; Graphic methods},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2024978,
	author = {Liu, Yan and Sun, Yanning and Chen, Chuanfa and Liu, Panpan and Liu, Yating},
	title = {Improving Urban Digital Elevation Models Based on Iinterpretable Random Forest Method Considering Spatial Heterogeneity; [城区数字高程模型修正方法：顾及空间异质性的可解释随机森林模型]},
	year = {2024},
	journal = {Journal of Geo-Information Science},
	volume = {26},
	number = {4},
	pages = {978 – 988},
	doi = {10.12082/dqxxkx.2024.230590},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193797667&doi=10.12082%2fdqxxkx.2024.230590&partnerID=40&md5=44a8446f81469103be89670ced8ed0c3},
	abstract = {Due to the influence of buildings, the Global Digital Elevation Model (GDEM) obtained through remote sensing measurement still contains various ground features in urban areas and cannot reflect the bare Earth's surface. This limits its application as basic data in hydrological simulation, geological disaster prediction, urban construction, and other fields. Therefore, in order to improve the quality of GDEM products in urban areas, this paper proposed a GDEM correction method that takes into account spatial heterogeneity and SHAP (Shapley Additive Explanations) feature screening. Firstly, the Gaussian Mixture Model (GMM) and Theissen polygons were used to divide the study region into the several sub-areas to solve the heterogeneity of GDEM correction error and the spatial relationship of characteristic factors. Then, the Random Forest (RF) was used as the base model, coupled with the SHAP interpretable framework, to screen the optimal feature variables in each subregion. Finally, based on the selected features, the corresponding modified model was reconstructed for urban GDEM correction. To verify the practicability and efficiency of the proposed method, 23 initial feature variables were selected in this paper. COPDEM (COPDEM30) with 30 m resolution in New York was taken as the research object, and airborne Light Detection And Ranging (LiDAR) DTM data were used as the reference. The proposed method was compared with three other methods, including spatially heterogeneous random forest (SH-RF), global random forest (FS- RF), and traditional Random Forest (RF), as well as an existing GDEM (FABDEM) product which removes building and vegetation biases. The experimental results show that the proposed method had the best prediction performance, with its Mean Absolute Error (MAE) decreasing from 5.209 m to 1.436 m and median error (RMSE) decreasing from 8.884 m to 2.258 m, followed by SH-RF, with its MAE decreasing by 3.607 m and RMSE decreasing by 6.389 m. RF was the worst, with MAE decreasing by 3.179 m and RMSE decreasing by 5.838 m. In addition, compared to FABDEM, the MAE and RMSE of the proposed method were reduced by 42.3% and 63.2%, respectively. The migration experiments on the proposed method showed that, compared to the original COPDEM30, the modified GDEM's MAE and RMSE were reduced by 50.5% and 50.4%, respectively. The visual comparison of the DEM before and after modification also showed that the modified COPDEM30 not only retained the topographic features well but also had similar elevation distribution with LiDAR DTM. Therefore, the method in this paper shows a novel generalization ability. © 2024 Science Press. All rights reserved.},
	author_keywords = {accuracy assessment; correction; feature selection; global digital elevation model; SHAP; spatial heterogeneity; urban area},
	keywords = {Digital instruments; Errors; Feature extraction; Forecasting; Forestry; Geomorphology; Object detection; Object recognition; Optical radar; Remote sensing; Surveying; Accuracy assessment; Correction; Digital elevation model; Features selection; Global digital elevation model; Random forests; Shapley; Shapley additive explanation; Spatial heterogeneity; Urban areas; accuracy assessment; correction; digital elevation model; game theory; heterogeneity; machine learning; remote sensing; urban area; Gaussian distribution},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Raghuwanshi2024178,
	author = {Raghuwanshi, Kapil Dev and Yagnik, Shruti},
	title = {A Survey: Detection of Heart-Related Disorders Using Machine Learning Approaches},
	year = {2024},
	journal = {Communications in Computer and Information Science},
	volume = {2050 CCIS},
	pages = {178 – 188},
	doi = {10.1007/978-3-031-58953-9_14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196073854&doi=10.1007%2f978-3-031-58953-9_14&partnerID=40&md5=a679fcafc7ba63cc21f0de64d72cb2b8},
	abstract = {Heart-related illnesses often known as CVDs (cardiovascular diseases) seem to be the leading cause of mortality globally in recent years. Consequently, a precise, workable, and trustworthy technique is necessary to recognize this disorder before time and begin the suitable treatment course. In this automated analysis of vast and complex health datasets, numerous machine learning methods are employed to scrutinize the information. Various machine learning techniques that have been developed by researchers are now being used by healthcare professionals to aid in the detection of heart-related disorders. Proposed study examines several models based on different methodological approaches, assessing the functionality of each. The Naive-Bayes model, SVM model (Support Vector Machines model), KNN model (K-Nearest Neighbor Model), DT model (Decision Trees Model), Ensemble models, and Supervised learning techniques based on RF model (Random Forest Model) are highly favored by researchers. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.},
	author_keywords = {Decision Tree; Heart Disease; KNN; Random Forest; SVM},
	keywords = {Bayesian networks; Cardiology; Diseases; Heart; Learning algorithms; Learning systems; Nearest neighbor search; Support vector machines; Automated analysis; Cardiovascular disease; Health care professionals; Heart disease; KNN; Machine learning approaches; Machine learning methods; Machine learning techniques; Random forests; SVM; Decision trees},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Aslanyan2021,
	author = {Aslanyan, Artur and Ganiev, Bulat and Lutfullin, Azat and Farhutdinov, Ildar Z. and Gulyaev, Danila and Farakhova, Rushana and Zinurov, Linar and Nikonorova, Anastasiya},
	title = {The integrated technology of residual reserves localization and profit increase on brownfields},
	year = {2021},
	journal = {Society of Petroleum Engineers - SPE Europec featured at 82nd EAGE Conference and Exhibition, EURO 2021},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118429170&partnerID=40&md5=56ec9560b943c78bafab89da385ba18c},
	abstract = {Brown fields that are currently experiencing production decline can benefit a lot from production enhancement operations based on localization of residual reserves and geology clarification. The set of solutions includes targeted recommendations for additional well surveys followed by producers and injectors workovers, like whole wellbore or selective stimulation, polymer flow conformance, hydraulic fracturing and side tracking. As a result, previously poorly drained areas are involved in production, which increases current rates and ultimate recovery. The integrated technology of residual reserves localization and production increase includes: 1. Primary analysis of the production history for reservoir blocks ranking by production increase potential. 2. Advanced bottom-hole pressures and production history analysis by multiwell deconvolution for pressure maintenance system optimization and production enhancement. 3. Advanced production logging for flow profile and production layer-by-layer allocation. 4. Conducting pulse-code interference testing for average saturation between wells estimation. 5. 3D reservoir dynamic model calibration on advanced tests findings. 6. Multi-scenario development planning for the scenario with biggest NPV regarding surface infrastructure. The presented integrated technology is carried stage by stage. Based on the data analysis at the first stage (the Prime analysis) it is possible to get three types of results. The top-level assessment of the current development opportunities of the area, evaluation of current residual reserves on base of displacement sweep efficiency estimation, and evaluation of the potential production increase for various blocks of the field. Results of the second stage were obtained for the block deemed with the highest potential for production increase. Those results may reveal possible complications, and relevant workovers can be advised along with additional surveys that can further help to locate current reserves. The last stage of Prime analysis provides the most suitable choice was to perform an advanced logging and well-testing, as they include both single-well and multi-well tests. Pulse-code interference tests, multi-well retrospective tests and reservoir-oriented production logging make it possible to scan the reservoir laterally and vertically, which is especially important for multi-layered fields. The reservoir parameters obtained from the test results are used to calibrate the dynamic reservoir model. The effects of production enhancement operations are calculated from the 3D model. The set of possible activities is evaluated in terms of their financial efficiency based on the economic model of the operator company using multi-scenario approach on a specifically created digital twin of the field. The unique feature of this approach lies in an integrated usage of advanced production history analysis, advanced logging and well-testing technologies, as well as further calibration of the dynamic reservoir model based on test results and used-friendly interface for field digital twin interaction. This paper demonstrates on how to use the field tests results to calibrate the reservoir model and increase the accuracy of production forecasting by reducing the model uncertainty, with intent to increase profit of brownfields.  Copyright © 2021 Society of Petroleum Engineers.},
	keywords = {3D modeling; Bottom hole pressure; Efficiency; Gasoline; Oil wells; Petroleum reservoir evaluation; Surveys; 'current; Brownfields; Enhancement operations; Integrated technologies; Localisation; Multi wells; Production enhancement; Production increase; Residual reserves; Workover; Well testing},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{El Yazidi2024,
	author = {El Yazidi, Mayssa and Orgel, Csilla and Sefton-Nash, Elliot and De Marchi, Guido and Rickbir, Bahia and Baratoux, David and Bouley, Sylvain and Filiberto, Justin and D'Incecco, Piero and Leone, Giovanni and Slim Shimi, Najet and Srarfi, Feyda and Bradák, Balázs},
	title = {Analysis of faults and pit chains in Noctis Labyrinthus: Implications for early extension and possible magmatic plumbing},
	year = {2024},
	journal = {Icarus},
	volume = {415},
	doi = {10.1016/j.icarus.2024.116075},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189562262&doi=10.1016%2fj.icarus.2024.116075&partnerID=40&md5=2752a02a9e47e09aecbbebf891b6cb15},
	abstract = {Noctis Labyrinthus has been a region of disputed origin due to its complexity and poor understanding of how various processes and mechanisms may have combined to form it. The surface is an integrated record of intensive tectonic activity expressed by a multiple extended sets of dip-slip faults oriented in different directions, and thought to have acted on this region over its history. These faults are always coalescent to pits and pit chains displaying a complicated geological history in the region. To understand this geological history, we mapped the surface features in Noctis Labyrinthus using the High-Resolution Stereo Camera (HRSC) onboard Mars Express ND2 nadir channel basemaps, and we adapted the Digital Terrain Map (DTM) from the Mission Experiment Gridded Data Record (MEGDR) of Mars Orbiter Laser Altimeter (MOLA) onboard Mars Global Surveyor (MGS) for the topography. We have investigated the spatial distribution and trend of fault systems, the pit chains' morphology, and the correlation between these two types of features. Our results show three fault systems: i) NS and NNE-SSW, ii) EW and ENE-WSW, and iii) NNW-SSE and NW. The analysis of the faults trending, cross-cutting correlation and the superimposition led to identify multiple intersections between these faults that have been alongside with the reactivations of some inherited faults. We interpreted the first system of fault to be related to coeval lateral extension, generated by regional stress tensor, which is probably related to the slight bending of Valles Marineris within two phases of bidirectional extension. The second system of faults has been generated by the radial oblate stress tensor related to the formation of the small shield volcanoes in Syria Planum. However, the third system is likely related to the external driving process, probably in the Tharsis province. We classified pits in four evolutionary stages based on their morphometric attributes. We believe that the formation of the pit chains in Noctis Labyrinthus is related to a surface collapse after a pressure drop related to the magma chamber deflation associated with Syria Planum volcanic province. We propose a deformational model based on early extension and magmatic plumbing as driving processes for the formation of Noctis Labyrinthus. © 2023},
	author_keywords = {Extensional tectonic; Faults; Grabens; Magmatic plumbing; Noctis Labyrinthus; Pit chains},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Maffeis20231690,
	author = {Maffeis, Massimo and Casavecchia, Roberto and Casini, Andrea and Larcher, Mario and Fernandez, Mario and Cabrera, July Marcela Aparicio and Paletta Piovezan, Fernanda A. and Garcia, Ignacio Berenguer and Toffoletto, Gianluca and Giammanco, Fabio and Franzone, Giovanni and Amadei, Francesco},
	title = {ENEL GRIDS' NETWORK DIGITAL TWIN®: THE FOUNDATION LAYER OF INTEGRATED SUITE FOR DISTRIBUTION SYSTEMS DESIGN},
	year = {2023},
	journal = {IET Conference Proceedings},
	pages = {1690 – 1694},
	doi = {10.1049/icp.2023.0996},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181532412&doi=10.1049%2ficp.2023.0996&partnerID=40&md5=b9173b4b27146434f8fb4bc257f7b8e0},
	abstract = {One of the strongest needs of an Electric Utility is the in-depth knowledge of its assets and networks. With its Network Digital Twin® program Enel Grids put in place the basis for expanding and accelerating this knowledge to its deepest. A fundamental part of this journey is collecting, organizing, and processing the survey data coming from the field. Applying innovative and cutting-edge technologies allows to squeeze all the possible values from the collected data, put in place a real "data democratization" across all the organization and collect important benefits such as improved asset location and management, infrastructure health monitoring, predictive and fast maintenance processes, optimized network operation and emergency management. Hence Network Digital Twin® foundation layer leads to enhanced grid resilience and boosts data-driven decisions, playing an important role in terms of sustainability and achievement of Network carbon neutrality. Network Digital Twin® Foundation Layer is based on technologies that ranges from cloud and big data to cutting-edge Artificial Intelligence computer vision models based on machine learning, used to perform massive and automated extraction of electric networks features from the collected survey data. The survey data consist of 3D point cloud data from LIDAR sensors and images (RGB and thermal) collected within reglementary periodic inspection process using helicopter, drones, mobile mapping systems and terrestrial laser scanner. Survey data represents a frequently updated snapshot of the “real world” (the visible networks assets, both from the aerial network and from the Primary and Secondary substations) and it is integrated by additional satellite data. In this paper we want to share the main technologies that run "under the hood" and play a central role in the realization of the project, the challenges and problems involved with their implementation and their use, the lessons learned, the impacts of their use on consolidated business processes and how Enel Grids approached the systematic collection and management of the huge amounts of data that grows every day together with the integration of the Network Digital Twin® within its corporate IT solutions platform. © The Institution of Engineering and Technology 2023.},
	keywords = {Antennas; Artificial intelligence; Digital instruments; Electric power distribution; Electric utilities; Integrated control; Optical radar; Risk management; Cutting edge technology; Distribution systems; Grid network; In-depth knowledge; Infrastructure health monitoring; Location infrastructure; Maintenance process; Management infrastructure; Optimized network operation; Survey data; Data handling},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Mao20231442,
	author = {Mao, Runze and Li, YuanJiang and Zhang, Houxiang},
	title = {Simulation Method in Automotive, Aviation and Maritime Industries for Digital Twin: A Brief Survey},
	year = {2023},
	journal = {Proceedings of the 18th IEEE Conference on Industrial Electronics and Applications, ICIEA 2023},
	pages = {1442 – 1447},
	doi = {10.1109/ICIEA58696.2023.10241843},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173614838&doi=10.1109%2fICIEA58696.2023.10241843&partnerID=40&md5=b4501ada2c93778296e97f5d24489ce6},
	abstract = {In this work, a brief survey of simulation methods used to create digital twins (DTs) or assist DTs in the automotive, aviation, and marine industries is presented. The simulation methods are classified as model-driven, data-driven, and hybrid methods. In addition, simulation methods in these three industries are studied from the phases of design, manufacturing, and operation. The similarities, differences and characteristics of the simulation methods applied to the automotive, aviation and maritime industries are discussed and summarized from several aspects. Model-driven approaches are used more frequently than the other two methods in design and manufacturing phases, while hybrid methods have great potential to support different operations of DT-related studies in the reviewed three industries. In addition, issues of prognostics and health management (PHM) such as fault diagnosis, remaining useful life (RUL) has recently been more inclined to be studied using data-driven approaches. According to our analysis we believe that as DT technology evolves, the hybrid approach will become the mainstream strategy for DT-based modeling.  © 2023 IEEE.},
	author_keywords = {automotive; aviation; digital twin; maritime; simulation method},
	keywords = {Automotives; Aviation industry; Classifieds; Data-driven methods; Hybrid method; Maritime; Maritime industry; Model-driven; Simulation method; Three industries; Marine industry},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Green Open Access}
}

@ARTICLE{Flumerfelt2020542,
	author = {Flumerfelt, Shannon},
	title = {Leveraging system complexity for improvement},
	year = {2020},
	journal = {Total Quality Management and Business Excellence},
	volume = {31},
	number = {5-6},
	pages = {542 – 549},
	doi = {10.1080/14783363.2018.1434769},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041799688&doi=10.1080%2f14783363.2018.1434769&partnerID=40&md5=13290b644760c98ff5f71449c98f9a74},
	abstract = {The purpose of this article is to describe theoretical and current thought leadership on managing system complexity to engage improvement for better results. The literature review presented describes the current trends in systems management as a means for continuous improvement, rather than as an exercise in oversimplification of systems through reductionist strategies. This conceptualisation of holistic management of organisations is opening up pathways for best practice and emerging trends, such as through methods focused on the explicit, such as for reducing risk and increasing efficiency via model-based or simulation-based engineering, and the Digital Twin. But also through methods focused on the tacit, such as through shedding of paradigms that serve as barriers to systems management through adaptive management, standardisation of outcomes, integration structures and integrated ideation of system congruence are being used for improving results. A literature review and presentation of four case studies are provided make the case for the need and effect of managing system complexity for better results. These trends in systems thought leadership and practices in systemology open up organisational capacity to engage in enriched systems management. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.},
	author_keywords = {continuous improvement; lean; system complexity; systems management},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Čech2021,
	author = {Čech, Martin and Beltman, Arend-Jan and Ozols, Kaspars},
	title = {Pushing mechatronic applications to the limits via smart motion control},
	year = {2021},
	journal = {Applied Sciences (Switzerland)},
	volume = {11},
	number = {18},
	doi = {10.3390/app11188337},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114735077&doi=10.3390%2fapp11188337&partnerID=40&md5=8c927ae73447c41516005dc238e3ce29},
	abstract = {Modern machines strive to run at limit performance and dependability while their operational area and size are getting restricted. To achieve those objectives, often swift integration of custom-made subsystems is required, either actuators, sensors, electronic, or SW modules. Such a diverse suite of elements needs specific approaches and tools for fast optimization and adjustment following model-based system engineering (MBSE) and digital twinning principles. The large-scale I-MECH project was an industry-driven initiative striving to give a scientific response to those demands. The intermediate results were summarized in the authors’ previous work. The purpose of this paper is to report on final project results, namely specific performance achievements and figures based on measurable KPIs. After a brief description of key technologies, special focus is given to industrial printing technology based on a generic substrate carrier. However, it is shown that similar and consistent methodology can be applicable in many other industrial domains, such as semiconductors, healthcare robotics, machining, packaging, etc. Thus, the main merit of this survey is a holistic approach to motion control design. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Application specific integrated circuit (ASIC); Computer vision; Cyber-physical systems; Digital twin; Edge computing; Electronics; Industrial communication; Low-power sensing; Mechatronics; Motion control; Real-time (RT) control; Robotics; Service-oriented architecture (SOA); Smart system integration},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access, Gold Open Access}
}

@ARTICLE{Rovati2023,
	author = {Rovati, Lucrezia and Gary, Phillip J. and Cubro, Edin and Dong, Yue and Kilickaya, Oguz and Schulte, Phillip J. and Zhong, Xiang and Wörster, Malin and Kelm, Diana J. and Gajic, Ognjen and Niven, Alexander S. and Lal, Amos},
	title = {Development and usability testing of a patient digital twin for critical care education: a mixed methods study},
	year = {2023},
	journal = {Frontiers in Medicine},
	volume = {10},
	doi = {10.3389/fmed.2023.1336897},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182979325&doi=10.3389%2ffmed.2023.1336897&partnerID=40&md5=8ee93a706f8e8bf5b693b264f14f4d2f},
	abstract = {Background: Digital twins are computerized patient replicas that allow clinical interventions testing in silico to minimize preventable patient harm. Our group has developed a novel application software utilizing a digital twin patient model based on electronic health record (EHR) variables to simulate clinical trajectories during the initial 6 h of critical illness. This study aimed to assess the usability, workload, and acceptance of the digital twin application as an educational tool in critical care. Methods: A mixed methods study was conducted during seven user testing sessions of the digital twin application with thirty-five first-year internal medicine residents. Qualitative data were collected using a think-aloud and semi-structured interview format, while quantitative measurements included the System Usability Scale (SUS), NASA Task Load Index (NASA-TLX), and a short survey. Results: Median SUS scores and NASA-TLX were 70 (IQR 62.5–82.5) and 29.2 (IQR 22.5–34.2), consistent with good software usability and low to moderate workload, respectively. Residents expressed interest in using the digital twin application for ICU rotations and identified five themes for software improvement: clinical fidelity, interface organization, learning experience, serious gaming, and implementation strategies. Conclusion: A digital twin application based on EHR clinical variables showed good usability and high acceptance for critical care education. Copyright © 2024 Rovati, Gary, Cubro, Dong, Kilickaya, Schulte, Zhong, Wörster, Kelm, Gajic, Niven and Lal.},
	author_keywords = {critical care; medical education; medical intensive care unit; patient safety; patient-specific modeling; simulation training},
	keywords = {Article; controlled study; digital twin; electronic health record; human; intensive care; internal medicine; medical education; medical intensive care unit; patient safety; practice guideline; semi structured interview; simulation; simulation training; usability testing; workload},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access}
}

@CONFERENCE{Morey2021,
	author = {Morey, Philippe and Affolter, Jean-Francois and Carpita, Mauro},
	title = {Optimizing transformer parasitics in an inductor-less resonant converter},
	year = {2021},
	journal = {2021 23rd European Conference on Power Electronics and Applications, EPE 2021 ECCE Europe},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119079898&partnerID=40&md5=676d05dbd5cef97626158e86f853be6f},
	abstract = {In this paper, the optimization of a 10 kW serial MPPT DC/DC converter designed to inject solar power into a low-voltage DC-microgrid is presented. The prototype was designed as two cascaded stages; an interleaved multi-branch boost topology followed by an inductor-less resonant topology. Traditionally, resonant converters use an external inductor, separating the resonant tank from the transformer. This has practical reasons but also has the drawback of an additional component and added weight. For this reason, the design exploits the leakage inductance of the transformer. However, the first prototype was overheating even in no-load conditions.This paper presents the optimization study conducted to eliminate this overheating. The analysis and simulations performed allowed to identify and pinpoint the source of the issue, namely highlighting the role of the parasitic capacitances in the high di/dt current step in the transformer. Simulations where first done to rough out the problem and get a better understanding of the different interactions between various parasitic elements. A literature review was conducted, and a theoretical analysis method was then applied. This allowed to estimate the order of magnitude of the transformer's parasitic capacitances and helped fine tune the simulation model. Based on the performed analysis, recommendations and specifications were prepared for a new transformer. An new optimized transformer was then built and tested. The results were better than expected. The parasitics were reduced by 90%, showing that the issue had correctly been identified and addressed. © 2021 EPE Association.},
	author_keywords = {Converter circuit; High frequency power converter; Interleaved converters; Silicon Carbide (SiC); Transformer},
	keywords = {Capacitance; DC-DC converters; Electric inductors; Power electronics; Solar energy; Timing circuits; Topology; Converter circuits; High frequency power converter; Interleaved converters; Optimisations; Optimizing transformers; Parasitics; Parasitics capacitance; Resonant converters; Silicon carbide; Transformer; Silicon carbide},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Nandam2024,
	author = {Nandam, Vineela and Patel, P.L.},
	title = {A framework to assess suitability of global digital elevation models for hydrodynamic modelling in data scarce regions},
	year = {2024},
	journal = {Journal of Hydrology},
	volume = {630},
	doi = {10.1016/j.jhydrol.2024.130654},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185007054&doi=10.1016%2fj.jhydrol.2024.130654&partnerID=40&md5=58677b35d200eba3c523ec915bbb626b},
	abstract = {With increase in availability of global Digital Elevation Models (DEMs) that are derived from distinctive techniques with varying characteristics, it is essentially required to assess their accuracies prior to use them in the flood inundation modelling, especially in urban catchments. In the present study, firstly, the vertical accuracy of the global DEMs is assessed using datasets of three independent Ground Control Point (GCP) measurements while accounting for the spatial and temporal variabilities with respect to DEM data acquisitions. Next, we investigate their suitability for flood inundation modelling by developing hydrodynamic models. The accuracy of global DEMs, viz. SRTM, MERIT, AW3D30, COPDEM30, TanDEM-X 12, TanDEM-X 90, FABDEM, and CartoDEM are assessed for Lower Tapi Basin (LTB), India using ICESat-1 and ICESat-2 laser altimetry, and GNSS surveyed locations. The potential of TanDEM-X 12 DEM for deriving Digital Terrain Model (DTM) from the Digital Surface Model (DSM) has been explored by adopting Simple Morphological Filter (SMRF) and void-filling approaches. Finally, a comprehensive performance evaluation of these DEMs that includes DSMs and DTMs in flood inundation modelling is carried out by simulating the severe fluvial flood event of magnitude 25,768 m3/sec that occurred in the year 2006 in LTB through 1D-2D coupled Hydro-Dynamic (HD) models using HEC-RAS software. Prior to development of HD models using global DEMs and DTMs as terrain layer, a benchmark model has been simulated for the same event with terrain layer as topographic surveyed contours of the basin. The least flood depth error has been observed for the FABDEM model (RMSE: 1.59 m), followed by the model with terrain topography as the derived “TanDEM-X 12 DTM-AMP-HEM-EVF” (RMSE: 1.88 m). On the other hand, the HD model based on TanDEM-X 12 DTM AMP-HEM-EVF has been able to simulate flood inundation extent better than the FABDEM model for the same flood event in the basin with Critical Success Index (CSI) scores 0.91 and 0.81 respectively. The TanDEM-X 12 DTM-AMP-HEM-EVF terrain-based model indicated consistently superior results in capturing flood inundation extent, leaving behind all the DSMs whose CSI scores are found to be less than 50 %. The global DEMs that are inherently DSMs in nature are unsuitable for flood inundation modelling as they fail to capture the flood pathways in urban topographies with precision. From the systematic framework followed in the present study, it is inferred that the processed TanDEM-X 12 DEM for the removal of artifacts could be a successful replacement to obtain bare-earth topography. In the absence of TanDEM-X 12 DEM, the freely available FABDEM can be used as an alternative source for terrain layer in flood modelling as its results are comparable to the derived terrain topography of TanDEM-X 12 DEM. The study is one of the first to assess the performance of FABDEM in flood modelling of coastal urban floodplain. © 2024 Elsevier B.V.},
	author_keywords = {DEM accuracy; Digital Terrain Model; FABDEM; ICESat; Lower Tapi Basin; TanDEM-X},
	keywords = {India; Tapi Basin; Catchments; Flood control; Geomorphology; Hydrodynamics; Landforms; Surveying; Topography; Digital elevation model; Digital elevation model accuracy; Digital terrain model; Dynamics models; FABDEM; Flood inundation modeling; ICESat; Low tapi basin; Modeling accuracy; TanDEM-X; accuracy assessment; digital elevation model; digital terrain model; flood; GNSS; hydrological modeling; Shuttle Radar Topography Mission; topography; Floods},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Kocaman2024,
	author = {Kocaman, Ezgi and Kuru Erdem, Merve and Calis, Gulben},
	title = {Machine learning thermal comfort prediction models based on occupant demographic characteristics},
	year = {2024},
	journal = {Journal of Thermal Biology},
	volume = {123},
	doi = {10.1016/j.jtherbio.2024.103884},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197567566&doi=10.1016%2fj.jtherbio.2024.103884&partnerID=40&md5=6f6f59ad12881f5e00f43586f7858690},
	abstract = {This study aims to investigate the predictive occupant demographic characteristics of thermal sensation (TS) and thermal satisfaction (TSa) as well as to find the most effective machine learning (ML) algorithms for predicting TS and TSa. To achieve this, a survey campaign was carried out in three mixed-mode buildings to develop TS and TSa prediction models by using six ML algorithms (Logistic Regression, Naïve Bayes, Decision Tree (DT), Random Forest (RF), K-Nearest Neighborhood (KNN) and Support Vector Machine). The prediction models were developed based on six demographic characteristics (gender, age, thermal history, education level, income, occupation). The results show that gender, age, and thermal history are significant predictors of both TS and TSa. Education level, income, and occupation were not significant predictors of TS, but were significant predictors of TSa. The study also found that RF and KNN are the most effective ML algorithms for predicting TS, while DT and RF are the most effective ML algorithms for predicting TSa. The study found that the accuracy of TS prediction models ranges from 83% to 99%, with neutral being the most correctly classified scale. The accuracy of TSa prediction models ranges from 84% to 97%, with dissatisfaction being the most common misclassification. © 2024},
	author_keywords = {Demographic characteristics; Machine learning; Prediction model; Thermal satisfaction; Thermal sensation},
	keywords = {Adolescent; Adult; Aged; Algorithms; Female; Humans; Machine Learning; Male; Middle Aged; Thermosensing; Young Adult; adult; aged; algorithm; article; comfort; decision tree; demographics; female; heat sensation; human; human experiment; logistic regression analysis; machine learning; major clinical study; male; middle aged; neighborhood; prediction; random forest; support vector machine; temperature sense; adolescent; young adult},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Al-Alawi2023446,
	author = {Al-Alawi, Adel Ismail and Messaadia, Mourad and Mehrotra, Arpita and Sanosi, Sohayla Khidir and Elias, Hala and Althawadi, Aysha Hisham},
	title = {Digital transformation adoption in human resources management during COVID-19},
	year = {2023},
	journal = {Arab Gulf Journal of Scientific Research},
	volume = {41},
	number = {4},
	pages = {446 – 461},
	doi = {10.1108/AGJSR-05-2022-0069},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146134927&doi=10.1108%2fAGJSR-05-2022-0069&partnerID=40&md5=1f637e99e2f495b7f8c81f2d4f36db59},
	abstract = {Purpose: The purpose of this study is to discover the factors related to human resource (HR) digital transformation (DT) in Bahrain during coronavirus disease 2019 (COVID-19) and to analyze the impact of e-human resource management (e-HRM) on organizational performance. These factors are funding, information technology (IT) infrastructure, technical support, digital skills or talents, organizational culture, employee resistance and top management support. These factors were tested to determine whether they affected HR DT in Bahrain during COVID-19. There are no findings in the researched literature regarding the proposed factors that affect HR DT in Bahrain during COVID-19 in this study. Design/methodology/approach: For data collection, a quantitative method was applied by conducting an online survey and distributing it to HR executives and employees from multiple organizations in Bahrain, both in the private and public sectors. Findings: This study proposes a DT adoption model based on seven factors extracted from the literature review. A questionnaire was deployed, and accurate data were collected, processed and then analyzed. The logit model shows determinants factor to the DT adoption where all variables have a positive effect. Originality/value: Using technology in an organization’s HR practices, known as e-HRM or HR DT, is becoming more crucial since the COVID-19 pandemic. Unlike European countries, the HR sector in Bahrain is not prepared to adopt the e-HRM process. This paper proposes a model that enables the HR sector to adopt digital technologies. This model is based on the key factors that enable an effective transition to the sector’s digitalization (e-HRM). Future research is sought to provide additional insights into the same factors and measure their effect on HR DT during COVID-19 in other countries. © 2022, Adel Ismail Al-Alawi, Mourad Messaadia, Arpita Mehrotra, Sohayla Khidir Sanosi, Hala Elias and Aysha Hisham Althawadi.},
	author_keywords = {COVID-19; Digital transformation adoption; e-HRM; Human resource management (HRM); Logistic regression},
	keywords = {Bahrain; COVID-19; human resource; information technology; regression analysis; resource management; technology adoption},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Fei202449,
	author = {Fei, Cheng},
	title = {Design Thinking Models and Tools to Support the Design Process},
	year = {2024},
	journal = {Lecture Notes in Educational Technology},
	volume = {Part F2540},
	pages = {49 – 77},
	doi = {10.1007/978-981-97-0076-9_4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191343641&doi=10.1007%2f978-981-97-0076-9_4&partnerID=40&md5=e2bdda32349e76ea59d831080f246b10},
	abstract = {Design Thinking (DT) is not only about solving problems but it also delivers multi-dimension, innovative, and pragmatic perspectives which reflect users’ demands. In education, DT has been regarded as a constructivist teaching methodology that integrates both divergent thinking and aggregated thinking. DT is a process from conceiving, prototyping to evaluation and finally defining innovative solutions. Such a process should be flexible, nonlinear, and dynamic. However, it depends on the application process of the Design Thinking models. Based on literature review and case study analysis, this section (1) reviews a list of typical Design Thinking models and their processes, (2) compares and analyzes these models to describe their core attributes and thinking characteristics, and (3) introduces commonly used thinking tools to explain how they provide new solutions for teaching design and engaging students in learning. This study provides new insights for educators to explore how to build a future school. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yadav202225,
	author = {Yadav, B.K.V. and Lucieer, A. and Jordan, G.J. and Baker, S.C.},
	title = {Using topographic attributes to predict the density of vegetation layers in a wet eucalypt forest},
	year = {2022},
	journal = {Australian Forestry},
	volume = {85},
	number = {1},
	pages = {25 – 37},
	doi = {10.1080/00049158.2021.2004687},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121533556&doi=10.1080%2f00049158.2021.2004687&partnerID=40&md5=825dd17dcf9857fa2b17e46c00a506a5},
	abstract = {Mapping the structure of forest vegetation with field surveys or high-resolution light detection and ranging (LiDAR) data is costly. We tested whether landscape topography and underlying geology could predict the vegetation density of a 19 km2 area of wet eucalypt forest at the Warra Long-Term Ecological Research Supersite, Tasmania, Australia. Using spatial layers for 12 topographic attributes derived from digital terrain models (DTMs) and a geology layer, we predicted the vegetation density of three strata with a high degree of accuracy (validation root mean square error ranged from 9.0% to 13.7%). The DTMs with 30 m resolution provided greater predictive accuracy than DTMs with higher resolution. The importance of different variables depended on spatial resolution and strata. Among the predictor variables, geology generally had the highest predictive importance, followed by solar radiation. Topographic Position Index, aspect, and System for Automated Geoscientific Analyses (SAGA) Wetness Index had moderate importance. This study demonstrates that geological and topographic attributes can provide useful predictions for the density of vegetation layers in a tall wet sclerophyll primary forest. Given the good performance of the model based on 30 m DTM resolution, the predictive power of the models could be tested on a larger geographical area using lower-density LiDAR point clouds combined with medium-resolution satellite data. © 2021 Institute of Foresters of Australia (IFA).},
	author_keywords = {airborne LiDAR; digital terrain model; geology; random forest; topographic attributes; variable importance; vegetation density; wet eucalypt forest},
	keywords = {Density; Forecasts; Forestry; Geology; Plants; Resolution; Topography; Decision trees; Forecasting; Forestry; Geology; Mean square error; Topography; Vegetation; Airborne light detection and ranging; Digital terrain model; Eucalypt forest; High resolution; Light detection and ranging; Random forests; Topographic attribute; Variable importances; Vegetation density; Wet eucalypt forest; Optical radar},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Guzmán2024261,
	author = {Guzmán, Carmen and Santos, Francisco J. and Ahumada, Pedro},
	title = {Analysis of Digital Transformation in agrifood cooperatives from a gender perspective; [Análisis de la transformación digital en las cooperativas agroalimentarias desde la perspectiva de género]},
	year = {2024},
	journal = {CIRIEC-Espana Revista de Economia Publica, Social y Cooperativa},
	number = {111},
	pages = {261 – 303},
	doi = {10.7203/CIRIEC-E.111.27932},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202654935&doi=10.7203%2fCIRIEC-E.111.27932&partnerID=40&md5=83f263de17554a7edf166ae8b7f5344f},
	abstract = {Digital Transformation (DT) implies the emergence of new business models based on the widespread use of digital technologies. DT is necessary to improve productivity and access to markets, however, from a gender perspective, there is a digital divide. In this research, the objective is to study the DT from a gender perspective in a specific sector, the agri-food sector, and in a type of specific social economy entity that is fundamental in rural areas, the agri-food cooperatives. In these companies, there is still a gender gap, especially due to the lower proportion of women in the management teams and, in addition, they lag behind other companies in terms of DT. Specifically, DT is analysed in these cooperatives both globally and through the analysis of each of its dimensions: infrastructures, products, organisation, processes and customers. To this end, the study is based on data from a survey among a sample of agri-food cooperatives in Extremadura, a region with a strong specialisation in the agri-food sector and where cooperatives play an important economic, social and environmental role. The results show that the presence of women in the presidency of these cooperatives has a positive influence on the presence of women in their management teams. Furthermore, there are no significant differences in the overall DT of these co-operatives with respect to the gender factor, although there are differences in the specific dimensions of “customers” and “processes”. © (2024), (CIRIEC-Espana Revista de Economia Publica). All rights reserved.},
	author_keywords = {agri-food cooperatives; brecha digital; cooperativas agroalimentarias; digital divide; Digital transformation; gender; género; Transformación digital},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{MacAs2023223,
	author = {MacAs, Catarina and Campos, Joao R. and Lourenco, Nuno},
	title = {Understanding the Forest: A Visualization Tool to Support Decision Tree Analysis},
	year = {2023},
	journal = {Proceedings of the International Conference on Information Visualisation},
	pages = {223 – 229},
	doi = {10.1109/IV60283.2023.00047},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178503943&doi=10.1109%2fIV60283.2023.00047&partnerID=40&md5=d29215027552146202c79d74cc2873d9},
	abstract = {Decision Trees (DTs) are one of the most widely used supervised Machine Learning algorithms. The algorithm constructs binary tree data structures that partition the data into smaller segments according to different rules. Hence, DTs can be used as a learning process of finding the optimal rules to separate and classify all items of a dataset. Since the algorithm relies on a decision process similar to rule-based decisions, they are easily interpretable. However, DTs can be difficult to analyse when dealing with large datasets and/or with multiple trees, i.e. ensembles. To ease the analysis and validation of these models, we developed a visual tool which includes a set of visualizations that overview and give details of a set of trees. Our tool aims to provide different perspectives over the same data and provide further insights on how decisions are being made. In this article, we overview our design process, present the different visualization models and their iterative validation. We present a use case in the telecommunications domain. In concrete, we use the visual tool to help understand how a model based on DTs decides which is the best channel (i.e., phonecall, e-mail, SMS) to contact a client. © 2023 IEEE.},
	author_keywords = {Decision Tree; Random Forest; Visual Analytics},
	keywords = {Binary trees; Classification (of information); Decision trees; Iterative methods; Learning algorithms; Supervised learning; Visualization; Binary tree data structures; Decision process; Decision tree analysis; Learning process; Machine learning algorithms; Random forests; Supervised machine learning; Visual analytics; Visual tools; Visualization tools; Large dataset},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Li2020,
	author = {Li, Xiao and Cao, Jiarou and Liu, Zhenggang and Luo, Xinggang},
	title = {Sustainable business model based on digital twin platform network: The inspiration from haier's case study in China},
	year = {2020},
	journal = {Sustainability (Switzerland)},
	volume = {12},
	number = {3},
	doi = {10.3390/su12030936},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081212815&doi=10.3390%2fsu12030936&partnerID=40&md5=9aba7def0b60d8f45fec0275c39afaf8},
	abstract = {Sustainability and digitalization have become the main direction of transformation of enterprises. Building a digital twin platform network can provide enterprises with a comprehensive view of products, manufacture, supply chain, customer experience, and profitability, which is conducive to the construction of a sustainable business model. The purpose of this paper is to study how enterprises use digital twin platform networks to generate economic, social and environmental benefits in various dimensions and their coupling relationships. Based on the literature review, this paper constructs a five-dimensional framework of a sustainable business model, and analyses the coupling relationship between dimensions. Using Haier as a way to verify the five-dimensional framework, it explores the dynamic mechanism of the Haier digital twin platform network, constructs an integrated framework based on coupling perspective and compares it with other two home appliance enterprises. The study shows that, through the digital twin platform network, enterprises can remove the disadvantage of focusing on a single product life cycle, and form a comprehensive network, so as to promote overall sustainable upgrades. This paper draws generic strategies and digital transformation suggestions for enterprises to innovate the sustainable business model. The conclusion enriches the research on sustainable business models both theoretically and practically and provides a feasible reference for the transformation of enterprises in digital economy environment. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Coupling relationship; Digital twin platform network; Haier; Sustainable business model; Transformation mechanism},
	keywords = {China; business; innovation; integrated approach; life cycle; life cycle analysis; literature review; perception; profitability; supply chain management; sustainability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 67; All Open Access, Gold Open Access}
}

@CONFERENCE{Trauer2020,
	author = {Trauer, Jakob and Schweigert-Recksiek, Sebastian and Okamoto, Luis Onuma and Spreitzer, Karsten and Mörtl, Markus and Zimmermann, Markus},
	title = {Data-driven engineering definitions and insights from an industrial case study for a new approach in technical product development},
	year = {2020},
	journal = {Proceedings of the NordDesign 2020 Conference, NordDesign 2020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094678741&partnerID=40&md5=fe43b5dc97c20c23abae056273137e55},
	abstract = {The growing digitization affects all areas of engineering. Together with fast-paced trends, it drives complexity and uncertainty in many domains. Yet, its potentials are manifold and, in most cases, outweigh the disadvantages. Beneath terms such as "big data", "digital twin", the term "data-driven engineering" has evolved over the last years. However, neither in literature nor in industry, there is a unified definition or understanding of the term. The presented research is based on a literature review as well as an industrial case study. Several databases were screened systematically for the literature review and forward and backward searches were used additionally. The case study was conducted in a collaboration with a company in the climate system sector. First, a literature-based distinction between the terms model-based, model-driven, data-based, and data-driven as well as definitions of data-driven engineering were investigated. Representatives of the company then evaluated these findings in a workshop and together with the industry partner a consistent definition was developed. The authors define data-driven engineering as a framework for product development in which the goal-oriented collection and use of sufficiently connected product lifecycle data guides and drives decisions and applications in the product development process. Further, promising use cases for the industry partner regarding data-driven engineering were formulated. The use cases were initially evaluated and prioritized regarding their cost-benefit ratio. Symbioses with other strategies of the company such as Digital Twins, model-based engineering, and solution space engineering are outlined. For academia, the presented findings provide a consistent definition that can be used as a promising direction for future research. Especially a procedure model for the systematic conception and implementation of data-driven engineering would be beneficial. For industry, this paper provides insights on potentials of data-driven engineering, a differentiation from related concepts, and very concrete use-cases serving as a starting point for a company-specific implementation. © Proceedings of the NordDesign 2020 Conference, NordDesign 2020. All rights reserved.},
	author_keywords = {Data driven design; Design process; Internet of things (IoT); Product development; Use-phase data},
	keywords = {Concrete industry; Cost benefit analysis; Digital twin; Industrial research; Life cycle; Product development; Cost benefit ratio; Forward-and-backward; Framework for product development; Industrial case study; Literature reviews; Model-based engineering; Product development process; Product lifecycle data; Digital storage},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Ooi2024,
	author = {Ooi, Erwyn Chin Wei and Isa, Zaleha Md and Manaf, Mohd Rizal Abdul and Fuad, Ahmad Soufi Ahmad and Ahmad, Azman and Mustapa, Mimi Nurakmal and Marzuki, Nuraidah Mohd},
	title = {Factors influencing the intention to use the ICD-11 among medical record officers (MROs) and assistant medical record officers (AMROs) in Ministry of Health, Malaysia},
	year = {2024},
	journal = {Scientific Reports},
	volume = {14},
	number = {1},
	doi = {10.1038/s41598-024-60439-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191880326&doi=10.1038%2fs41598-024-60439-2&partnerID=40&md5=8ce3c708b3f3d513106162473ca69a77},
	abstract = {The transition of ICD has never been a straightforward initiative. As nations transition to ICD-11, ensuring its acceptance among the users is essential. To our knowledge, there are limited studies about the instrument and ICD-11 adoption. Therefore, the purpose of this study was to design an instrument and investigate the factors influencing the intention to use the ICD-11 among medical record officers (MROs) and assistant medical record officers (AMROs) at Ministry of Health (MOH) Malaysia facilities. Based on the current literature, a model based on the decomposed theory of planned behaviour (DTPB) was proposed. The model consisted of 13 dimensions and 12 hypotheses identified from previous studies. Using PLS-SEM, 185 survey data points were analysed. The study findings showed that ten factors have a significant impact on the suggested model. Users' subjective norm was the most influential factor in their intention to use ICD-11. Unexpectedly, perceived usefulness and was found to have no significant influence. This study is important for policymakers in strategising ICD-11 implementation efforts. This study's novelty lies in applying a DTPB theory model in the context of the intention to use ICD-11. © The Author(s) 2024.},
	keywords = {Adult; Attitude of Health Personnel; Female; Humans; Intention; International Classification of Diseases; Malaysia; Male; Middle Aged; Surveys and Questionnaires; adult; behavior; female; health personnel attitude; human; International Classification of Diseases; Malaysia; male; middle aged; questionnaire},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Madaan202065060,
	author = {Madaan, Vishu and Goyal, Anjali},
	title = {Predicting Ayurveda-Based Constituent Balancing in Human Body Using Machine Learning Methods},
	year = {2020},
	journal = {IEEE Access},
	volume = {8},
	pages = {65060 – 65070},
	doi = {10.1109/ACCESS.2020.2985717},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083731055&doi=10.1109%2fACCESS.2020.2985717&partnerID=40&md5=965060e906de8e38d09af69ef044afb6},
	abstract = {Human Body constitution (prakriti) defines what is in harmony with human nature and what will cause to move out of balance and experience illness. Tridosha defines the three basic energies or principles that determine the function of our body on the physical and emotional levels. The three energies are known as VATT, PITT and KAPH. Each individual has a unique balance of all three of these energies. Some people will be predominant in one, while others will be a mixture of two or more. Ayurveda-dosha studies have been used for a long time, but the quantitative reliability measurement of these diagnostic methods still lags behind. A careful and appropriate analysis leads to an effective treatment. To collect a meaningful data set, a questionnaire with 28 different characteristics is validated by Ayurveda experts. Authors calculate Cronbach alpha of VATT-Dosha, PITT-Dosha and KAPH-Dosha as 0.94, 0.98 and 0.98, respectively to check the reliability of the questionnaire. Authors analyzed questionnaires of 807 healthy persons aged 20-60 years and found 62.1% men and 37.9% women. The class imbalance problem is resolved with oversampling and the equally distributed data set of randomly selected 405 persons is used for the actual experiment. Using computer algorithms, we randomly divide the data set (8:2) into a training set of 324 persons and a test data set of 81 persons. Model is trained using traditional machine learning techniques for classification analysis as Artificial Neural Network (ANN), K-Nearest Neighbour (KNN), Support Vector Machine (SVM), Naive Bayes (NB) and Decision Tree (DT). System is also implemented using ensemble of several machine learning methods for constitution recognition. Evaluation measures of classification such as root mean square error (RMSE), precision, recall, F-score, and accuracy is calculated and analyzed. On analyzing the results authors find that the data is best trained and tested with CatBoost, which is tuned with hyper parameters and achieves 0.96 precision, 0.95 recall, 0.95 F-score and 0.95 accuracy rate. The experimental result shows that the proposed model based on ensemble learning methods clearly surpasses conventional methods. The results conclude that advances in boosting algorithms could give machine learning a leading future. © 2013 IEEE.},
	author_keywords = {Ayurveda; human body constituents; hyper parameter tuning; KAPH; optimized training model; PITT; VATT},
	keywords = {Adaptive boosting; Balancing; Barium compounds; Decision trees; Mean square error; Nearest neighbor search; Statistical tests; Support vector machines; Surveys; Class imbalance problems; Classification analysis; Conventional methods; K nearest neighbours (k-NN); Machine learning methods; Machine learning techniques; Reliability measurements; Root mean square errors; Learning systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 25; All Open Access, Gold Open Access}
}

@ARTICLE{El-Diasty2020,
	author = {El-Diasty, Mohammed},
	title = {Optimal lowest astronomical tide estimation using maximum likelihood estimator with multiple ocean models hybridization},
	year = {2020},
	journal = {ISPRS International Journal of Geo-Information},
	volume = {9},
	number = {5},
	doi = {10.3390/ijgi9050327},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085701646&doi=10.3390%2fijgi9050327&partnerID=40&md5=cf3541b074b7db45326876feaba37e8e},
	abstract = {Developing an accurate Lowest Astronomical Tide (LAT) in a continuous form is essential for many maritime applications as it can be employed to develop an accurate continuous vertical control datum for hydrographic surveys applications and to produce accurate dynamic electronic navigation charts for safe maritime navigation by mariners. The LAT can be developed in a continuous (surface) using an estimated LAT surface model from the hydrodynamic ocean model along with coastal discrete LAT point values derived from tide gauges data sets to provide the corrected LAT surface model. In this paper, an accurate LAT surface model was developed for the Red Sea case study using a Maximum Likelihood Estimator (MLE) with multiple hydrodynamic ocean models hybridization, namely, WebTide, FES2014, DTU10, and EOT11a models. It was found that the developed optimal hybrid LAT model using MLE with multiple hydrodynamic ocean models hybridization ranges from 0.1 m to 1.63 m, associated with about 2.4 cm of uncertainty at a 95% confidence level in the Red Sea case study area. To validate the accuracy of the developed model, the comparison was made between the optimal hybrid LAT model developed from multiple hydrodynamic ocean models hybridization using the MLE method with the individual LAT models estimated from individual WebTide, FES2014, DTU10, or EOT11a ocean models based on the associated uncertainties estimated at a 95% confidence level. It was found that the optimal hybrid LAT model accuracy is superior to the individual LAT models estimated from individual ocean models with an improvement of about 50% in average, based on the estimated uncertainties. The importance of developing optimal LAT surface model using the MLE method with multiple hydrodynamic ocean models hybridization in this paper with few centimeters level of uncertainty can lead to accurate continuous vertical datum estimation that is essential for many maritime applications. © 2020 by the author.},
	author_keywords = {Hybrid model; LAT; MLE; Ocean; Red sea},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Long2024,
	author = {Long, Wuyan and Bao, Zhikang and Chen, Ke and Thomas Ng, S. and Yahaya Wuni, Ibrahim},
	title = {Developing an integrative framework for digital twin applications in the building construction industry: A systematic literature review},
	year = {2024},
	journal = {Advanced Engineering Informatics},
	volume = {59},
	doi = {10.1016/j.aei.2023.102346},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183605156&doi=10.1016%2fj.aei.2023.102346&partnerID=40&md5=d046efc231315715c421456b0cf03815},
	abstract = {In response to the global advocacy for digitalization, numerous industrial sectors are actively advancing the implementation of digital twins (DTs). However, the construction industry has been slow to adopt DT, lagging behind sectors such as manufacturing and medicine, where digitalization has made significant strides. Given the demonstrated benefits and potential of DT, it is crucial to increase research efforts for enhancing its application in the construction industry. To accurately identify prospective areas for future research, a comprehensive review and analysis of the status quo of relevant research is urgently needed. This study undertook a systematic literature review to understand how DT can enhance operational intelligence in the building construction industry. The findings, based on a review of pertinent literature, revealed that existing studies predominantly focused on the application of DT during the construction phase and the operation and maintenance phase. However, its potential use during the planning and design phase and the demolition and recovery phase was largely overlooked. Three main categories of key digital facilitating technologies for DT in the building construction industry have been identified: (1) data-related technologies; (2) high-fidelity modeling technologies; and (3) model-based simulation technologies. These technologies have finally culminated into an integrative framework. Furthermore, three significant research gaps in the application of DT in the building construction industry have been identified, suggesting future studies explore how to (1) centralize diverse stakeholders from a lifecycle perspective; (2) address various existing technological defects; and (3) establish a universal industry standard. This paper provides a valuable roadmap for relevant stakeholders to understand the status quo of DT application and the existing deficiencies in its full implementation, which should facilitate the efficient digitalization of the construction industry. © 2023 Elsevier Ltd},
	author_keywords = {A lifecycle perspective; An integrative framework; Building construction industry; Digital twin; Systematic literature review},
	keywords = {Buildings; Construction; Life cycle; A lifecycle perspective; An integrative framework; Building construction industry; Industrial sector; Integrative framework; ITS applications; Prospectives; Research efforts; Status quo; Systematic literature review; Construction industry},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@ARTICLE{Zheng2021,
	author = {Zheng, Yang and Sun, Hongyan and Cong, Lele and Liu, Chenlu and Sun, Qian and Wu, Nan and Cong, Xianling},
	title = {Prognostic Value of ctDNA Mutation in Melanoma: A Meta-Analysis},
	year = {2021},
	journal = {Journal of Oncology},
	volume = {2021},
	doi = {10.1155/2021/6660571},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106355240&doi=10.1155%2f2021%2f6660571&partnerID=40&md5=3cef90977bab57c00ced63ad8d2f9d6a},
	abstract = {Purpose. Melanoma is the most aggressive form of skin cancer. Circulating tumor DNA (ctDNA) is a diagnostic and prognostic marker of melanoma. However, whether ctDNA mutations can independently predict survival remains controversial. This meta-analysis assessed the prognostic value of the presence or change in ctDNA mutations in melanoma patients. Methods. We identified studies from the PubMed, EMBASE, Web of Science, and Cochrane databases. We estimated the combined hazard ratios (HRs) for overall survival (OS) and progression-free survival (PFS) using either fixed-effect or random-effect models based on heterogeneity. Results. Sixteen studies including 1,781 patients were included. Both baseline and posttreatment detectable ctDNA were associated with poor OS (baseline detectable vs. undetectable, pooled HR = 1.97, 95% CI = 1.64-2.36, P<0.00001; baseline undetectable vs. detectable, pooled HR = 0.19, 95% CI = 0.11-0.36, P<0.00001; posttreatment detectable vs. undetectable, pooled HR = 2.36, 95% CI = 1.30-4.28, P=0.005). For PFS, baseline detectable ctDNA may be associated with adverse PFS (baseline detectable vs. undetectable, pooled HR = 1.41, 95% CI = 0.84-2.37, P=0.19; baseline undetectable vs. detectable, pooled HR = 0.43, 95% CI = 0.19-0.95, P=0.04) and baseline high ctDNA and increased ctDNA were significantly associated with adverse PFS (baseline high vs. low/undetectable, pooled HR = 3.29, 95% CI = 1.73-6.25, P=0.0003; increase vs. decrease, pooled HR = 4.48, 95% CI = 2.45-8.17, P<0.00001). The baseline BRAFV600 ctDNA mutation-positive group was significantly associated with adverse OS compared with the baseline ctDNA-negative group (pooled HR = 1.90, 95% CI = 1.58-2.29, P<0.00001). There were no significant differences in PFS between the baseline BRAFV600 ctDNA mutation-detectable group and the undetectable group (pooled HR = 1.02, 95% CI = 0.72-1.44, P=0.92). Conclusion. The presence or elevation of ctDNA mutation or BRAFV600 ctDNA mutation was significantly associated with worse prognosis in melanoma patients. © 2021 Yang Zheng et al.},
	keywords = {bevacizumab; circulating tumor DNA; dabrafenib; dacarbazine; immune checkpoint inhibitor; ipilimumab; mitogen activated protein kinase kinase inhibitor; nivolumab; pembrolizumab; temozolomide; trametinib; vemurafenib; adjuvant chemotherapy; adjuvant therapy; advanced cancer; Article; cancer combination chemotherapy; cancer immunotherapy; cancer palliative therapy; cancer patient; cancer prognosis; cancer survival; Cochrane Library; controlled study; droplet digital polymerase chain reaction; Embase; follow up; gene mutation; genetic association; genetic heterogeneity; genetic susceptibility; human; Medline; melanoma; meta analysis; molecularly targeted therapy; monoclonal antibody therapy; monotherapy; overall survival; progression free survival; protein blood level; survival rate; systematic review; treatment response; Web of Science},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Ma20222718,
	author = {Ma, Songhua and Hu, Kaixin and Hu, Tianliang},
	title = {Digital twins of fixtures supporting rapid design and performance tracking; [支持快速设计与性能跟踪的夹具数字孪生模型]},
	year = {2022},
	journal = {Jisuanji Jicheng Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS},
	volume = {28},
	number = {9},
	pages = {2718 – 2725},
	doi = {10.13196/j.cims.2022.09.006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164293971&doi=10.13196%2fj.cims.2022.09.006&partnerID=40&md5=19a3dc608996f184b8dad9bddc92b406},
	abstract = {To meet the requirements of clamping and positioning in the cutting of precision parts,and to support the rapid modification design of fixtures,the rapid design and performance tracking system of fixtures based on digital twins were researched and established.A high-fidelity model of the fixture-workpiece system was established based on the digital twin theory.The definition included a fixture composition model based on ontology technology,a structural model based on design theory,and an accuracy model based on Skin Model Shapes (SMS) to support rapid fixtures Modified design.Tracking the vibration data and surface quality data of the fixture during the cutting process,a dynamic perception data set was formed,and the simulation of fixture assembly accuracy and the prediction of clamping stability were realized.Through the establishment of fixture rapid design and performance tracking prototype system,the usefulness of the fixture digital twin model was verified,and the clamping stability and positioning accuracy of precision parts cutting processing under a long life cycle were realized. © 2022 CIMS. All rights reserved.},
	author_keywords = {digital twin; dynamic sensing data; fixture design; high-fidelity model},
	keywords = {Cutting; Life cycle; Dynamic sensing; Dynamic sensing data; Fixture design; High-fidelity modeling; Model-based OPC; Modification designs; Performance tracking; Precision parts; Rapid design; Sensing data; Fixtures (tooling)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Gong2020,
	author = {Gong, Shiqi and Xu, Meng and Zhang, Yiyun and Shan, Yamin and Zhang, Hao},
	title = {The Prognostic Signature and Potential Target Genes of Six Long Non-coding RNA in Laryngeal Squamous Cell Carcinoma},
	year = {2020},
	journal = {Frontiers in Genetics},
	volume = {11},
	doi = {10.3389/fgene.2020.00413},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084646869&doi=10.3389%2ffgene.2020.00413&partnerID=40&md5=b9c8f46d1ca47716aa7a85cf48ebbf89},
	abstract = {Studies have shown that long non-coding RNA (lncRNA) may act as the carcinogenic factor or tumor suppressor of laryngeal squamous cell carcinoma (LSCC). This study aims to identify the prognostic value and potential target protein-coding genes (PCGs) of lncRNAs in LSCC. The LSCC datasets were collected from The Cancer Genome Atlas (TCGA). Statistical and bioinformatic methods were used to establish and evaluate the prognostic model, identify the correlation between lncRNAs and clinical characteristics, and screen for PCGs co-expressed with lncRNAs. Weighted gene co-expression network analysis (WGCNA) identified PCG modules associated with clinical characteristics. The expression of lncRNAs and PCGs was analyzed using our LSCC patients by RT-qPCR. LINC02154, LINC00528, SPRY4-AS1, TTTY14, LNCSRLR, and KLHL7-DT were selected to establish the prognostic model. The overall survival (OS) of low-risk patients forecasted by the model was significantly better than high-risk patients. Receiver operating characteristic (ROC) curve and concordance index (C-index) validated the accuracy of the prognostic model. Chi-square test showed that six lncRNAs were associated with one of the clinical characteristics, i.e., gender, clinical stage, T and N stage, respectively. WGCNA identified PCG modules associated with gender, clinical stage, T and N stage. We took the intersection of the PCG modules of WGCNA, the differentially expressed PCGs between LSCC and normal samples, and the PCGs co-expressed with six lncRNAs. The intersection PCGs survival analysis showed that four PCGs, i.e., STC2, TSPAN9, SMS, and TCEA3 affected the OS of LSCC. More importantly, the differential expression of six lncRNAs and four PCGs between LSCC and normal samples was verified by our LSCC patients. In conclusion, we successfully established a prognostic model based on six-lncRNA RiskScore and initially screened the potential target PCGs of six lncRNAs for further basic and clinical research. © Copyright © 2020 Gong, Xu, Zhang, Shan and Zhang.},
	author_keywords = {bioinformatic analysis; laryngeal squamous cell carcinoma; long non-coding RNA; prognostic signature; WGCNA},
	keywords = {long non coding RNA klhl7 dt; long non coding RNA linc00528; long non coding RNA linc02154; long non coding RNA lncsrlr; long non coding RNA spry4 as1; long non coding RNA ttty14; long untranslated RNA; unclassified drug; adult; alcohol consumption; Article; cancer grading; cancer prognosis; cancer staging; clinical research; controlled study; female; gene expression; high risk patient; human; human tissue; immunohistochemistry; larynx squamous cell carcinoma; low risk patient; major clinical study; male; middle aged; overall survival; protein expression; real time reverse transcription polymerase chain reaction; sensitivity and specificity; smoking; survival analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; All Open Access, Gold Open Access, Green Open Access}
}

@BOOK{Jnitova2020143,
	author = {Jnitova, Victoria and Efatmaneshnik, Mahmoud and Joiner, Keith F. and Chang, Elizabeth},
	title = {Improving enterprise resilience by evaluating training system architecture: Method selection for australian defense},
	year = {2020},
	journal = {A Framework of Human Systems Engineering: Applications and Case Studies},
	pages = {143 – 183},
	doi = {10.1002/9781119698821.ch9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103572315&doi=10.1002%2f9781119698821.ch9&partnerID=40&md5=7dfdf1e2a175330882cc1df114c4e85c},
	abstract = {Australian Defense mandates a Systems Approach to Defense Learning (SADL) for training planning and management. This approach is a derivative of systems engineering (SE) methodologies that appears to be isolated from the SE community and recent developments concerning enterprise resilience. We propose to develop a model-based methodology for the Defense Training System (DTS) design and resilience measurement, implying that DTS workforce has been “engineered” into the Department of Defense Architecture Framework (DoDAF) targeting system’s resilience. This paper presents an initial phase of the research, setting a foundation of the research methodology and conceptual framework, supported by a significant system resilience literature review and DTS resilience case study formulation. Basic factors and measures for DTS resilience are proposed to guide development of the transformation road map from “As Is” (current) to “To Be” (desired) DTS architecture. There is potential for broader application of our model-based resilience framework for any enterprise seeking to be more resilient, especially through better engineering of its training systems and personnel. © 2021 by The Institute of Electrical and Electronics Engineers, Inc. All rights reserved.},
	author_keywords = {Academic literature; Defense training system; Enterprise architecture; Enterprise information systems; Model-based conceptual resilience framework},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Habib2022,
	author = {Habib, Hassan and Menhas, Rashid and McDermott, Olivia},
	title = {Managing Engineering Change within the Paradigm of Product Lifecycle Management},
	year = {2022},
	journal = {Processes},
	volume = {10},
	number = {9},
	doi = {10.3390/pr10091770},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138716644&doi=10.3390%2fpr10091770&partnerID=40&md5=97f9e3a5a14b4068c3e41d9437f74ec1},
	abstract = {Managing change in organizations is a laborious task that consumes value added time in various segments of the product lifecycle including design and development, production, delivery, and product disposition. Product lifecycle management plays an important role in minimizing the time required for managing engineering changes. This research aims to perform an extensive survey of the literature in this area. There is no consolidated review available in this area summarizing advances in engineering change management vis-à-vis product lifecycle management. Thus, the paper gives an overview of product lifecycle management-based thinking and change management. This review puts forward the most relevant research regarding the practices and frameworks developed for managing engineering change in an organization. These include model-based definition, digital twin, process-based semantic approach, service-oriented architecture, Unified Modeling Language, and unified feature modeling. The gaps between the extents of conformance to success factors have been identified as extent of integration, standardization, versatility of application, support of existing systems, and the extent of product lifecycle management support. © 2022 by the authors.},
	author_keywords = {Change Management (CM); Digital Twin (DT); Engineering Change Management (ECM); Product Lifecycle Management (PLM)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@CONFERENCE{Hasavari2023,
	author = {Hasavari, Shirin and Song, Yeong Tae and Lawner, Benjamin},
	title = {Increase Patients' Survivability During Emergency Care Using Blockchain-Based Digital Twin Technology},
	year = {2023},
	journal = {Proceedings - 2023 IEEE/ACIS 21st International Conference on Software Engineering Research, Management and Applications, SERA 2023},
	doi = {10.1109/SERA57763.2023.10456759},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187223056&doi=10.1109%2fSERA57763.2023.10456759&partnerID=40&md5=0f6babfdf9389d61ddbfa4474289d026},
	abstract = {Effective communication of patient clinical and care information between pre-hospital services and Emergency Departments (EDs) is crucial for the rapid and effective treatment of patients, potentially saving lives. Recent research indicates a frequent lack of patient data metrics, such as vital signs, which represents a potential limitation in the comprehensiveness of handoffs received by ED physicians from pre-hospital providers. To address this challenge, we propose a solution that utilizes a patient's digital twin and permissioned blockchain technology to ensure complete, real-time, secure, and shareable access to patients' vital signs and other data metrics for Emergency Departments. This solution seeks to answer the following questions: Which data model for the digital twin best represents the patient during transport, meeting the requirements of the receiving facility? How can patient digital twin data, including the history of clinical information and patient care, be shared in real-time with the receiving facility? How can EHR-compliant digital twin data be produced to satisfy the reconciliation between the Patient Digital Twin (PDT) and the Electronic Health Record (EHR)? Method: We conducted a comprehensive literature review and a series of interviews with both pre-hospital and hospital care providers to identify the problem and develop a model based on it. To address the second question, we designed an architecture that includes all parties in the care team, during and after the transport. For the third question, we explored NEMSIS data exchange standards and a Natural Language Processing (NLP) Module. Objective: Our goal is to improve the survivability of patients in emergency care through secure, effective, and real-time sharing of patient metric data between Emergency Medical Services (EMS) and Emergency Departments or Trauma Centers. This approach also aims to reduce the waiting time for patients upon arrival at the receiving facility. © 2023 IEEE.},
	author_keywords = {Blockchain-based digital twin; Emergency departments; EMS; Hyperledger Fabric; IOMT; Pre-hospital},
	keywords = {Blockchain; Electronic data interchange; Emergency services; Hospital data processing; Natural language processing systems; Block-chain; Blockchain-based digital twin; Emergency care; Emergency departments; Emergency medical services; Hyperledg fabric; IOMT; Pre-hospital; Real- time; Vital sign; Patient treatment},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hojo2020,
	author = {Hojo, Ai and Takagi, Kentaro and Avtar, Ram and Tadono, Takeo and Nakamura, Futoshi},
	title = {Synthesis of l-band sar and forest heights derived from TanDEM-X DEM and 3 digital terrain models for Biomass Mapping},
	year = {2020},
	journal = {Remote Sensing},
	volume = {12},
	number = {3},
	doi = {10.3390/rs12030349},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080915888&doi=10.3390%2frs12030349&partnerID=40&md5=14a365b86e80c0f55780f478a8b76e96},
	abstract = {In this study, we compared the accuracies of above-ground biomass (AGB) estimated by integrating ALOS (Advanced Land Observing Satellite) PALSAR (Phased-Array-Type L-Band Synthetic Aperture Radar) data and TanDEM-X-derived forest heights (TDX heights) at four scales from 1/4 to 25 ha in a hemi-boreal forest in Japan. The TDX heights developed in this study included nine canopy height models (CHMs) and three model-based forest heights (ModelHs); the nine CHMs were derived from the three digital surface models (DSMs) of (I) TDX 12 m DEM (digital elevation model) product, (II) TDX 90 m DEM product and (III) TDX 5 m DSM, which we developed from two TDX-TSX (TerraSAR-X) image pairs for reference, and the three digital terrain models (DTMs) of (i) an airborne Light Detection and Ranging (LiDAR)-based DTM (LiDAR DTM), (ii) a topography-based DTM and (iii) the Shuttle Radar Topography Mission (SRTM) DEM; the three ModelHs were developed from the two TDX-TSX image pairs used in (III) and the three DTMs (i to iii) with the Sinc inversion model. In total, 12 AGB estimation models were developed for comparison. In this study, we included the C-band SRTM DEM as one of the DTMs. According to Walker et al. (2007), the SRTM DEM serves as a DTM for most of the Earth's surface, except for the areas with extensive tree and/or shrub coverage, e.g., the boreal and Amazon regions. As our test site is located in a hemi-boreal zone with medium forest cover, we tested the ability of the SRTM DEM to serve as a DTM in our test site. This study especially aimed to analyze the capability of the two TDX DEM products (I and II) to estimate AGB in practice in the hemi-boreal region, and to examine how the different forest height creation methods (the simple DSM and DTM subtraction for the nine CHMs and the Sinc inversion model-based approach for the three ModelHs) and the different spatial resolutions of the three DSMs and three DTMs affected the AGB estimation results. We also conducted the slope-class analysis to see how the varying slopes influenced the AGB estimation accuracies. The results show that the combined use of the PALSAR data and the CHM derived from (I) TDX 12 m DEM and (i) LiDAR DTM achieved the highest AGB estimation accuracies across the scales (R2 ranged from 0.82 to 0.97), but the CHMs derived from (I) TDX 12 m DEM and another two DTMs, (ii) and (iii), showed low R2 values at any scales. In contrast, the two CHMs derived from (II) TDX 90 m DEM and both (i) LiDAR DTM and (iii) SRTM DEM showed high R2 values > 0.87 and 0.78, respectively, at the scales > 9.0 ha, but they yielded much lower R2 values at smaller scales. The three ModelHs gave the lowest R2 values across the scales (R2 ranged from 0.39 to 0.60). Analyzed by slope class at the 1.0 ha scale, however, all the 12 AGB estimation models yielded high R2 values > 0.66 at the lowest slope class (0° to 9.9°), including the three ModelHs (R2 ranged between 0.68 to 0.69). The two CHMs derived from (II) TDX 90 m DEM and both (i) LiDAR DTM and (iii) SRTM DEM showed R2 values of 0.80 and 0.71, respectively, at the lowest slope class, while the CHM derived from (I) TDX 12 m DEM and (i) LiDAR DTM showed high R2 values across the slope classes (R2 > 0.82). The results show that (I) TDX 12 m DEM had a high capability to estimate AGB, with a high accuracy across the scales and the slope classes in the form of CHM, but the use of (i) LiDAR DTM was required. On the other hand, (II) TDX 90 m DEM was able to achieve high AGB estimation accuracies not only with (i) LiDAR DTM, but also with (iii) SRTM DEM in the form of CHM, but it was limited to large scales > 9.0 ha; however, all the models developed in this study have the possibility to achieve higher AGB estimation accuracies at the 1.0 ha scale in flat terrains with slope < 10°. The analysis showed the strengths and limitations of each model, and it also indicates that the data creation methods, the spatial resolutions of datasets and topographic features affects the effective spatial scales for AGB mapping, and the optimal combinations of these features should be chosen to obtain high AGB estimation accuracies. © 2020 by the authors.},
	author_keywords = {ALOS/PALSAR; Canopy height model; Forest above-ground biomass (AGB); LiDAR; Sinc inversion model; SRTM DEM; TanDEM-X DEM},
	keywords = {Biomass; Forestry; Mapping; Optical radar; Space-based radar; Synthetic aperture radar; Topography; Tracking radar; Above ground biomass; ALOS/PALSAR; Canopy Height Models; Inversion models; SRTM DEM; TanDEM-X; Surveying},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Gold Open Access}
}

@ARTICLE{Gkeli2023,
	author = {Gkeli, Maria and Potsiou, Chryssy},
	title = {3D crowdsourced parametric cadastral mapping: Pathways integrating BIM/IFC, crowdsourced data and LADM},
	year = {2023},
	journal = {Land Use Policy},
	volume = {131},
	doi = {10.1016/j.landusepol.2023.106713},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156245607&doi=10.1016%2fj.landusepol.2023.106713&partnerID=40&md5=6bef4aa1129c419c2ed65a7fe0422e89},
	abstract = {This paper is part of a doctoral dissertation (PhD) research aligned with global trends aiming to develop practical technical tools for the collection, management and visualization of three-dimensional (3D) property rights in urban areas. Recently, the Building Information Models (BIMs) claim a prominent position in the field of 3D cadastres and the digital twins of the cities. In addition to all other building information, the BIM can also provide data about the exact boundaries of all kind of legal spaces (e.g., property ownership rights as well as land use restrictions), under the support of Industry Foundation Classes (IFCs). However, the utilization of BIMs for 3D cadastral surveys is still accompanied with two main drawbacks. The first refers to BIMs limited availability, as till now they mainly encounter in new large constructions. The second refers to the complexity in defining the exact geometric location of the legal boundaries regarding the exterior/interior partitions of the construction and of the common spaces. The latter parameter is formed on the basis of the current legal legislation in each country. Through in-depth investigation of the current legal framework of each country it may be possible to visualize the various 3D legal spaces within complex constructions and thus to facilitate the integration of existing BIMs in the development of 3D cadasters that will enable a better understanding and communication of all involved parties in the operation of cross-boundary real estate markets. In parallel, crowdsourcing has already been proved to be a powerful data collection method for the initial participatory implementation of fast, reliable and affordable 3D cadastral surveys, utilizing all capabilities provided by the latest low-cost devices, mobile services (m-services), open-source software (OSS) and the international standard of Land Administration Domain Model (LADM ISO 19152). If no precise 3D building models are already available, the currently available 2D architectural plans combined with the additional geometric and descriptive cadastral information may be utilized for a participatory crowdsourced cadastral survey of the 3D property units. In this paper a ‘two-route’ crowdsourced approach is described. This approach suggests both the use of existing BIMs – those available – to proceed with 3D crowdsourced cadastral surveys of those constructions, as well as the use of 2D georeferenced basemaps (e.g., orthophotos for the compilation of 2D crowdsourced cadastral surveys, and all existing architectural floor plans of the constructions) to proceed with 3D crowdsourced cadastral surveys of all other constructions. A database schema describing the linkage between LADM standard, BIM/IFC and 3D crowdsourced geometric and descriptive cadastral information is developed and presented. A hybrid mobile application enabling the manipulation of BIM/IFC descriptive data – if existing; the collection of 3D crowdsourced geometric and descriptive information by property owners/users/non-professionals; the registration of the cadastral data and their relationships within a LADM-based cadastral geodatabase; the automated generation of 3D property unit models as block models (LoD1), using Model-driven approach; and the objects visualization in real-time, are developed. An investigation regarding the legally correct representation of the location of property unit boundaries, focusing mainly in the Greek territory is conducted. A practical experiment for each one of the cases of the ‘two-route’ crowdsourced approach is implemented, for two multi-storey buildings in the city of Athens, Greece. The potentials of the proposed crowdsourced solution as well as the achieved geometric accuracy – in the absence of BIM – are discussed and assessed. The results show that integrating BIM data with cadastral information derived from crowdsourcing, may significantly contribute to the implementation of 3D Cadastres, providing also a better visual understanding of 3D property rights. Nonetheless, even in the absence of a BIM the achieved accuracy seems to satisfy the cadastral specifications of the Greek cadaster enhancing the potential of exploiting crowdsourced data in the initial phases of the cadastral formal procedures. © 2023},
	author_keywords = {3D cadastre; 3D modelling; BIM; Crowdsourcing; IFC; LADM},
	keywords = {Athens [Georgia]; Georgia; United States; administrative system; computer simulation; database; digital mapping; local participation; numerical model; software; three-dimensional modeling; visualization},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Hannemann2022101,
	author = {Hannemann, Izabelle and Rodrigues, Sarah and Loures, Eduardo and Deschamps, Fernando and Cestari, Jose},
	title = {Applying a decision model based on multiple criteria decision making methods to evaluate the influence of digital transformation technologies on enterprise architecture principles},
	year = {2022},
	journal = {IET Collaborative Intelligent Manufacturing},
	volume = {4},
	number = {2},
	pages = {101 – 111},
	doi = {10.1049/cim2.12046},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123801871&doi=10.1049%2fcim2.12046&partnerID=40&md5=6a2fb1cee174faaf33f9db9c5194eee0},
	abstract = {Organisations all over the world are going through the process of digital transformation (DT). Enterprise Architecture (EA) is a method and an organising principle that aligns the business's objectives and strategies with the Information Technology strategy and execution plan. EA provides a guide to direct the evolution and transformation of enterprises with technology. The EA principles are one of the key concepts in the definition of EA; they assist in recognizing the organization vision and validating the outcomes. However, the lack of adequate instruments for assessing the current state and identifying opportunities for EA management procedures improvement often leave organisations unsure of where to begin improving their procedures. The aim of this paper is to help organisations identify these improvement opportunities. To do so, a decision model was developed to evaluate the influence DT technologies have on the EA principles proposed by The Open Group Architecture Framework (TOGAF). A literature review was conducted, and five main DT Technologies applied in the EA scope were identified. With that, a decisional model was created based on two decision-making methods called Decision-Making Trial and Evaluation Laboratory and PROMETHEE. The 21 architecture principles proposed by TOGAF were evaluated and the influence the technologies exercised on the principles were identified. As a result, Big Data and Cloud Computing technologies were indicated as having the greatest effect over the analysed principles, therefore concluding that when applied in the EA scope, these technologies can help organisations improve their EA procedures. © 2022 The Authors. IET Collaborative Intelligent Manufacturing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.},
	author_keywords = {architecture principles; digital transformation; enterprise architecture; technologies; TOGAF},
	keywords = {Business objectives; Business strategy; Decision modeling; Decision-making method; Digital transformation; Enterprise Architecture; Model-based OPC; Multiple criteria decision making; Open group architecture frameworks; Transformation technologies; Decision making},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Lai20231748,
	author = {Lai, Junyu and Chen, Zhiyong and Zhu, Junhong and Ma, Wanyi and Gan, Lianqiang and Xie, Siyu and Li, Gun},
	title = {Deep Learning Based Traffic Prediction Method for Digital Twin Network},
	year = {2023},
	journal = {Cognitive Computation},
	volume = {15},
	number = {5},
	pages = {1748 – 1766},
	doi = {10.1007/s12559-023-10136-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160445078&doi=10.1007%2fs12559-023-10136-5&partnerID=40&md5=699e64dd1e388dd6eb7816ddae554401},
	abstract = {Network traffic prediction (NTP) can predict future traffic leveraging historical data, which serves as proactive methods for network resource planning, allocation, and management. Besides, NTP can also be applied for load generation in simulated and emulated as well as digital twin networks (DTNs). This paper focuses on accurately predicting background traffic matrix (TM) of typical local area network (LAN) for traffic synchronization in DTN. A survey is firstly conducted on DTN, conventional model, and deep learning based NTP methods. Then, as the major contribution, a linear feature enhanced convolutional long short-term memory (ConvLSTM) model based NTP method is proposed for LAN. An autoregressive unit is integrated into the ConvLSTM model to improve its linear prediction ability. In addition, this paper further optimizes the proposed model from both spatial and channel-wise dimensions. Particularly, a traffic pattern attention (TPA) block and a squeeze & excitation (SE) block are derived and added to the enhanced ConvLSTM (eConvLSTM) model. Comparative experiments demonstrate that the eConvLSTM model outperforms all the baselines. It can improve the prediction accuracy by reducing the mean square error (MSE) up to 10.6% for one-hop prediction and 16.8% for multi-hops prediction, compared to the legacy CovnLSTM model, with still satisfying the efficiency requirements. The further enhancement of the eConvLSTM model can additionally reduce the MSE about 2.1% for one-hop prediction and 4.2% for multi-hops prediction, with slightly degrading efficiency. The proposed eConvLSTM model based NTP method can play a vital role on DTN traffic synchronization. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Deep neural network; Digital twin network; LSTM; Traffic matrix; Traffic prediction},
	keywords = {Deep neural networks; E-learning; Efficiency; Electric loads; Information management; Learning systems; Long short-term memory; Matrix algebra; Mean square error; Digital twin network; LSTM; Means square errors; Memory modeling; Model-based OPC; Network traffic predictions; Prediction methods; Traffic matrix; Traffic prediction; TWIN networks; Forecasting},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Bronze Open Access}
}

@CONFERENCE{Diventi2023,
	author = {Diventi, Anthony and Forsbacka, Matthew and Rainbolt, Kevin and Cornford, Steven and Feather, Martin},
	title = {NASA's safety, reliability, and mission assurance digital future},
	year = {2023},
	journal = {Proceedings - Annual Reliability and Maintainability Symposium},
	volume = {2023-January},
	doi = {10.1109/RAMS51473.2023.10088205},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153176267&doi=10.1109%2fRAMS51473.2023.10088205&partnerID=40&md5=e2227fe2c12da71b44c655d731db3a1e},
	abstract = {The evolution from "document-centric"to "data-centric"and "model-centric"information leveraging structured data and model-based approaches is at the heart of digital engineering transformational efforts underway across industry and government. It is these approaches that pave the way for data lakes, Authoritative Sources of Truth (ASOTs), and systems-of-systems interoperability and the corresponding transformational benefits thereof. Such benefits include increased data availability, data access equity, data traceability, real-Time analytics, batch analytics, and (most importantly) acceleration of the time-To-value and time-To-insights associated with engineering products and analyses. The longer-Term benefits of reusability, customization and traceability are even more promising.For Safety and Mission Assurance (SMA), and Mission Success (SMS) activities; realization of such benefits is essential to provide engineers and analysts alike vital information when needed to support critical decision making throughout the entire life cycle. The SMA community often operate in parallel with engineering activities, for which information exchange with relevant context is paramount. Far too often, such information lags key decision points and/or is absent of the robust, integrated, knowledge needed, given inherent barriers associated with traditional document-centric means to data sharing, analysis, and reporting.This paper provides an overview of how NASA's Office of Safety and Mission Assurance (OSMA) is evolving its policies, standards, guidance, and training to transform to eliminate such barriers, thus realizing the benefits emerging in this new digital era. A roadmap for achieving this digital future is presented along with key building blocks involving use and implementation of concepts such as: Objectives-Hierarchies, Objective-Driven Requirements, Accepted Standards, Safety and Assurance Cases, data digitization (i.e., ontologies, structured data, and model-centric data), FAIR (Findable, Accessible, Interoperable,Reusable) and/or FAIRUST (Findable, Accessible, Interoperable, Reusable, Understandable, Secure, and Trusted) principles [1]. This paper also describes how OSMA, leveraging the Agency's overall commitment to Digital Transformation (DT), is using the power of Policy, "Digital"Domain representation, Product Evolution, and Community Outreach and Engagement as part of a strategic vision and roadmap to evolve and transform its SMA organizations to become better able to serve its stakeholders and customers. Future publications will elaborate on these building blocks and deeper concepts. © 2023 IEEE.},
	author_keywords = {Assurance-Case; Authoritative Source of Truth; Digital Transformation; Digital Twin; Safety & Mission Success; Systems Modeling Language},
	keywords = {Decision making; Interoperability; Life cycle; Metadata; NASA; Reusability; Safety engineering; Assurance case; Authoritative source of truth; Digital transformation; Mission assurances; Mission success; Safety & mission success; Safety assurance; Structured data; System modeling language; System models; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Sultana20191074,
	author = {Sultana, Jabeen and Usha Rani, M. and Farquad, M.A.H.},
	title = {Knowledge Discovery from Recommender Systems using Deep Learning},
	year = {2019},
	journal = {Proceedings of the 2nd International Conference on Smart Systems and Inventive Technology, ICSSIT 2019},
	pages = {1074 – 1078},
	doi = {10.1109/ICSSIT46314.2019.8987766},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080094956&doi=10.1109%2fICSSIT46314.2019.8987766&partnerID=40&md5=6fc38096e80c375bb0fa240650820e5a},
	abstract = {Knowledge discovery of educational data plays prominent role in the process of making decisions in order to deliver correct educational reforms. knowledge discovery can be done to extract students' sentiments towards learning behavior of the course, difficulties faced, time spent for the course duration in learning the concepts and worries or fears of students like whether they may pass or fail the final exam. As student feedback is essential to assess the effectiveness of learning technologies, the hidden knowledge of students can be discovered by conducting survey or feedback form or online course satisfaction survey at the end of the courses in order to obtain the meaningful information so that, necessary steps can be taken to improve the learning process. The prime motto of our research is to discover the knowledge from the twitter data and analyze public sentiments towards education using deep learning techniques and discovering the best technique which yields optimal results. Therefore, we propose a model based on deep learning approach to discover knowledge from educational tweets. In this paper efficiency of knowledge learnt by MLP and CNN is compared with DTREE. © 2019 IEEE.},
	author_keywords = {CNN and Classification; DTREE; EDM; knowledge discovery; MLP; Twitter educational data},
	keywords = {Data mining; Education computing; Learning systems; Social networking (online); Students; Surveys; DTREE; Educational reforms; Learning approach; Learning behavior; Learning techniques; Learning technology; Public sentiments; Twitter educational data; Deep learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Fassi20242692,
	author = {Fassi, Youssof and Heiries, Vincent and Boutet, Jerome and Boisseau, Sebastien},
	title = {Toward Physics-Informed Machine-Learning-Based Predictive Maintenance for Power Converters-A Review},
	year = {2024},
	journal = {IEEE Transactions on Power Electronics},
	volume = {39},
	number = {2},
	pages = {2692 – 2720},
	doi = {10.1109/TPEL.2023.3328438},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181158859&doi=10.1109%2fTPEL.2023.3328438&partnerID=40&md5=c79b9872e9da152ab9fbe5dce021993b},
	abstract = {Predictive maintenance for power electronic converters has emerged as a critical area of research and development. With the rapid advancements in deep-learning techniques, new possibilities have emerged for enhancing the performance and reliability of power converters. However, addressing challenges related to data resources, physical consistency, and generalizability has become crucial in achieving optimal strategies. This comprehensive review article presents an insightful overview of the recent advancements in the field of predictive maintenance for power converters. It explores three paradigms: model-based approaches, data-driven techniques, and the emerging concept of physics-informed machine learning (PIML). By leveraging the integration of physical knowledge into machine-learning architectures, PIML holds great promise for overcoming the aforementioned concerns. Drawing upon the current state-of-art, this review identifies common trends, practical challenges, and significant research opportunities in the domain of predictive maintenance for power converters. The analysis covers a broad spectrum of approaches used for parameter identification, feature engineering, fault detection, and remaining useful life estimation. This article not only provides a comprehensive survey of recent methodologies but also highlights future trends, serving as a resource for researchers and practitioners involved in the development of predictive maintenance strategies for power converters.  © 1986-2012 IEEE.},
	author_keywords = {Anomaly detection; artificial intelligence (AI); condition monitoring; digital twin; fault analysis; physics-informed machine learning (PIML); power converters; power electronics; predictive maintenance; remaining useful life (RUL)},
	keywords = {Anomaly detection; Condition monitoring; Fault detection; Learning systems; Machine learning; Maintenance; Power converters; Power electronics; Anomaly detection; Artificial intelligence; Fault analysis; Machine-learning; Physic-informed machine learning; Power electronics converters; Power-electronics; Predictive maintenance; Remaining useful life; Remaining useful lives; E-learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Ai2022,
	author = {Ai, Wenjie and Liu, Shulin and Liao, Hongping and Du, Jiaqing and Cai, Yulin and Liao, Chenlong and Shi, Haowen and Lin, Yongda and Junaid, Muhammad and Yue, Xuejun and Wang, Jun},
	title = {Application of hyperspectral imaging technology in the rapid identification of microplastics in farmland soil},
	year = {2022},
	journal = {Science of the Total Environment},
	volume = {807},
	doi = {10.1016/j.scitotenv.2021.151030},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117318713&doi=10.1016%2fj.scitotenv.2021.151030&partnerID=40&md5=0e01880c52a2db8687ef6b6548d78cc1},
	abstract = {Microplastics (MPs) are emerging environmental pollutants and their accumulation in the soil can adversely affect the soil biota. This study aims to employ hyperspectral imaging technology for the rapid screening and classification of MPs in farmland soil. In this study, a total of 600 hyperspectral data are collected from 180 sets of farmland soil samples with a hyperspectral imager in the wavelength range of 369– 988 nm. To begin, the hyperspectral data are preprocessed by the Savitzky-Golay (S-G) smoothing filter and mean normalization. Second, principal component analysis (PCA) is used to minimize the dimensions of the hyperspectral data and hence the amount of data, making the subsequent model easier to construct. The cumulative contribution rate of the first three principal components is reached 98.37%, including the main information of the original spectral data. Finally, three models including decision tree (DT), support vector machine (SVM), and convolutional neural network (CNN) are established, all of which can achieve well classification effects on three MP polymers including polyethylene (PE), polypropylene (PP), and polyvinyl chloride (PVC) in farmland soil. By comparing the recognition accuracy of the three models, the classification accuracy of DT and SVM is 87.9% and 85.6%, respectively. The CNN model based on the S-G smoothing filter obtains the best prediction effect, the classification accuracy reaches 92.6%, exhibiting obvious advantages in classification effect. Altogether, these results show that the proposed hyperspectral imaging technique identifies the soil MPs rapidly and nondestructively, and provides an effective automated method for the detection of polymers, requiring only rapid and simple sample preparation. © 2021},
	author_keywords = {Convolutional neural network; Hyperspectrum; Machine learning; Microplastics; Rapid identification; Soil},
	keywords = {Farms; Hyperspectral Imaging; Microplastics; Plastics; Soil; Technology; Chlorine compounds; Convolution; Decision trees; Farms; Hyperspectral imaging; Microplastic; Neural networks; Polypropylenes; Polyvinyl chlorides; Principal component analysis; Soil surveys; Spectroscopy; Support vector machines; microplastic; polyethylene; polypropylene; polyvinylchloride; plastic; Convolutional neural network; Farmland soils; Hyper spectra; Hyperspectral Data; Hyperspectral imaging technologies; Microplastics; Rapid identification; Savitzky-Golay; Smoothing filters; Three models; agricultural soil; artificial neural network; image classification; imaging method; machine learning; plastic waste; principal component analysis; soil pollution; spectral analysis; accuracy; agricultural land; Article; classification algorithm; convolutional neural network; decision tree; hyperspectral imaging; prediction; principal component analysis; soil analysis; soil pollution; support vector machine; agricultural land; soil; technology; Soils},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 49}
}

@ARTICLE{Ndwandwe2021247,
	author = {Ndwandwe, Duduzile and Nnaji, Chukwudi A. and Mashunye, Thandiwe and Uthman, Olalekan A. and Wiysonge, Charles S.},
	title = {Incomplete vaccination and associated factors among children aged 12–23 months in South Africa: an analysis of the South African demographic and health survey 2016},
	year = {2021},
	journal = {Human Vaccines and Immunotherapeutics},
	volume = {17},
	number = {1},
	pages = {247 – 254},
	doi = {10.1080/21645515.2020.1791509},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088537896&doi=10.1080%2f21645515.2020.1791509&partnerID=40&md5=8c184dcec1a3f113ec9a2896ad94480f},
	abstract = {Background: Socioeconomic and health inequalities remain a huge problem in post-apartheid South Africa. Despite substantial efforts at ensuring universal access to vaccines, many children remain under-vaccinated in the country. This study aimed to assess the prevalence and factors associated with incomplete vaccination in the first year of life, among children aged 12–23 months in South Africa. Methods: The study is a secondary analysis of the 2016 South African Demographic and Health Survey. A multivariable logistic regression model was applied to the data on 708 children aged 12–23 months. The study outcome, vaccination completeness, was assessed using a composite assessment of nine doses of four vaccines; Bacillus Calmette–Guérin (BCG) (one dose), Polio (four doses), diphtheria-tetanus-pertussis containing vaccines (DTP) (three doses) and measles-containing vaccines (MCV) (one dose). Children who received all the nine doses were categorized as completely vaccinated. Independent variables included child, maternal, and demographic characteristics. Variables were included in the model based on literature findings. Bivariate analyses were used to examine the crude association between each independent variable and incomplete vaccination, while the multivariable logistic regression model was used to examine the adjusted association after controlling for other variables. Measures of association were presented as odds ratios (OR) with their 95% confidence intervals (CI). Results: About two-fifths (40.8%) of the children were incompletely vaccinated. The prevalence of incomplete vaccination was significantly high among children whose mothers did not receive antenatal care (ANC) during pregnancy (57.5%), and children living in Gauteng Province (52.2%). From the bivariate analyses, the odds of being incompletely vaccinated were three times higher in children whose mothers did not attend ANC compared with children whose mothers attended ANC (crude OR = 2.93; 95% CI 1.42–6.03). The odds were about three times higher in children living in Mpumalanga province (OR = 2.58; 95% CI 1.27–5.25) and in those living in Gauteng province (OR = 2.76; 95% CI 1.30–5.91), compared with those living in Free State province. Conversely, the odds were 32% lower in children from rich households, compared with those from poor households (OR = 0.68; 95% CI 0.47–0.98). In the adjusted model, the higher odds of incomplete vaccination in children whose mothers did not attend ANC were maintained in both magnitude and direction (adjusted OR [aOR] = 2.87; 95% CI 1.31–6.25). Similarly, compared with children living in Free State province, the higher odds of a child being incompletely vaccinated in Mpumalanga (aOR = 2.30; 95% CI 1.03–5.14) and in Gauteng (aOR = 3.10; 95% CI 1.35–7.15) provinces were maintained in both magnitude and direction. Conclusions: There is a substantial burden of incomplete childhood vaccination in South Africa. Maternal ANC attendance during pregnancy and area of residence significantly influences this burden. Interventions that promote broader health service utilization, such as ANC attendance, can help improve the awareness and uptake of routine childhood vaccination. It is also imperative to take into consideration the provincial disparities in childhood vaccination completeness, in planning and implementing interventions to improve vaccination coverage in the country. © 2020 The Author(s). Published with license by Taylor & Francis Group, LLC.},
	author_keywords = {Children; coverage; determinants; factors; incomplete vaccination},
	keywords = {BCG vaccine; diphtheria pertussis tetanus vaccine; measles vaccine; poliomyelitis vaccine; adult; Article; birth order; child; child growth; childhood mortality; cross-sectional study; education; female; fertility; health survey; Hepatitis B virus; household; human; immunization; independent variable; interview; male; marriage; maternal age; maternal mortality; occupation; patient counseling; physical violence; pregnancy; prevalence; questionnaire; residential area; sexual violence; South Africa; vaccination},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Enrique202277,
	author = {Enrique, Daisy Valle and Soares, António Lucas},
	title = {Cognitive Digital Twin Enabling Smart Product-Services Systems: A Literature Review},
	year = {2022},
	journal = {IFIP Advances in Information and Communication Technology},
	volume = {662 IFIP},
	pages = {77 – 89},
	doi = {10.1007/978-3-031-14844-6_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139061435&doi=10.1007%2f978-3-031-14844-6_7&partnerID=40&md5=7f502d4678fad23f14368d76f0c6ad98},
	abstract = {Cognitive Digital Twin (CDT) has been taking considerable attention in several recent studies. CDT is considered as a promising evolution of Digital Twin bringing new smart and cognitive capabilities. Therefore, it is important to understand how companies can exploit this new technology and create new data-driven business models. Considering that context this article aims to identify Smart PSS business model based on Cognitive Digital Twin platforms. To reach this goal a literature review was conducted. As a principal contribution this study brings a set of new business models to offer Smart PSS based on cognitive digital twins. Moreover, the study presents several real cases of companies that are currently using the cognitive capabilities supplied by edge companies of the digital twin technologies. © 2022, IFIP International Federation for Information Processing.},
	author_keywords = {Business model; Cognitive Digital Twin; Digital services; Product-services systems},
	keywords = {Information services; Business models; Cognitive capability; Cognitive digital twin; Data driven; Digital services; Literature reviews; Model-based OPC; New business models; Product-service systems; Smart products; Cognitive systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Shen2020,
	author = {Shen, Po-Huai and Shao, Bao-Hua and Lo, Nan-Chang and Huang, Kai-Yi},
	title = {Evaluate the performance of ecological niche modeling for the hbitat of japanese elaeocarpus},
	year = {2020},
	journal = {ACRS 2020 - 41st Asian Conference on Remote Sensing},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107198172&partnerID=40&md5=b9b3acff3131b9bc42b4f22380ec99cd},
	abstract = {Conservation management and planning often rely on results of ecological niche modeling (ENM) to assist in new population searching, identifying, and prioritizing important biodiversity areas. We use ENM as an accessory appliance in investigation to proceed spatial extrapolation. It not only solves the problem when searching areas are inaccessible but also decreases labor cost and time cost. The study chose Elaeocarpus japonicas (Japanese Elaeocarpus, JE) as target species because JE is a sort of pioneer tree species in second succession of forest ecology. Besides, it is a superior species in middle altitude of Taiwan which widely spread through mountain ridge area and has important implications to ecologists. It used decision tree (DT), random forest (RF), maximum entropy (MAXENT) and discriminant analysis (DA) to develop ecological niche models, which incorporate topographic variables, including elevation, slope, aspect, terrain position (TP), surface curvature (SC), profile curvature (PRC), plan curvature (PLC), and global solar radiation (GSR) in a GIS. Eventually, four terrain-related variables elevation, slope, TP, and GSR were incorporated into the models based on relative importance of all predictor variables. Model calibration and evaluation for these models were implemented efficiently in the ArcGIS and SPSS software and some software modules written by Python. In the model evaluation, RF significantly outperformed (kappa value 0.74) the others with the same result as indicated in previous studies. Followed by DT and MAXENT, DT was nearly on a level with MAXENT (kappa value 0.69 and 0.67, respectively). DA was the worst but still had reasonable performance (kappa value 0.59). The four models accurately predicted the spatial distribution of JEs in Huisun Experimental Forest Station (HEFS), and substantially reduced the distribution area to less than 10% of the entire study area. As a result, they were well suited for spatial distribution modeling of JETs. In the first round of ENM iterative process, they can prioritize either the field-survey areas where it is viable to collect fine spatial-resolution microclimatic, edaphic, or biotic data for refining predictions of potential habitat in later rounds of ENM or search areas for new population discovery under the conditions of limited funding and manpower. After all, it is much difficult to predict the spatial pattern of JE species accurately since it has wide-spread, scattered distribution. Hence, the follow-up study will attempt use high-resolution DEM generated from LiDAR to derive above-mentioned terrain-related variables so that the predictive accuracy of ENMs can be improved substantially. © 2020 ACRS 2020 - 41st Asian Conference on Remote Sensing. All rights reserved.},
	author_keywords = {Decision tree (DT); Ecological niche modeling (ENM); Japanese Elaeocarpus; Maximum entropy (MAXENT); Random forest (RF)},
	keywords = {Biodiversity; Computer software; Decision trees; Discriminant analysis; Ecology; Forestry; Landforms; Maximum entropy methods; Population statistics; Remote sensing; Wages; Conservation management; Distribution models; Ecological niche modeling; Ecological niche models; Global solar radiation (GSR); High-resolution DEM; Pioneer tree species; Predictive accuracy; Spatial distribution},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Johansen2019,
	author = {Johansen, Sigrid S. and Nejad, Amir R.},
	title = {On digital twin condition monitoring approach for drivetrains in marine applications},
	year = {2019},
	journal = {Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE},
	volume = {10},
	doi = {10.1115/omae2019-95152},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075898390&doi=10.1115%2fomae2019-95152&partnerID=40&md5=248deb85cbc0942d4af4f06582e4a409},
	abstract = {A digital twin is a virtual representation of a system containing all information available on site. This paper presents condition monitoring of drivetrains in marine power transmission systems through digital twin approach. A literature review regarding current operations concerning maintenance approaches in todays practices are covered. State-of-the-art fault detection in drivetrains is discussed, founded in condition monitoring, data-based schemes and model-based approaches, and the digital twin approach is introduced. It is debated that a model-based approach utilizing a digital twin could be recommended for fault detection of drivetrains. By employing a digital twin, fault detection would be extended to relatively highly diagnostic and predictive maintenance programme, and operation and maintenance costs could be reduced. A holistic model system approach is considered, and methodologies of digital twin design are covered. A physical-based model rather than a data based model is considered, however there are no clear answer whereas which type is beneficial. That case is mostly answered by the amount of data available. Designing the model introduces several pitfalls depending on the relevant system, and the advantages, disadvantages and appropriate applications are discussed. For a drivetrain it is found that multi-body simulation is advised for the creation of a digital twin model. A digital twin of a simple drivetrain test rig is made, and different modelling approaches were implemented to investigate levels of accuracy. Reference values were derived empirically by attaching sensors to the drivetrain during operation in the test rig. Modelling with a low fidelity model showed high accuracy, however it would lack several modules required for it to be called a digital twin. The higher fidelity model showed that finding the stiffness parameter proves challenging, due to high stiffness sensitivity as the experimental modelling demonstrates. Two industries that could have significant benefits from implementing digital twins are discussed; the offshore wind industry and shipping. Both have valuable assets, with reliability sensitive systems and high costs of downtime and maintenance. Regarding the shipping industry an industrial case study is done. Area of extra focus is operations of Ro-Ro (roll onroll off) vessels. The vessels in the case study are managed by Wilhelmsen Ship Management and a discussion of the implementation of digital twins in this sector is comprised in this article. © 2019 American Society of Mechanical Engineers (ASME). All rights reserved.},
	keywords = {Arctic engineering; Costs; Electric power transmission; Fault detection; Maintenance; Marine applications; Marine engineering; Offshore oil well production; Ships; Stiffness; Experimental modelling; Maintenance approaches; Multibody simulations; Operation and maintenance; Physical based modeling; Power transmission systems; Predictive maintenance; Virtual representations; Condition monitoring},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; All Open Access, Green Open Access}
}

@ARTICLE{Gratius2023172,
	author = {Gratius, Nicolas and Hou, Yu and Bergés, Mario and Akinci, Burcu},
	title = {Lessons learned on the implementation of probabilistic graphical model-based digital twins: A space habitat study},
	year = {2023},
	journal = {Journal of Space Safety Engineering},
	volume = {10},
	number = {2},
	pages = {172 – 181},
	doi = {10.1016/j.jsse.2023.04.001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152517199&doi=10.1016%2fj.jsse.2023.04.001&partnerID=40&md5=6d64833650d10d9bdeabab6e02327367},
	abstract = {Habitats for future human spaceflights will require more resilient environmental control and life support systems (ECLSS). To that end, it is important to facilitate decision making in case of unexpected failure by quantifying the uncertain and dynamic nature of the physical phenomena involved. Combining probabilistic and deterministic models is a particularly promising approach to address this issue. In particular, Probabilistic Graphical Model (PGM) based digital twins are relevant as they embed random variables evolving overtime. Previous research used this modeling method for several applications such as monitoring structural health or manufacturing processes. We envision that the space exploration sector can also benefit from this approach by using the insight gained on specific sub-systems. In this study, we propose lessons learned on the implementation process of PGM-based Digital Twin to quantify uncertainties for temperature prognosis in ECLSS. These findings are introduced as a step-by-step guideline which result in developing a probabilistic model applicable to space habitats. We focused on directed acyclic graphs as this type of PGM can integrate expert's knowledge with data which has been proven to enhance accuracy. A literature review was conducted to identify the state-of-the-art practices and the proposed lessons learned were derived from the study of a physical infrastructure meant to predict the behavior of a space habitat. A temperature control failure scenario was considered, and the Digital Twin estimated the time available before the temperature would become critical. Experiments were conducted on three office rooms to simulate the behavior of an ECLSS. The model was trained offline using historical sensor data and performed inference online by computing the conditional probability of a multivariate normal density. We found that a successful implementation process requires to iteratively go through four stages: outline, design, calibrate and evaluate. It involves selecting ECLSS relevant functionalities and an associated decision-making problem that relies on habitability criteria. Observable variables must be chosen according to a sensors architecture that is compatible with a typical habitat infrastructure. As real space systems are not easily available for model validation, we suggest evaluating early designs on high fidelity analogs. In future work, we envisage to further assess the impact of the design stage on the model's performance by considering computational cost and inference capability. © 2023},
	author_keywords = {Digital twin; ECLSS; Probabilistic graphical model; Space habitat},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Zemtsov20241,
	author = {Zemtsov, A.N.},
	title = {Multiscale Analysis of High Resolution Digital Elevation Models Using the Wavelet Transform},
	year = {2024},
	journal = {Scientific Visualization},
	volume = {16},
	number = {2},
	pages = {1 – 10},
	doi = {10.26583/sv.16.2.01},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194096411&doi=10.26583%2fsv.16.2.01&partnerID=40&md5=16556a3873797ced4a4a1bb3cbaf26e3},
	abstract = {A technique is proposed for choosing the optimal wavelet basis in terms of decorrelation of the spectral coefficients of the wavelet basis when solving the problem of representation of digital elevation models. In the course of the work, it was revealed that the selection of the spectral transform basis significantly affects the accuracy of the representation of the original model. The proposed method to the decomposition of digital elevation models based on the discrete wavelet transform does not require large computational costs. A technique is proposed for selection the optimal wavelet basis from the position of the minimum mean square error of the reconstructed signal, when quantizing the high-frequency expansion coefficients. Expressions are obtained for generating scaling and wavelet functions in space. The method developed to represent digital elevation models has good properties, which allows to significantly increase the resolution of digital elevation models in the implemented regional geoinformation system. © 2024 National Research Nuclear University. All rights reserved.},
	author_keywords = {data visualization; digital elevation models; digital twins; discrete wavelet transform; multiresolution; terrain model; wavelets},
	keywords = {Data visualization; Digital instruments; Geomorphology; Mean square error; Signal reconstruction; Surveying; Wavelet decomposition; De correlations; Digital elevation model; Discrete-wavelet-transform; High resolution; Multi scale analysis; Multiresolution; Optimal wavelet basis; Terrain Modeling; Wavelet; Wavelets transform; Discrete wavelet transforms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@CONFERENCE{Acuña2023619,
	author = {Acuña, Alejandro and Gonzalez-Almaguer, Carlos and Vazquez, Rubén and Peñalva, Jorge and López, Camila and Corona, María Carla},
	title = {NEW AUTOMOTIVE AND AERONAUTICAL MODELS AND DESIGN OF DIGITAL TWINS TO SUPPORT LEARNING IN TEC21 EDUCATIONAL MODEL},
	year = {2023},
	journal = {Proceedings of the 25th International Conference on Engineering and Product Design Education: Responsible Innovation for Global Co-Habitation, E and PDE 2023},
	pages = {619 – 624},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185702899&partnerID=40&md5=528a609f86ce0750cd5d7c05e7e3fbad},
	abstract = {One of the takeaways from distance learning during the COVID-19 lockdown was that virtual labs and mixed-reality lessons needed to be attractively designed. The MxRP simulator based on replicating processes of an ERP system of a virtual car assembly company, models based on Meccano, were used. Surveys were carried out with students and teachers to improve virtual and augmented reality practices. As a strategy to bring the lessons to the intramural education of the Tecnologico, or Academic Extension, models of their own cars and planes were designed to take advantage of our student's creativity. The prototypes of these models will be built by 3D printing and machining through a magnet-based clamping model to replicate the same experience in both augmented and virtual reality of the assemblies. New and models previously built by our students for automotive and aeronautical competitions will be also digitized, creating digital twins for learning. The paper shows the context, planning of process of design, prototyping, and construction of these models, with the help of students and professors of the research group. The collaboration of schools of Industrial Design, Industrial Engineering, Mechanics, and Mechatronics for creating and manufacturing these models. Technological advances lead us to replicate professions through virtual and augmented reality, as well as the creation of digital twins to increase the quality, efficiency, and manufacturing of a product. © 2023 Proceedings of the 25th International Conference on Engineering and Product Design Education: Responsible Innovation for Global Co-Habitation, E and PDE 2023. All rights reserved.},
	author_keywords = {Digital twins; educational innovation; higher education; professional education},
	keywords = {3D printing; Augmented reality; E-learning; Education computing; Enterprise resource planning; Learning systems; Mixed reality; Product design; Automotives; Distance-learning; Educational innovations; Educational modelling; High educations; Mixed reality; Professional education; Support learning; Virtual and augmented reality; Virtual lab; Students},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mataei2021740,
	author = {Mataei, Behrouz and Nejad, Fereidoon Moghadas and Zakeri, Hamzeh},
	title = {Pavement maintenance and rehabilitation optimization based on cloud decision tree},
	year = {2021},
	journal = {International Journal of Pavement Research and Technology},
	volume = {14},
	number = {6},
	pages = {740 – 750},
	doi = {10.1007/s42947-020-0306-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098659494&doi=10.1007%2fs42947-020-0306-7&partnerID=40&md5=22038c60aa88d86b4d446e02779e3e91},
	abstract = {The pavement management system (PMS) consists of several components including data collection, analysis, and reporting procedures. This system helps make decisions about the prioritization of road sections and selecting optimal maintenance strategy for the related road pavement networks. Considering the deteriorating rate of pavement sections and the limited budget and resources, it is important to find the most optimal maintenance and rehabilitation (M&R) scenarios for each pavement section. This study presents a model based on the cloud decision tree (CDT) theory for selecting the most optimal M&R strategies. A CDT system is presented for Iran’s national road network. The system includes a general decision-making model and various decision trees for every province of the country. Exclusive decision-making models were presented for freeways, highways, and main roads. Furthermore, different decision tree models are presented based on roads Annual Average Daily Traffic (AADT). Using the presented theory resulted in a general model with an accuracy of 80%. Evaluation of acquired decision trees showed that fatigue cracking and International Roughness Index (IRI) are the mo st important parameters to determine the appropriate M&R scenarios. Using these parameters provided results close to the re suits of experts’ surveys under real conditions, regardless of rank and traffic volume of the road sections. © 2020, Chinese Society of Pavement Engineering. Production and hosting by Springer Nature.},
	author_keywords = {Classification; Cloud DT; Maintenance; PMS; Rehabilitation},
	keywords = {Budget control; Decision theory; Decision trees; Information management; Maintenance; Pavements; Surveys; Annual average daily traffics; Decision making models; Decision tree models; Fatigue cracking; International roughness index; Optimal maintenance; Pavement maintenance and rehabilitations; Pavement management systems; Decision making},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@ARTICLE{Busato20191105,
	author = {Busato, Laura and Boaga, Jacopo and Perri, Maria Teresa and Majone, Bruno and Bellin, Alberto and Cassiani, Giorgio},
	title = {Hydrogeophysical characterization and monitoring of the hyporheic and riparian zones: The Vermigliana Creek case study},
	year = {2019},
	journal = {Science of the Total Environment},
	volume = {648},
	pages = {1105 – 1120},
	doi = {10.1016/j.scitotenv.2018.08.179},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052151726&doi=10.1016%2fj.scitotenv.2018.08.179&partnerID=40&md5=08bd7fe591b55af32f67bb702c715e6e},
	abstract = {The hyporheic and riparian zones are critical domains in a river ecosystem since they mediate the interactions between surface water and groundwater. These domains are generally strongly heterogeneous and difficult to access; yet their characterization and monitoring still rely mostly on hard-to-perform invasive surveys that provide only point information. These well-known issues, however, can be overcome thanks to the application of minimally invasive methods. In this paper, we present the results of the hydrogeophysical characterization of the Vermigliana Creek's hyporheic and riparian zones, performed at an experimental site in the Adige catchment, northern Italy, by means of electrical resistivity tomography (ERT), distributed temperature sensing (DTS), and hydrological modeling. A major advancement is given by the placement of electrodes and of an optical fiber in horizontal boreholes at some depth below the river bed, put in place via directional drilling. The results of this static and dynamic (time-lapse) geophysical characterization identify the presence of two subdomains (the sub-riverbed and the left and right banks) and define the water flow and solute dynamics. The ERT information is then used, together with other hydrological data, to build a 3D subsurface hydrological model (driven mainly by the watercourse stage variations) that is calibrated against local piezometric information. A solute transport model is then developed to reproduce the variations observed in the dynamic geophysical monitoring. The results show good agreement between ERT data and the model outcome. In addition, the transport model is also consistent with the temperature data derived from DTS, even though some slight discrepancies show that the heat capacity of the solid matrix and heat conduction cannot be totally neglected. © 2018 Elsevier B.V.},
	author_keywords = {Distributed temperature sensing; Electrical resistivity tomography; Flow and transport modeling; Instrumentation under the riverbed; Semi-horizontal boreholes},
	keywords = {Adige Basin; Italy; Trentino-Alto Adige; Trento; Vermigliana Creek; Boreholes; Catchments; Ecosystems; Electric conductivity; Flow of water; Geophysics; Heat conduction; Hydraulics; Optical fibers; Rivers; Specific heat; Temperature sensors; Water supply; ground water; surface water; Distributed temperature sensing; Electrical resistivity tomography; Flow and transport; Flow modelling; Hydrogeophysical; Hyporheic zone; Instrumentation under the riverbed; Riparian zones; Semi-horizontal borehole; Transport modelling; borehole; electrical resistivity; flow modeling; hydrological modeling; hyporheic zone; instrumentation; monitoring; riparian zone; river bed; temperature gradient; tomography; analytic method; Article; calibration; catchment; distributed temperature sensing; electrical resistivity tomography; environmental monitoring; fiber optics; hydrogeophysics; hydrology; hyporheic zone; Italy; priority journal; riparian ecosystem; river ecosystem; solute; transport kinetics; water flow; Groundwater},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 36}
}

@ARTICLE{Rau2024285,
	author = {Rau, Maulana Ibrahim and Julzarika, Atriyon and Yoshikawa, Natsuki and Nagano, Takanori and Kimura, Masaomi and Setiawan, Budi Indra and Ha, Lan Thanh},
	title = {Application of topographic elevation data generated by remote sensing approaches to flood inundation analysis model},
	year = {2024},
	journal = {Paddy and Water Environment},
	volume = {22},
	number = {2},
	pages = {285 – 299},
	doi = {10.1007/s10333-023-00967-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184464128&doi=10.1007%2fs10333-023-00967-1&partnerID=40&md5=f50f862305598028259e986c524471ec},
	abstract = {High-resolution topographic data are crucial for delta water management, such as hydrological modeling, inland flood routing, etc. Nevertheless, the availability of high-resolution topographic data is often lacking, particularly in low-lying regions in developing countries. This data scarcity poses a significant obstacle to inland flood modeling. However, collecting detailed topographic data is demanding, time-consuming, and costly, making remote sensing techniques a promising solution for developing flood inundation analysis models worldwide. This study presents a novel understanding for utilizing topographical elevations obtained using remote sensing techniques to create a flood inundation analysis model. In a study of three watersheds, Kameda, Niitsu, and Shirone (Japan), the assessment of digital terrain models (DTMs) showed that remote sensing-based DTMs (RS-DTMs) exhibited high reliability of coefficient of determination (R2) and root-mean-square errors, compared with the airborne LiDAR-based topography from the Geospatial Information Authority of Japan. Comparing the flood modeling results from LiDAR data and RS-DTM, with Kameda and Niitsu performing favorable outcomes, Shirone exhibited less accurate results. We hypothesized that this was caused by the topographic distortions due to lack of evenly distributed reference points. Hence, we revised the topography by adjusting both the slope and intercept from the regression equation. This verification successfully showed that the flood inundation volume correlation improved, achieving R2 results for the three watersheds ranging from 0.975 to 0.997 and Nash–Sutcliffe Efficiencies ranging from 0.938 to 0.986 between the resulting flood models based on the LiDAR data and RS-DTM. Based on these findings, we recognized the significance of uniformly distributed geodetic height points. In areas lacking height references, high-precision survey instruments can be employed for achieving uniform distribution. © The International Society of Paddy and Water Environment Engineering 2024.},
	author_keywords = {DTM; Flood inundation model; Remote sensing; Topography},
	keywords = {Japan; Developing countries; Flood control; Floods; Mean square error; Optical radar; Remote sensing; Water management; Watersheds; Analysis models; Data sensing; Digital terrain model; Elevation data; Flood inundation modeling; Flood modeling; High resolution; Remote sensing techniques; Remote-sensing; Topographic data; digital elevation model; flood routing; hydrological modeling; remote sensing; slope; topography; water management; Topography},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Chen2023139,
	author = {Chen, Yu and Jupp, Julie R.},
	title = {Challenges to requirements management in complex rail transport projects},
	year = {2023},
	journal = {International Journal of Product Lifecycle Management},
	volume = {15},
	number = {2},
	pages = {139 – 177},
	doi = {10.1504/IJPLM.2023.135336},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179789926&doi=10.1504%2fIJPLM.2023.135336&partnerID=40&md5=82dbfc250d8d877a34727efd1f54dc63},
	abstract = {This paper presents a study of requirements management practices on rail transport projects, focusing on the planning and acquisition stages. Linear rail networks are comprised of connected cyber-physical systems. As the transport sector embraces digital twinning in support of more strategic approaches to asset lifecycle management, requirements management functions must also handle a complex of asset information requirements. High levels of integration in complex project environments present significant challenges for requirements management. Using literature and interview surveys, challenges to requirements management practices are investigated. Findings highlight deficiencies in enterprise-wide requirements management methods and tools, with challenges compounded by deficiencies in consistent requirements modelling, common data models, and interoperable toolchains to support continuous, integrated model-based requirements management workflows. Findings identify opportunities to integrate systems and digital engineering functions through alignment of data and processes, and increase systems engineering expertise to improve requirements traceability and change management across system review activities. Copyright © 2023 Inderscience Enterprises Ltd.},
	author_keywords = {asset information requirements; challenges; complex rail transport systems; cyber-physical systems; digital engineering; requirements management; systems engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Gagula2023109,
	author = {Gagula, A.C. and Bolanio, K.P. and Bermoy, M.M. and Salupado, C.A. and Arayan, M.G.},
	title = {INTEGRATING GEOSPATIAL TECHNIQUES AND UAS TECHNOLOGY TO UPDATE LIDAR DTM FOR FLOOD MODELING IN LAS NIEVES, AGUSAN DEL NORTE, PHILIPPINES},
	year = {2023},
	journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
	volume = {48},
	number = {4/W6-2022},
	pages = {109 – 116},
	doi = {10.5194/isprs-archives-XLVIII-4-W6-2022-109-2023},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148759412&doi=10.5194%2fisprs-archives-XLVIII-4-W6-2022-109-2023&partnerID=40&md5=928743e94c509dbc59d97944ca41e719},
	abstract = {The Digital Terrain Model (DTM) is essential in generating the topographic structure of an area by eliminating its external features. Conventional survey techniques are still employed to obtain accurate geographic data on the earth's surface. However, accurate land surveys are now achievable because of the development of Unmanned Aerial Vehicles (UAVs). This study aims to update the LiDAR DTM for Flood Modeling in Las Nieves, Agusan del Norte, using GNSS and UAS integration. The Static GNSS survey was carried out to collect precise points for the direct georeferencing of the DJI Phantom 4 GNSS-RTK UAV. There is a continuous investigation of the influence of flight parameters in creating DTM. Hence, this study also evaluates the effects of overlap percentage and flight altitude on the quality of the generated DTM. There were 16 flight plans prepared using various combinations of flight parameters. The UAS data collected was processed using the Structure from Motion. The quality of the DTM was assessed based on its accuracy and level of completeness to identify the optimal parameters for generating the data model. Based on the results of the accuracy and completeness of the DTMs, the optimal parameters for generating are 90% and 120 meters. Subsequently, the UAS-based DTM generated using this combination of flight parameters was utilized in updating the existing DTM of Las Nieves to create the flood model of the area. The flood model was generated using the hydrologic and hydraulic modeling of the HEC RAS Mapper.  Copyright © 2023 A. C. Gagula et al.},
	author_keywords = {DTM; Flight parameters; Flood Depth Map; Structure from Motion; UAS},
	keywords = {Antennas; Floods; Global positioning system; Unmanned aerial vehicles (UAV); Aerial vehicle; Depthmap; Digital terrain model; Flight parameters; Flood depth map; Flood modeling; Geospatial techniques; Optimal parameter; Structure from motion; UAS; Optical radar},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Huo20225547,
	author = {Huo, Huaying and Chang, Yigang and Tang, Yu},
	title = {Analysis of treatment effect of acupuncture on cervical spondylosis and neck pain with the data mining technology under deep learning},
	year = {2022},
	journal = {Journal of Supercomputing},
	volume = {78},
	number = {4},
	pages = {5547 – 5564},
	doi = {10.1007/s11227-021-03959-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115860831&doi=10.1007%2fs11227-021-03959-2&partnerID=40&md5=b20e72154cf6a673a1e32c06358ce3f8},
	abstract = {This study was to explore the value of data mining model in evaluating the treatment effect of acupuncture on patients with cervical spondylosis (CS) and neck pain. A total of 270 patients with CS and neck pain recruited in the acupuncture clinic of Shanxi Provincial People’s Hospital were selected as the research objects in this study and were divided into an acupuncture needle group (group A), an acupoint latent acupuncture group (group B), and a puncture bloodletting group (group C) randomly, with 90 cases in each group. The Northwick Park Questionnaire (NPQ), McGill Pain Questionnaire (MPQ), and Role-Physical (RP), Physiological Function (PF), General Health (GH), and Body Pain (BP) of all patients were recorded before treatment, at the completion of the fifth acupuncture, at the end of treatment, one-month follow-up, two-month follow-up, and three-month follow-up to analyse the clinical treatment effect. Based on the artificial neural network (ANN) algorithm, a curative effect evaluation method and data mining model was further established to compare the accuracy rate of data processing by different data models, and the data processed by the data mining model were compared with the clinical data to analyse the feasibility of the data mining model. The test results found that the NPQ and MPQ values of patients in group B were significantly lower than those in groups A and C (P < 0.05). At the end of treatment, the PF and BP values of patients in groups B and C were significantly higher than those of group A (P < 0.05). The RP value of patients in group B was significantly higher than that in groups A and C after treatment and during follow-up (P < 0.05). The GH value of patients in group B was significantly higher than that in groups A and C (P < 0.05). The clinical treatment efficiency in group B was significantly higher than that in the other two groups (P < 0.05). Different sample sizes of decision tree (DT-1), clustering algorithm (CA-1), and support vector machines (SVM-1) had no effect on the accuracy rate of efficacy judgement of the model. Moreover, the accuracy rate of DT-2, CA-2, and SVM-2 using the neighbourhood learning algorithm increased as increase of the sample size. The correct rate of treatment efficiency judgement of the SVM model and the data mining model reached the maximum value of 81.48% and 82.64%, respectively. It suggested that the data mining model based on the ANN algorithm could accurately estimate and evaluate the overall efficacy data of patients with CS and neck pain. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Acupuncture; Artificial neural network algorithm; Cervical spondylosis and neck pain; Data mining technology; Evaluation of clinical treatment efficiency},
	keywords = {Clustering algorithms; Data mining; Decision trees; Deep learning; Efficiency; Neural networks; Surveys; Artificial neural network algorithm; Cervical spondylosis; Cervical spondylosis and neck pain; Clinical treatments; Data mining models; Data mining technology; Evaluation of clinical treatment efficiency; Follow up; Neck pains; Treatment efficiency; Acupuncture},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@ARTICLE{Abbasian2022,
	author = {Abbasian, Hassan and Solgi, Eisa and Mohsen Hosseini, Seyed and Hossein Kia, Seyed},
	title = {Modeling terrestrial net ecosystem exchange using machine learning techniques based on flux tower measurements},
	year = {2022},
	journal = {Ecological Modelling},
	volume = {466},
	doi = {10.1016/j.ecolmodel.2022.109901},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124285985&doi=10.1016%2fj.ecolmodel.2022.109901&partnerID=40&md5=2f35b29d58f59bb3b7e469cfdfabe834},
	abstract = {Identifying the complex relationships of Net Ecosystem Exchange (NEE) of CO2, as an underlying factor of land surface, and atmosphere interactions is extremely important to the dynamic of carbon fluxes. Assessment of the model-based estimation of land-atmosphere carbon flux across various plant functional types (PFTs) can support the accurate identification of the carbon cycle and the adaptation and mitigation of climate change programs. Five different machine learning methods named Multiple Linear Regression (MLR), Support Vector Machine (SVM), Decision Tree (DT), Gradient Boosting Machine (GBM) and Random Forest (RF) were used to predict daily NEE magnitude. In this study, 24 sites classified into four PFTs of Deciduous Broadleaf Forest (DBF), Evergreen Needle-leaf Forest (ENF), Mixed Forest (MF) and Grassland (GRA) were examined through ground-based flux tower data. The numbers of sites were six, four, six and eight for DBF, ENF, MF and GRA respectively, while measurement periods varied from two to thirteen years. The model calibration and validation were carried out using 70%and 30% of the data-set, respectively. The models’ performances were assessed using statistical indices including the coefficient of determination (R2), the Nash-Sutcliffe efficiency (NSE), bias error (Bias) and root mean square error (RMSE) through Python software. Based on statistical indices, the models showed different levels of capability when analyzing data from the DBF, ENF, MF and GRA sites. Among the models, RF showed the best performance, MLR showed the poorest performance, while SVM, GBM and DT models all had moderate responses. The effect of both air and soil temperatures, as the state variables, were examined to assess model performance. Whether soil temperature is included in the model plays a more important role in the performance of the models in grassland than in forest. Soil temperature inclusion, as an input variable, improved the models’ performance about 14% in grassland, while it improved performance 2.4%, 2.4% and 3.5% in ENF, MF and DBF, respectively. Finally, to assess the models' performances, the NEE behavior in terms of over- or under- estimation was investigated across each PFT and over various phenological periods. The results indicate that high uncertainty occurs between the 140th and 220th days of the Julian calendar for forested areas and between the 120th and 210thdays for grassland. © 2022},
	author_keywords = {Carbon fluxes; Eddy covariance; Phenological periods; Plant functional type; Soil temperature; State variables},
	keywords = {Adaptive boosting; Carbon; Climate change; Climate models; Computer software; Decision trees; Ecosystems; Forestry; Linear regression; Mean square error; Random forests; Soil surveys; Soils; Support vector regression; Broadleaf forest; Carbon fluxes; Eddy covariance; Mixed forests; Modeling performance; Net ecosystem exchange; Phenological period; Plant functional type; Soil temperature; State-variables; adaptive management; carbon sequestration; eddy covariance; evergreen forest; functional role; grassland; ground-based measurement; land-atmosphere interaction; machine learning; mitigation; mixed forest; net ecosystem exchange; regression analysis; soil temperature; support vector machine; Temperature},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@ARTICLE{Ota2019687,
	author = {Ota, Ryosaku and Ishii, Hiromu and Tsuda, Masahiro and Higuchi, Yuriko and Yamashita, Fumiyoshi},
	title = {A model-based comparative meta-analysis of the efficacy of dolutegravir-based and efavirenz-based regimens in HIV-infected patients},
	year = {2019},
	journal = {Journal of Infection and Chemotherapy},
	volume = {25},
	number = {9},
	pages = {687 – 694},
	doi = {10.1016/j.jiac.2019.03.015},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064268400&doi=10.1016%2fj.jiac.2019.03.015&partnerID=40&md5=a9b3c552e6e548faefa47868cfbc6dab},
	abstract = {Currently, combinations of typical types of antiretroviral agents have been adopted as chemotherapy for human immunodeficiency virus (HIV) infection, comprising two nucleoside analogue reverse transcriptase inhibitors plus one of a non-nucleoside reverse transcriptase inhibitor, an integrase strand-transfer inhibitor, and a protease inhibitor. Although several meta-analyses have been conducted to determine first-line combination antiretroviral therapy, this has yet to be confirmed due to the technical limitation associated. In the present study, we applied a model-based meta-analysis (MBMA) approach, because it allows integration of information from clinical trials with varying dosing, duration, and sampling time points, resulting in enlargement of available data sources. We performed a bibliographic search to identify clinical trials involving dolutegravir (DTG)-based and efavirenz (EFV)-based regimens in HIV-infected, antiretroviral therapy-naïve adults, and then identified 30 independent trial data. The time course of drug effect was described by a consecutive first-order kinetic model and analyzed using the nonlinear mixed effect modeling approach. The developed model suggests that the DTG-based regimen provides a faster-acting and more sustainable drug effect than the EFV-based regimen. Moreover, the drug effect tends to appear more slowly and decay faster in severe patients having higher viral load or smaller baseline CD4 count. © 2019 Japanese Society of Chemotherapy and The Japanese Association for Infectious Diseases},
	author_keywords = {Combination antiretroviral therapy; Dolutegravir; Efavirenz; Model-based meta-analysis; Nonlinear mixed effect modeling},
	keywords = {Adult; Anti-Retroviral Agents; Antiretroviral Therapy, Highly Active; Benzoxazines; Heterocyclic Compounds, 3-Ring; HIV Infections; Humans; Meta-Analysis as Topic; Models, Theoretical; abacavir; dolutegravir; efavirenz; emtricitabine; lamivudine; tenofovir disoproxil; antiretrovirus agent; benzoxazine derivative; dolutegravir; efavirenz; fused heterocyclic rings; antiretroviral therapy; Article; Cochrane Library; comparative effectiveness; human; Human immunodeficiency virus infection; Medline; meta analysis; randomized controlled trial (topic); systematic review; adult; highly active antiretroviral therapy; Human immunodeficiency virus infection; meta analysis (topic); procedures; theoretical model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Madubuike2022145,
	author = {Madubuike, Obinna C. and Anumba, Chimay J. and Khallaf, Rana},
	title = {A REVIEW OF DIGITAL TWIN APPLICATIONS IN CONSTRUCTION},
	year = {2022},
	journal = {Journal of Information Technology in Construction},
	volume = {27},
	pages = {145 – 172},
	doi = {10.36680/j.itcon.2022.008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129248492&doi=10.36680%2fj.itcon.2022.008&partnerID=40&md5=8aed7924cd913ab30c9316c9f239113c},
	abstract = {The emergence of digital twin technology presents tremendous opportunities for several industry sectors. A digital twin is defined as the virtual representation of a physical asset that collects and sends real-time information. A digital twin collects data from the physical asset in real-time and uses this data to create a virtual model of the physical object. Its functionality depends on the bi-directional coordination of data between the physical and virtual models. This is likened to cyber-physical systems, which seek to provide bi-directional coordination between the physical and virtual worlds. While digital twins have found applications in the various industrial sectors such as aerospace, manufacturing, and industrial engineering, their applications in the construction industry are relatively limited. Although some level of progress has been made in the construction industry with the application of a digital twin, it still lags in other sectors. Virtual models of constructed facilities are developed and used to plan and construct the actual facility, with changes in the physical facility being automatically reflected in the virtual model based on real-time data and vice-versa. The digital twin shows promising possibilities in the design, construction, operation, and maintenance of a facility. This paper reviews the development and implementation of digital twin technology in the construction industry and compares its use with other industries while assessing the benefits of DT to the construction industry. A systematic literature review including a thematic analysis was employed to address the purpose of this study. Limitations associated with the existing and emerging applications are also identified. It concludes by highlighting the importance of DT applications in the construction sector.  © 2022 The author(s).},
	author_keywords = {Bi-directional; Cyber-Physical Systems; Data Analytics; Digital Twin; Real-Time; Virtual Model},
	keywords = {Construction industry; Cyber Physical System; Embedded systems; Real time systems; Virtual reality; Bi-directional; Data analytics; Industry sectors; Physical assets; Physical modelling; Physical objects; Real- time; Real-time information; Virtual models; Virtual representations; Data Analytics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 54; All Open Access, Gold Open Access}
}

@ARTICLE{Mendes2022141,
	author = {Mendes, Thiago Augusto and de Sousa, Marlon Barbosa and Pereira, Sávio Aparecido Dos Santos and Dos Santos, Kamila Almeida and Formiga, Klebber Teodomiro Martins},
	title = {Use of the HEC-RAS model based on LiDAR information for urban flood assessment; [Uso do modelo HEC-RAS com base em informações de LiDAR para avaliação de inundações urbanas]},
	year = {2022},
	journal = {Engenharia Sanitaria e Ambiental},
	volume = {27},
	number = {1},
	pages = {141 – 157},
	doi = {10.1590/S1413-415220200276},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127709852&doi=10.1590%2fS1413-415220200276&partnerID=40&md5=0195cfdf4e12cf33658aef3ca3ad1a6f},
	abstract = {The disordered growth of cities and the excessive waterproofing are problematic of the large urban centers, having as a main consequence the occurrence of floods and overflows. Within this context, hydrodynamic modeling can be an important tool for the determination of floodable areas, allowing the estimation of flood quotas for different scenarios of return periods (TR) and project rainfall, thus allowing to more accurately represent reality and minimize errors arising from hydraulic designs. Working with Geographic Information System (GIS), where the channel geometry is extracted using the high precision Digital Terrain Model (DTM) generated by LiDAR (Light Detection and Ranging) survey and hydrodynamic modeling software (HEC-RAS), it was possible to evaluate different flood scenarios in the channeled concrete section, in the Botafogo Stream in Goiânia, Goiás. With the results of hydrodynamic modeling, it was possible to evaluate the propagation behavior of the generated flows, finding that for precipitation with TR 50 years or older, the plumbing limits do not support the generated and transported volumes, causing flooding in six critical stretches. The data obtained by HEC-RAS could be validated from photographic records released by the press and topographic survey of flooded sites, so that the integration between GIS and hydrodynamic modeling proved to be efficient for the study of floodable areas. © 2022 Associação Brasileira de Engenharia Sanitária e Ambiental.},
	author_keywords = {hydrological anhydrodynamic modeling; urban drainage; urbanization effects},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access}
}@ARTICLE{Korokhin2018120,
	author = {Korokhin, Viktor and Velikodsky, Yuri and Shkuratov, Yuriy and Kaydash, Vadym and Mall, Urs and Videen, Gorden},
	title = {Using LROC WAC data for Lunar surface photoclinometry},
	year = {2018},
	journal = {Planetary and Space Science},
	volume = {160},
	pages = {120 – 135},
	doi = {10.1016/j.pss.2018.05.020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048146170&doi=10.1016%2fj.pss.2018.05.020&partnerID=40&md5=263837f1ef71962cc1284e7e43d63d43},
	abstract = {All available lunar digital elevation models, e.g., SELENE LALT DEM, SELENE DTM LISM, Chang'E-1 LAM DEM, GLD100, SLDEM2015, have certain disadvantages, including insufficient resolution and/or the presence of defects as well as mismatching reference coordinate systems, making it difficult to incorporate the topographic effect on photometric LROC WAC observations. We here propose a photoclinometry technique that can be used to account for this effect. To do so, we modify our algorithm used to construct seamless photometric mosaics (Korokhin et al., PSS 2016, 122, 70–87) to determine local slopes simultaneously with parameters of photometric function during the mosaicing procedure. This technique can be useful for improvement of quality of remote sensing of surfaces with complex topography. We also develop a new algorithm for constructing the lunar digital elevation model based on the simultaneous use of laser altimetric measurements (LRO LOLA) and local longitudinal slopes obtained photoclinometrically from LROC WAC data. The algorithm provides a digital elevation model with accuracy and resolution not worse than SLDEM2015, yet demonstrating significantly fewer defects and artifacts. High-quality topo data can be useful for geology, geomorphology and for navigation/exploration/mission planning. © 2018 Elsevier Ltd},
	author_keywords = {Digital Elevation Model (DEM); Lunar Orbiter Laser Altimeter (LOLA); Lunar Reconnaissance Orbiter (LRO); Moon; Photoclinometry; Photometry; Surface; Wide Angle Camera (WAC)},
	keywords = {Aneroid altimeters; Digital instruments; Forestry; Geomorphology; Lunar missions; Moon; Orbits; Photometry; Remote sensing; Surfaces; Digital elevation model; Laser altimeter; Lunar reconnaissance orbiters; Photoclinometry; Wide angle cameras; Surveying},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@CONFERENCE{San20083,
	author = {San, D. Koc and Turker, M.},
	title = {Automatic building extraction from high resolution satellite images for map updating: A model based approach},
	year = {2008},
	journal = {Proceedings of the Urban and Regional Data Management - UDMS Annual 2007},
	pages = {3 – 13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-60749136879&partnerID=40&md5=b2e31b58d5f43651e8d8c47430076c40},
	abstract = {An approach was developed for automatically updating the buildings of an existing vector database from high resolution satellite images using spectral image classification, Digital Elevation Models (DEM) and the model-based extraction techniques. First, the areas that contain buildings are detected using spectral image classification and the normalized Digital Surface Model (nDSM). The classified output provides the shapes and the approximate locations of the buildings. However, those buildings that have similar reflectance values with the other classes were not able to be detected. Therefore, nDSM was generated by subtracting the Digital Terrain Model (DTM) from the Digital Surface Model (DSM). Next, the buildings were differentiated from the trees by using the Normalized Difference Vegetation Index (NDVI). Areas other than the buildings are excluded from further processing. The buildings that exist in the vector database but missing in the image were detected through analyzing the results of the classification and nDSM. Finally, the buildings constructed after the date of the compilation of the existing vector database were extracted through the proposed model-based approach and the vector database was updated with the new building boundaries. The method was implemented in a selected urban area in Ankara, Turkey using the IKONOS pan-sharpened and panchromatic images. The results show that the proposed approach is quite satisfactory for detecting and delineating the buildings from high resolution space images. © 2008 Taylor & Francis Group.},
	keywords = {Geomorphology; Image analysis; Image classification; Reflection; Surveying; Vectors; Vegetation; Ankara , turkeys; Automatic building extractions; Digital elevation models; Digital surface models; Digital terrain models; Extraction techniques; High resolution satellite images; High resolutions; Model-based; Model-based approaches; Normalized difference vegetation indices; Pan-sharpened; Panchromatic images; Reflectance values; Space images; Urban areas; Model buildings},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kasprzak201745,
	author = {Kasprzak, Marek and Sobczyk, Artur},
	title = {Searching for the void: Improving cave detection accuracy by multi-faceted geophysical survey reconciled with LiDAR DTM},
	year = {2017},
	journal = {Zeitschrift fur Geomorphologie},
	volume = {61},
	pages = {45 – 59},
	doi = {10.1127/zfg-suppl/2017/0327},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036516161&doi=10.1127%2fzfg-suppl%2f2017%2f0327&partnerID=40&md5=60b4445ab547ad0a998ea2336ff8550a},
	abstract = {This paper presents results of last studies in karst area of the Kleśnica Valley in the Sudetes (SW Poland), known for spectacular discoveries of new cave passages in 2012 - 2014. We used digital terrain model based on airborne laser scanning data and geophysical measurements (ground penetrating radar GPR and 2-dimensional electrical resistivity tomography ERT), combined with detailed underground mapping, representing a complex multi-faceted approach for resolving karst voids distribution issue. The combination of multisource data allowed us to reconcile the structural control on marbles cavities extent and point out the relationship between cave conduits pattern and topography. LiDAR data allowed analysis of the geomorphological features of slopes and detection of lithological boundaries. ERT inversion models supported a precise imaging of underground conduits levels in the Niedźwiedzia Cave, moreover indicating some potential locations for the new passages ca. 100 -120 m above the Kleśnica Valley floor and farther to the south. Likewise, the GPR method resulted in a satisfactory accuracy of cave chambers detection in the depth range of 15 - 20 metres. Presented methodology resulted in karst voids geophysical imaging, explicitly improving our understanding of the Niedźwiedzia Cave passages distribution, both known ones as well as not recognized nor explored by speleologists hitherto. © 2017 Gebrüder Borntraeger Verlagsbuchhandlung, Stuttgart, Germany.},
	author_keywords = {Electrical resistivity tomography; Ground penetrating radar; LiDAR DTM; Niedźwiedzia Cave; Sudetes},
	keywords = {Poland [Central Europe]; Sudetes; Nica; accuracy assessment; cave; digital terrain model; electrical resistivity; geophysical survey; ground penetrating radar; karst; lidar; tomography},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@CONFERENCE{Tu20101773,
	author = {Tu, Chun-Hao and Lo, Nan-Jang and Chang, Wei-I and Huang, Kai-Yi},
	title = {Evaluating the effect of vegetation index of SPOT imagery on the accuracy of castanopsis carlesii potential habitat model},
	year = {2010},
	journal = {31st Asian Conference on Remote Sensing 2010, ACRS 2010},
	volume = {2},
	pages = {1773 – 1778},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865617923&partnerID=40&md5=3fe63e1978db439abe8bc932c3526abd},
	abstract = {Afforestation for carbon reduction has become more important due to climatic change issues resulted from global warming recently. "Planting right tree species at right sites" can improve tree survival percent, thereby achieving the goal of plantation for carbon reduction. To find right sites for plantation, we attempted to predict the potential habitat of trees accurately by coupling 3S technologies and multivariate statistics. Long-leaf chinkapin (Castanopsis carlesii), whose seeds have long been identified as an important food source for animals in the Huisun study area, was chosen as a target for the study. The study overlaid the tree samples collected with GPS on the layers of vegetation indices derived from SPOT images, elevation, slope, aspect, and terrain position to analyze its spatial distribution by using GIS. Decision tree (DT), logistic multiple regression (LMR), and discriminant analysis (DA) models were developed to predict its potential habitat and evaluate the effect of vegetation indices on model accuracy. Results indicated that SPOT image vegetation indices could not improve the accuracies due to the limits of their spectral resolution and spatial resolution not enough to discriminate the dispersedly distributed species in Huisun. Airborne hyperspectral and LIDAR data will thus be used in follow-up studies to improve model accuracy. Accuracy assessment results also indicated that the accuracies of DT and LMR were nearly equal and much better than that of DA. These models were efficient in implementation of model development and validation. DT and LMR models greatly reduced the area of field survey to less than 10% of the entire study area at the first stage, and thus they were better suited for potential habitat modeling. More importantly, elevation was shown to be one of the most important predictor variables in the models based on spatial distribution analysis of the species.},
	author_keywords = {Decision tree (DT); Discriminant analysis (DA); Geographic information system (GIS); Logistic multiple regression (LMR); Remote sensing; Vegetation index},
	keywords = {Carbon; Coefficients; Ecosystems; GIS; Plants; Radar; Reforestation; Remote Sensing; Carbon; Decision trees; Discriminant analysis; Ecosystems; Geographic information systems; Global warming; Optical radar; Reforestation; Remote sensing; Spatial distribution; 3S Technology; Accuracy assessment; Carbon reduction; Climatic changes; Field surveys; Follow-up Studies; Food sources; HyperSpectral; LIDAR data; Model accuracy; Model development; Multiple regressions; Multivariate statistics; Potential habitat; Predictor variables; Spatial distribution analysis; Spatial resolution; SPOT image; Study areas; Tree species; Tree survival; Vegetation index; Vegetation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Gruber20179,
	author = {Gruber, Fabian E. and Baruck, Jasmin and Geitner, Clemens},
	title = {Algorithms vs. surveyors: A comparison of automated landform delineations and surveyed topographic positions from soil mapping in an Alpine environment},
	year = {2017},
	journal = {Geoderma},
	volume = {308},
	pages = {9 – 25},
	doi = {10.1016/j.geoderma.2017.08.017},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027854147&doi=10.1016%2fj.geoderma.2017.08.017&partnerID=40&md5=17dc5a50d878d5f6ca0bbfe502f87b6c},
	abstract = {Landform delineation has long been used in digital soil mapping to infer soil-relevant information. While its potential as an environmental variable in soil parameter modeling has been investigated for various automated landform delineations, little research has been invested into the relationship between the delineation of landforms by algorithms based on digital terrain models (DTM) and the perception of landforms by the soil surveyor during field work. Five Open Source automated landform classification algorithms and a support vector machine classifier based on single terrain parameters are investigated with regard to their ability to replicate topographic position, at two different scales, as described by surveyors for soil profile sites in the Alpine environment of South Tyrol. We also analyse how the variation of parameters and cell size affects the distribution of the computed landforms. While a clear trend regarding grid cell size and window size can be observed with regard to the difference between macro and meso scale topographic positions, the overall classification accuracy regarding the different topographic position classes was less promising. Although some automated classifications partly resemble the surveyor's classification, a considerable number of issues remain to be investigated in order to explain the lack of reproducibility of surveyor position, some of which are linked to the Alpine environment of the study area. These include the dominance of the backslope position, the objectivity of the surveyor in rugged terrain under forest cover, and the fuzzy nature of classifying topographic position, especially in steep terrain. By applying a forward stepwise feature selection procedure for a model based on single terrain parameters, we show that at macro scale a regional terrain parameter (topographic wetness index) and curvatures at a coarse DTM resolution of 50 m are the most influential in distinguishing topographic position, whereas at meso scale it is the topographic position index (TPI) with a search radius of just 70 m combined with slope gradient. This study is an important first step towards consolidating topographic perception during field survey and digital terrain analysis, which, at least in Alpine terrain, still requires more investigation. © 2017 Elsevier B.V.},
	author_keywords = {Automated landform classification; Soil profile site description; Soil surveyor; Support vector machine classification; Topographic position},
	keywords = {Alto Adige; Italy; Trentino-Alto Adige; Automation; Forestry; Geomorphology; Mapping; Parameter estimation; Soil surveys; Soils; Support vector machines; Landform classification; Soil profiles; Soil surveyors; Support vector machine classification; Topographic positions; algorithm; alpine environment; automation; classification; comparative study; digital terrain model; field survey; landform; soil profile; soil survey; support vector machine; topographic mapping; topography; Landforms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17}
}

@ARTICLE{Roub201629,
	author = {Roub, Radek and Kurkova, Marie and Hejduk, Tomaš and Novak, Pavel and Bureš, Ludak},
	title = {Comparing a hydrodynamic model from fifth generation dtm data and a model from data modified by means of crosolver tool},
	year = {2016},
	journal = {Acta Universitatis Carolinae, Geographica},
	volume = {51},
	number = {1},
	pages = {29 – 39},
	doi = {10.14712/23361980.2016.3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982085257&doi=10.14712%2f23361980.2016.3&partnerID=40&md5=42404f99266b9db6022d64b1e90c27b5},
	abstract = {Flooding is a natural phenomenon that occurs with varying intensity and at irregular time intervals. Floods are the natural disasters that pose the greatest direct threat to the Czech Republic. They may cause serious critical situations during which not only extensive material damages are incurred, but so too is the loss of human life in affected areas as well as vast devastation of the cultural landscape including environmental damages. The information issued by flood forecasting services about the character and size of flood areas for individual N-year flood discharge events and specific flood scenarios is important for eliminating the potential threats and consequences of such events. Hydrodynamic models provide an adequate image of depths and flow velocities at the longitudinal or cross profiles of the watercourse during a flood event. This is why information obtained from hydrodynamic models occupies a privileged position from the viewpoint of protecting human life and mitigating property damage. Altimetry data are the basic input into hydrodynamic models. One way to obtain such data is through the method of aerial laser scanning (ALS) from the digital terrain model (DTM). This method is considered one of the most accurate methods for obtaining altimetry data. Its major drawback is however its inability to record terrain geometry under water surfaces due to the fact that the laser beam is absorbed by the body of water. The absence of geometric data on watercourse cross sectional area may perceptibly affect results of modelling, especially if the capacity of a missing part of the channel represents a significant cross sectional area. One of the methods for eliminating this deficiency is sufficiently calculating channel depth by means of software tools such as CroSolver. This paper deals with the construction of a hydrodynamic model using fifth generation DTM data and compares outputs from this model at various discharges with a model based on the altimetry data modified using CroSolver. Outputs from the two hydrodynamic models are compared using HEC-RAS software with the use of depth estimate data and with the use of the unmodified DTM. The comparison is done on two watercourse reaches with different terrain morphology and watercourse size. A complementary output is the comparison of inundation areas issuing from both model variants. Our results indicate that differences in the outputs are significant, namely at lower discharges (Q1, Q5), whereas at Q50 and Q100 the difference is negligible with a great role played by the morphology of the modelled area and by the watercourse size.},
	author_keywords = {Aerial laser scanning (ALS); Crosolver; Cross profile - floods; HEC-RAS; Hydrodynamic model},
	keywords = {Czech Republic; aerial survey; altimetry; comparative study; computer simulation; digital terrain model; flood damage; flood forecasting; flooding; flow velocity; hydrodynamics; numerical model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Baldo2009193,
	author = {Baldo, Marco and Bicocchi, Claudio and Chiocchini, Ugo and Giordan, Daniele and Lollino, Giorgio},
	title = {LIDAR monitoring of mass wasting processes: The Radicofani landslide, Province of Siena, Central Italy},
	year = {2009},
	journal = {Geomorphology},
	volume = {105},
	number = {3-4},
	pages = {193 – 201},
	doi = {10.1016/j.geomorph.2008.09.015},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-60649118546&doi=10.1016%2fj.geomorph.2008.09.015&partnerID=40&md5=949226de325afba0bb694ccca18d5170},
	abstract = {The Radicofani Basin, stretching about 30 km NW-SE, is an intra-Central Apennine basin connected to Pliocene-Pleistocene extensional tectonics. It consists of an Early to Middle Pliocene succession including essentially shelf pelites. In the Radicofani area, province of Siena (Tuscany region), morphodynamic processes are very frequent with widespread badlands and rapidly evolving mudflows. In order to evaluate the general instability of the Radicofani area, geological and geomorphological surveys were carried out. The 1954, 1990 and 2003 aerial surveys allowed a comparison of the changes in the various morphological aspects of the study area, which suggested an increase in slope instability with time. A new complex translational landslide evolving into mudflows, activated during the winter of 2003, was monitored using an experimental system based on terrestrial LIDAR (Light Detection and Ranging) and GPS (Global Positioning System) technologies. This system allowed the monitoring of the morphologic and volumetric evolution of the landslide. A comparison of the monitoring data of October 2004, June 2005, May 2006 and May 2007 points out that the evolution is characterised by the sliding of displaced materials. A volume of about 1300 m3 of materials was removed during the period 2004-2005, 300 m3 for 2005-2006, and 400 m3 for 2006-2007. The greater initial mass movement probably reflects a greater static imbalance during the early period of landslide movement and increased rainfall. Therefore, the proposed monitoring system methodology allows the numerical evaluation of the landslide morphological evolution and to validate the landslide evolution model based on geological and geomorphological field surveys. © 2008 Elsevier B.V. All rights reserved.},
	author_keywords = {Central Italy; DTM; GPS; LIDAR; Mass wasting; Pelites},
	keywords = {Apennines; Eurasia; Europe; Italy; Radicofani Basin; Siena [Tuscany]; Southern Europe; Tuscany; aerial survey; geomorphology; GPS; landslide; lidar; monitoring system; morphodynamics; pelite; Pleistocene; Pliocene},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 94}
}

@CONFERENCE{Fuad2018,
	author = {Fuad, N.A. and Ismail, Z. and Majid, Z. and Darwin, N. and Ariff, M.F.M. and Idris, K.M. and Yusoff, A.R.},
	title = {Accuracy evaluation of digital terrain model based on different flying altitudes and conditional of terrain using UAV LiDAR technology},
	year = {2018},
	journal = {IOP Conference Series: Earth and Environmental Science},
	volume = {169},
	number = {1},
	doi = {10.1088/1755-1315/169/1/012100},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051995360&doi=10.1088%2f1755-1315%2f169%2f1%2f012100&partnerID=40&md5=7439e3dec8873cbce204e6eea1923a4f},
	abstract = {Unmanned Aerial Vehicle (UAV) with Light Detecting Radar (LiDAR) sensor can be used to obtained high ground resolution data and generate good quality of Digital Terrain Model (DTM) as much can decrease the cost of data acquisition and processing time. This study aims to evaluate the influences of flying altitude and terrain on DTM accuracies obtained with UAV-based LiDAR. In this study, point clouds from UAV AL3 S1000 and AL3 - 32 LiDAR were used for generating DTM on two different terrains (i.e. flat, slope and overall) with three different flying altitudes (i.e. 20m, 40m and 60m) and validate with ground control points by using 129 reference points which taken from ground survey technique (GPS, total station and optical levelling). The Root Mean Square Error (RMSE) of point clouds elevation obtained at different altitudes for the flat area are 0.015m, 0.027m and 0.105m at the altitudes of 20m, 40m and 60m, respectively. Meanwhile, RMSE values for slope area are 0.267m, 0.298m and 0.343m at the altitudes of 20m, 40m and 60m, respectively. Overall study area gives the RMSE values of 0.323m, 0.450m and 0.616m at 20m, 40m and 60m altitude, respectively. The result shows that the change of RMSE values influenced by the different of altitude and terrain, which provides accurate and faster results. © Published under licence by IOP Publishing Ltd.},
	keywords = {Aircraft detection; Altitude control; Antennas; Data acquisition; Data handling; Electric substations; Landforms; Mean square error; Remote sensing; Rock mechanics; Unmanned aerial vehicles (UAV); Accuracy evaluation; Different terrains; Digital terrain model; Ground control points; Ground resolution; Processing time; Reference points; Root mean square errors; Optical radar},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access, Gold Open Access}
}

@ARTICLE{Takeda1997934,
	author = {Takeda, K. and Horiuchi, K. and Yagyu, Y. and Yamada, M.},
	title = {Trend of aging on the number of teeth investigated through a survey of dental health representation in a simple equation by using a kinetic method and its application},
	year = {1997},
	journal = {[Nippon kōshū eisei zasshi] Japanese journal of public health},
	volume = {44},
	number = {12},
	pages = {934 – 941},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031317699&partnerID=40&md5=6d07d4985918f60a6b31e13c78fc70b2},
	abstract = {A survey of dental health was conducted on 6,933 residents (aged 61.8 +/- 11.1) in 4 villages Nara Prefectural Uchiyoshino Health Center administers. Examinees were divided into 7 ages groups. The mean value of present teeth at each age group was calculated. Age dependency of number of teeth was investigated with a kinetic model based on the following premise. 1. Changes between age groups can be treated as a time series occurrence. 2. Number of missing teeth can be estimated as 29 - that of present teeth. Results show that the rate of decrease of the number of present teeth is of a first order in itself and the number of missing teeth. It can be mathematically expressed as follows: -dX/dt = k.X.(29 - X).......eq. (1). in which X = number of present teeth, 29 - x = number of missing teeth k = rate constant, t = time. From this simple relationship the following are suggested 1. Tooth loss is caused by the interaction of present teeth and sites where teeth are lost. 2. Integration of eq. (1) yields ln¿(29 - X)/X¿ = 29.k.t + a.......eq. (2). in which 0 < X < 29, t = time from the youngest age group (20-29), a = constant. It appears that, from eq. (2), the dental health of a community is determined by two coefficients, 29.k and a. 3. Helping a person realize his present and future dental state and to motivate action for prevention of tooth loss by showing him his rate of decrease, "tooth age" and the predicted number of teeth a certain years later calculated from eq. (1) and eq. (2), can be of great value. 4. Eq. (2) is transformed to X = 29/¿exp(29.k.t + a) + 1¿, which represents the reverse S-shaped curve of the age dependency of the number of teeth. 5. Applying this method to cohort analysis will enable forecast of the trend of the number of teeth.},
	keywords = {Adult; Aged; Aged, 80 and over; Aging; Dental Health Surveys; Female; Humans; Male; Middle Aged; Models, Statistical; Tooth Loss; adult; aged; aging; article; female; health survey; human; male; middle aged; periodontal disease; physiology; statistical model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Tewari2009479,
	author = {Tewari, H.C. and Rao, G. Surya Prakasa and Prasad, B. Rajendra},
	title = {Uplifted crust in parts of western India},
	year = {2009},
	journal = {Journal of the Geological Society of India},
	volume = {73},
	number = {4},
	pages = {479 – 488},
	doi = {10.1007/s12594-009-0033-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-64549139230&doi=10.1007%2fs12594-009-0033-9&partnerID=40&md5=887ab513c17553647d146945bdd2eb21},
	abstract = {During northward movement of the Indian sub-continent, after its breakup from the Gondwanaland in the Late Cretaceous, the western part of India traversed over the Reunion plume. The Saurashtra peninsula and the Cambay Basin are two important geological regions in this part. Two and half dimensional density models, based on the crustal seismic structure, were generated to establish a relationship between these two regions. These models indicate that the crust is 32-33 km thick in the eastern Saurashtra and the northern part of the Cambay Basin. The shallower crust is in a triangular region formed by the extension of the western limb of the Proterozoic Aravalli trend in Saurashtra, its eastern limb and the Narmada fault in the south. Compared to 36-37 km thick crust to the west and 38-40 km to the east of this region the crust in the above triangular region is uplifted by 4 to 6 km. This uplift took place either after the deposition of Mesozoic sediments or was concon-dtant with the rise of Reunion plume prior to the extrusion of the Deccan volcanics as the region was close to the axis of the plume. © Geol. Soc. India.},
	author_keywords = {Crust; Deccan volcanics; deep seismic; Density; Reunion plume; Uplift},
	keywords = {crustal structure; Deccan Traps; deep seismic sounding; seismic survey; uplift},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 25}
}

@ARTICLE{Fernandes2018169,
	author = {Fernandes, Farley and Mateus, Américo and Leonor, Susana and Sequeira, Manuel and Gaio, Rui},
	title = {Gamethinking: A roadmap to a design thinking-based model for game development education; [Gamethinking: Une roadmap pour un modèle fondé sur design thinking pour l´éducation au development du jeu]; [Gamethinking: Um roadmap para um modelo baseado em design thinking para a educação em desenvolvimento de jogos]; [Gamethinking: Un roadmap para un modelo basado en el design thinking para la educación en desenvolvimento de videojuegos]},
	year = {2018},
	journal = {Revista Lusofona de Educacao},
	volume = {40},
	number = {40},
	pages = {169 – 182},
	doi = {10.24140/ISSN.1645-7250.RLE40.11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064704431&doi=10.24140%2fISSN.1645-7250.RLE40.11&partnerID=40&md5=be5c16f4872633d39f54867a6a5d24f9},
	abstract = {The mindset, behavior, and attitude of students in game development, project-based learning (PBL) B.Sc.’s towards the creative processes has proven to be a barrier with regards to the need to increase of the creativity of the student’s project proposals. While it is true that a systematic use of Design Thinking (DT) throughout the curriculum would certainly improve the situation, a different approach is clearly needed, most likely an approach that combines DT with gamification, and that makes optimal use of project-based learning (PBL) using agile methods. In order to refine and clarify our possible contribution on the conjunction of such perspectives, we conducted an initial literature review that validated our objectives. Complementarily, we also defined a roadmap to transform such intentions into practical actions towards the creation of a new game development model based on DT principles. This whole definition is based on IDEAS(R)EVOLUTION methodology, which gives structure for the research on the long run. © 2018, Edicoes Universitarias Lusofonas. All rights reserved.},
	author_keywords = {Agile; Design thinking; Gamification; Project-based learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Latella2017254,
	author = {Latella, Diego and Loreti, Michele and Massink, Mieke},
	title = {FlyFast: A scalable approach to probabilistic model-checking based on mean-field approximation},
	year = {2017},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {10500 LNCS},
	pages = {254 – 275},
	doi = {10.1007/978-3-319-68270-9_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032663101&doi=10.1007%2f978-3-319-68270-9_13&partnerID=40&md5=d5d299331d537bacb236a7117529ba03},
	abstract = {Model-checking is an effective formal verification technique that has also been extended to quantitative logics and models such as PCTL and DTMCs as well as CSL and CTMCs/CTMDPs. Unfortunately, the state-space explosion problem of classical model-checking algorithms affects also quantitative extensions. Mean-field techniques provide approximations of the mean behaviour of large population models. These approximations are deterministic: a unique value of the fractions of agents in each state is computed for each time instant. A drastic reduction of the size of the model is obtained enabling the definition of an efficient model-checking algorithm. This paper is a survey of work we have done in the last few years in the area of mean-field approximated probabilistic model-checking. We start with a brief description of FlyFast, an on-the-fly model checker we have developed for approximated bounded PCTL model-checking, based on mean-field population DTMC approximation. Then we show an example of use of FlyFast in the context of Collective Adaptive Systems. We also discuss two additional interesting front-ends for FlyFast; the first one is a translation from CTMC-based population models and (a fragment of) CSL that allows for approximate probabilistic model-checking in the continuous stochastic time setting; the second one is a translation from a predicate-based process interaction language that allows for probabilistic model-checking of models based on components equipped both with behaviour and with attributes, on which predicates are defined that can be used in component interaction primitives. © Springer International Publishing AG 2017.},
	author_keywords = {Collective adaptive systems; Discrete time markov chains; Mean-field approximation; Probabilistic on-the-fly model-checking; Time bounded probabilistic computation tree logic},
	keywords = {Adaptive systems; Computation theory; Markov processes; Stochastic models; Stochastic systems; Trees (mathematics); Discrete time Markov chains; Mean field approximation; Model checking algorithm; On the flies; Probabilistic computation tree logic; Probabilistic model checking; State-space explosion; Verification techniques; Model checking},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Buyuksalih2008865,
	author = {Buyuksalih, G. and Jacobsen, K.},
	title = {DSM generation with high resolution space imagery over mountainous forest area},
	year = {2008},
	journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
	volume = {37},
	pages = {865 – 871},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024182393&partnerID=40&md5=6e60902f248ee0ae932ff2a0eef78f66},
	abstract = {For an extension of the water catchments areas for the Greater Istanbul Municipality investigations in a mountainous forest area in Turkey, at the Bulgarian border, named Istiranca, have been made. In the project area a height model from SPOT-5 HRS, the SRTM C-band height model and a digital elevation model (DEM) for the topographic map 1:25 000 are available like also a Cartosat-1 stereo pair. For comparison a SPOT-5 HRS height model has been checked in Istanbul vicinity against a reference DEM from more accurate maps 1:5000 and also the SRTM C-band height model.Based on the Cartosat-1 stereo pair a digital surface model (DSM) has been generated by automatic image matching. For the orientation of the Cartosat-1 images control points from the topographic map 1:25 000 have been used, leading in the average to RMSX=6.78m, RMSY=7.11m and RMSZ=6.08m. This accuracy is dominated by the limited control point quality, but with the 14 well distributed control points a satisfying orientation can be guaranteed. The automatic matching by least squares resulted in an astonishing complete coverage of the area by matched points. The matching failed only in small sub-areas covered by clouds. In the forest, not influenced by clouds, more than 98% of the possible object points have been matched successfully with correlation coefficients above 0.5 and with a maximum of the correlation coefficients in the range of 0.85. For mountainous forest areas this is an unusual good result. The matching was made for every third pixel, leading to a point spacing of approximately 7.5m. SRTM C-band and the results from image matching with optical images lead to DSMs, including the height of the visible surface, while the reference height model from the topographic map 1:25 000 refers to the bare ground. The SRTM height model is available with a spacing of 3 arcsec, including only limited morphologic details. All the height models have been compared to each other. A strange correspondence of the SPOT-5 HRS height model to the SRTM Cband height model can be seen, which only can be explained by filling the gaps in the SPOT-5 HRS height model in the forest area by SRTM C-band data. © 2008 International Society for Photogrammetry and Remote Sensing. All rights reserved.},
	author_keywords = {Databases; DEM/DTM; Digital; Modelling; Satellite},
	keywords = {Catchments; Database systems; Forestry; Geometrical optics; Image coding; Image matching; Maps; Models; Quality control; Remote sensing; Satellites; Surveying; Tracking radar; Automatic image matching; Automatic matching; Correlation coefficient; DEM/DTM; Digital; Digital elevation model; Digital surface model (DSM); Distributed control; Stereo image processing},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Xirui2009315,
	author = {Xirui, Yang and Chang, Li},
	title = {Study of dynamic traffic assignment model based on different route choice rules},
	year = {2009},
	journal = {Proceedings of the 2nd International Conference on Modelling and Simulation, ICMS2009},
	volume = {4},
	pages = {315 – 319},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049120383&partnerID=40&md5=e5b913224a4d41b254dead49ea95d9b5},
	abstract = {The present work on multi-class analytical DTA model assumes that the link travel time function is identical for all travelers. Again the interactions among multiple classes are neglected. Moreover the different route choice rules to different users are hardly considered. Therefore, a multi-class analytical DTA model is proposed in this paper. The user classes typically refer to different classes whose route choice rules are different, such as dynamic system optimization, dynamic user equilibrium, and stochastic dynamic user equilibrium. According to travel time characteristic of each user class, the multidimensional travel time function based on the extended BPR function is built which can satisfy FIFO requirement and express the asymmetric interactions among user classes. At the same time, the interdependences among user classes are modeled based on bilevel programming, and the solution algorithm on bi-level model is designed using iterative optimization algorithm. Finally, the model is applied to the experimental network.},
	author_keywords = {Dynamic traffic assignment (DTA); Iterative optimization; Multiple user-classes; Route choice rules},
	keywords = {Differential thermal analysis; Dynamical systems; Optimization; Traffic control; Traffic surveys; Asymmetric interaction; Bi-level programming; Dynamic Systems; Dynamic traffic assignments; Dynamic user equilibrium; Iterative Optimization; Iterative optimization algorithms; Level model; Link travel time; Multi-class; Multiple class; Multiple user; Route choice; Solution algorithms; Stochastic dynamics; Travel time; Travel time functions; User class; Dynamic models},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Bjelanovic2018,
	author = {Bjelanovic, Ivan and Comeau, Philip G. and White, Barry},
	title = {High resolution site index prediction in boreal forests using topographic and wet areas mapping attributes},
	year = {2018},
	journal = {Forests},
	volume = {9},
	number = {3},
	doi = {10.3390/f9030113},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042753963&doi=10.3390%2ff9030113&partnerID=40&md5=6c90a30e818f3a31ba578209273ac586},
	abstract = {The purpose of this study was to evaluate the relationships between environmental factors and the site index (SI) of trembling aspen, lodgepole pine, and white spruce based on the sampling of temporary sample plots. LiDAR generated digital elevation models (DEM) and wet areas mapping (WAM) provided data at a 1 m resolution for the study area in Alberta. Six different catchment areas (CA), ranging from 0.5 ha to 10 ha, were tested to reveal optimal CA for calculation of the depth-to-water (DTW) index from WAM. Using different modeling methods, species-specific SI models were developed for three datasets: (1) topographic and wet area variables derived from DEM andWAM, (2) onlyWAM variables, and (3) field measurements of soil and topography. DTW was selected by each statistical method for each species and, in most cases, DTW was the strongest predictor in the model. In addition, differences in strength of relationships were found between species. Models based on remotely-sensed information predicted SI with a root mean squared error (RMSE) of 1.6 m for aspen and lodgepole pine, and 2 m for white spruce. This approach appears to adequately portray the variation in productivity at a fine scale and is potentially applicable to forest growth and yield modeling and silviculture planning. © 2018 by the authors.},
	author_keywords = {Boreal forests; Depth-to-water index; Forest productivity; Site index},
	keywords = {Forests; Mapping; Picea Engelmannii; Pinus Contorta; Populus Tremuloides; Silviculture; Alberta; Canada; Picea glauca; Pinus contorta; Catchments; Mapping; Mean square error; Optical radar; Productivity; Surveying; Boreal forests; Depth-to-water; Digital elevation model; Environmental factors; Forest growth and yield; Forest productivity; Root mean squared errors; Site index; biological production; boreal forest; coniferous tree; data set; deciduous tree; digital elevation model; environmental factor; evergreen tree; growth rate; prediction; remote sensing; silviculture; site index; spatial resolution; topographic mapping; Forestry},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access, Gold Open Access}
}

@ARTICLE{Rollins2014263,
	author = {Rollins, Brent L. and Ramakrishnan, Shravanan and Perri, Matthew},
	title = {Direct-to-Consumer Advertising of Predictive Genetic Tests: A Health Belief Model Based Examination of Consumer Response},
	year = {2014},
	journal = {Health Marketing Quarterly},
	volume = {31},
	number = {3},
	pages = {263 – 278},
	doi = {10.1080/07359683.2014.936295},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905996578&doi=10.1080%2f07359683.2014.936295&partnerID=40&md5=642354b6953a4f10ae894c8c54260e72},
	abstract = {Direct-to-consumer (DTC) advertising of predictive genetic tests (PGTs) has added a new dimension to health advertising. This study used an online survey based on the health belief model framework to examine and more fully understand consumers' responses and behavioral intentions in response to a PGT DTC advertisement. Overall, consumers reported moderate intentions to talk with their doctor and seek more information about PGTs after advertisement exposure, though consumers did not seem ready to take the advertised test or engage in active information search. Those who perceived greater threat from the disease, however, had significantly greater behavioral intentions and information search behavior. © 2014 Copyright Taylor & Francis Group, LLC.},
	author_keywords = {direct-to-consumer; health belief model; predictive genetics tests},
	keywords = {Adolescent; Adult; Advertising as Topic; Aged; Consumer Behavior; Cross-Sectional Studies; Data Collection; Female; Genetic Testing; Humans; Information Seeking Behavior; Male; Middle Aged; Models, Psychological; adolescent; adult; advertizing; aged; consumer attitude; cross-sectional study; female; genetic screening; human; information processing; information seeking; male; middle aged; psychological model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{De Morisson Valeriano2000678,
	author = {De Morisson Valeriano, Márcio and Garcia, Gilberto José},
	title = {The estimate of topographical variables for soil erosion modelling through geoprocessing},
	year = {2000},
	journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
	volume = {33},
	pages = {678 – 685},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865018325&partnerID=40&md5=397287dfb0d8bf1f03aee3f6aaa3173f},
	abstract = {: The technical requirements for geoprocessing of topographical data were studied, from digitizing until the obtention of the topographical factor of the Universal Soil Loss Equation (USLE), through calculation of slope (angle) and length. The development of the methods were based on data from manual cartographic survey and geographical information system (GIS) results of Sao Joaquim creek watershed (Pirassununga, SP, Brazil). A preliminary study, on the spatial behavior of the variables, their relations to local relief and to the topographical factor sensibility supported the selection of geoprocessing optimal procedures and parameters to achieve the maximum quality of the generated digital terrain models (DTM), and therefore, of all GIS results. Tests with the slope calculation indicated the need of high resolution, and the importance of post-processing (smoothing) and numerical fit. A method to obtain slope length was developed using spatial analysis functions of GIS. Results of the anisotropic cost analysis were correlated to measureed length, requiring linear fit. Errors for digital estimates prevailed in steep relief areas. © 2000 International Society for Photogrammetry and Remote Sensing. All rights reserved.},
	author_keywords = {Agriculture; Dtm/dem/dsm; GIS; Information extraction; Model-based processing; Raster; Soil conservation; Watersheds},
	keywords = {Agriculture; Information retrieval; Information systems; Information use; Remote sensing; Sediment transport; Soil conservation; Soils; Watersheds; Digital terrain model; Dtm/dem/dsm; Model-based processing; Raster; Soil erosion modelling; Spatial behaviors; Technical requirement; Universal soil loss equation; Geographic information systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Viji2019917,
	author = {Viji, D. and Singh, Ujjwal Kumar and Sandhu, Armaan Singh},
	title = {Survey on prediction and analysis of train delay},
	year = {2019},
	journal = {Journal of Advanced Research in Dynamical and Control Systems},
	volume = {11},
	number = {4 Special Issue},
	pages = {917 – 920},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071943302&partnerID=40&md5=5c1986d29b33e4b398f44fa1ccaf6fac},
	abstract = {Train delay is one of the biggest challenges that is faced by the railway systems across the globe. The mechanism used to predict the train delay is based on a lot of factors that are directly or indirectly linked to delay of trains. They rely on the rules that are made by the experts of railway systems that are based on univariate statistics. There has been an increased interest in the application of advanced data analytics to solve train specific problems such as maintenance of railway assets, automatic visual inspection systems, network area estimation, energy efficient railway operations. Prediction of failures by most of the existing methods is done by comparing the current or computed values with a set of standard values. A few proposed methods are based on: Fuzzy rules and the univariate autoregressive integrated moving average (ARIMA) model, Graph topology, scheduling algorithms for PROFINET and MVB, the adaptive iterative learning control (AILC) mechanism and Markov model based methods. This paper presents a survey of the existing train delay detection techniques. © 2019, Institute of Advanced Scientific Research, Inc.. All rights reserved.},
	author_keywords = {AILC; ARIMA; DFS; DTDPS; HDFS; QOS; SPN; ST Maps},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Portal201688,
	author = {Portal, A. and Gailler, L.-S. and Labazuy, P. and Lénat, J.-F.},
	title = {Geophysical imaging of the inner structure of a lava dome and its environment through gravimetry and magnetism},
	year = {2016},
	journal = {Journal of Volcanology and Geothermal Research},
	volume = {320},
	pages = {88 – 99},
	doi = {10.1016/j.jvolgeores.2016.04.012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964334262&doi=10.1016%2fj.jvolgeores.2016.04.012&partnerID=40&md5=3825fc2df3e259a3c7e821e2d035d85a},
	abstract = {Volcanic lava domes are compound edifices resulting from complex growth processes including intrusion and extrusion phases, explosions and collapses. Here, we present the study of a complex volcanic system, located in the Chaîne des Puys volcanic field (French Massif Central, France) and centred on the Puy de Dôme volcano, an 11,000 years old volcano. Our approach is based on a morpho-structural analysis of a high resolution DTM (0.5 m) and geophysical imaging methods. Both gravity and magnetic high resolution surveys have been carried out on the lava dome and the nearby volcanic structures. We computed 3D inverse and 2D forwards models. Based on our current knowledges about volcanic dome structure, the geophysical models allow us to propose a synthetic geological model of the inner structure of the Puy de Dôme and surrounding areas. This model suggests a scenario for the formation of the lava dome and the inferred intrusions located on both sides. The Puy de Dôme could possibly be the southern tip of the northern intrusion. © 2015 Elsevier B.V.},
	author_keywords = {Gravimetry; Inverse modelling; Lava dome; Magnetism; Puy de Dôme volcano},
	keywords = {Auvergne; Chaine des Puys; France; Puy de Dome [Chaine des Puys]; Domes; Gravimeters; Gravimetric analysis; Magnetism; Volcanoes; Geological modeling; Geophysical imaging; Geophysical models; High-resolution survey; Inverse modelling; Lava dome; Volcanic structures; Volcanic systems; geophysical method; gravimetry; gravity survey; igneous intrusion; inverse problem; lava dome; magnetic survey; numerical model; Structural geology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16}
}

@ARTICLE{Caten2013359,
	author = {Caten, Alexandre Ten and Dalmolin, Ricardo Simão Diniz and Pedron, Fabrício de Araújo and Ruiz, Luis Fernando Chimelo and da Silva, Carlos Antônio},
	title = {An appropriate data set size for digital soil mapping in Erechim, Rio Grande do Sul, Brazil; [Volume de dados adequado para o mapeamento digital de solos no município de Erechim, Rio Grande do Sul, Brasil]},
	year = {2013},
	journal = {Revista Brasileira de Ciencia do Solo},
	volume = {37},
	number = {2},
	pages = {359 – 366},
	doi = {10.1590/S0100-06832013000200007},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878841156&doi=10.1590%2fS0100-06832013000200007&partnerID=40&md5=08d361d1a29b11e78ee800d5db78083c},
	abstract = {Digital information generates the possibility of a high degree of redundancy in the data available for fitting predictive models used for Digital Soil Mapping (DSM). Among these models, the Decision Tree (DT) technique has been increasingly applied due to its capacity of dealing with large datasets. The purpose of this study was to evaluate the impact of the data volume used to generate the DT models on the quality of soil maps. An area of 889.33 km2 was chosen in the Northern region of the State of Rio Grande do Sul. The soil-landscape relationship was obtained from reambulation of the studied area and the alignment of the units in the 1:50,000 scale topographic mapping. Six predictive covariates linked to the factors soil formation, relief and organisms, together with data sets of 1, 3, 5, 10, 15, 20 and 25 % of the total data volume, were used to generate the predictive DT models in the data mining program Waikato Environment for Knowledge Analysis (WEKA). In this study, sample densities below 5 % resulted in models with lower power of capturing the complexity of the spatial distribution of the soil in the study area. The relation between the data volume to be handled and the predictive capacity of the models was best for samples between 5 and 15 %. For the models based on these sample densities, the collected field data indicated an accuracy of predictive mapping close to 70 %.},
	author_keywords = {Decision tree; Mapping unit; Pedometry; Soil survey},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Xie200350,
	author = {Xie, Chi and Lu, Jinyang and Parkany, Emily},
	title = {Work Travel Mode Choice Modeling with Data Mining: Decision Trees and Neural Networks},
	year = {2003},
	journal = {Transportation Research Record},
	number = {1854},
	pages = {50 – 61},
	doi = {10.3141/1854-06},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-1942472940&doi=10.3141%2f1854-06&partnerID=40&md5=f413727bd21275c9e4c9783ae250dd85},
	abstract = {Among discrete choice problems, travel mode choice modeling has received the most attention in the travel behavior literature. Most traditional mode choice models are based on the principle of random utility maximization derived from econometric theory. Alternatively, mode choice modeling can be regarded as a pattern recognition problem in which multiple human behavioral patterns reflected by explanatory variables determine the choices between alternatives or classes. The capability and performance of two emerging pattern recognition data mining methods, decision trees (DT) and neural networks (NN), for work travel mode choice modeling were investigated. Models based on these two techniques are specified, estimated, and comparatively evaluated with a traditional multinomial logit (MNL) model. For comparison, a unique three-layer formulation of the MNL model is presented. The similarities and differences of the models' mechanisms and structures are identified, and the mechanisms and structures in the models' specifications and estimations are compared. Two performance measures, individual prediction rate and aggregate prediction rate, which represent the prediction accuracies for individual and mode aggregate levels, respectively, were applied to evaluate and compare the performances of the models. Diary data sets from the San Francisco, California, Bay Area Travel Survey 2000 were used for model estimation and evaluation. The prediction results show that the two data mining models offer comparable but slightly better performances than the MNL model in terms of the modeling results, while the DT model demonstrates the highest estimation efficiency and most explicit interpretability, and the NN model gives a superior prediction performance in most cases.},
	keywords = {Behavioral research; Data mining; Data reduction; Industrial economics; Mathematical models; Pattern recognition; Scheduling; Trees (mathematics); Econometric theory; Multinominal logit (MNL) models; Transportation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 145}
}

@ARTICLE{Patalay2016242,
	author = {Patalay, Praveetha and Hayes, Daniel and Deighton, Jessica and Wolpert, Miranda},
	title = {A Comparison of Paper and Computer Administered Strengths and Difficulties Questionnaire},
	year = {2016},
	journal = {Journal of Psychopathology and Behavioral Assessment},
	volume = {38},
	number = {2},
	pages = {242 – 250},
	doi = {10.1007/s10862-015-9507-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939221296&doi=10.1007%2fs10862-015-9507-9&partnerID=40&md5=b2cf2c55fd2578b0f8da8cbf59b4b66a},
	abstract = {The Strengths and Difficulties Questionnaire (SDQ) is one of the most widely used measures of young people’s mental health difficulties in research and clinical decision-making. Although the SDQ is available in both paper and computer survey formats, cross-format equivalences have yet to be established. The current study aimed to assess the measure’s equivalence across paper- and computer-based survey formats in a community-based school setting. The study examined self-reported measures completed by a matched sample of 11–14 year olds in secondary schools in England (589 completed paper version; 589 online version). Analyses demonstrate that the factor structure, although did not vary by survey format, resulted in poorly fitting models limiting the use of model based invariance testing. Results indicate that the measure does not operate similarly across different formats, with scale-level mean differences observed for the hyperactivity scale, which also affects the total difficulties score, with higher scores seen in the paper version. Responses to the impact supplement were also influenced by survey format, with higher impact in specific domains disclosed on the computer-based measure. Item-level differential item functioning was observed for four items in the measure; two from the prosocial scale where the DIF is large enough to affect the scale (DTF, ν2 = 0.14). The inconsistency across survey formats highlights the need for more assessment of influences of different survey formats on young people, their perceived privacy and their mental health disclosures via different media. The findings also highlight the potential confounding effect of format when different methods of data collection are used, with a potentially substantive impact on cross-sample comparisons and within child clinical review. © 2015, Springer Science+Business Media New York.},
	author_keywords = {Computer; DIF; Format effects; Psychometric properties; SDQ; Validation},
	keywords = {adolescent; Article; child; clinical decision making; computer; controlled study; England; female; high school; human; hyperactivity; information processing; intermethod comparison; internal consistency; male; mental health; paper; questionnaire; self report; Strengths and Difficulties Questionnaire},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Oliveira2014117,
	author = {Oliveira, F.F. and Piteri, M.A. and Meneguette, M.},
	title = {Developing an opensource software system for the digital terrain model based on TIN; [Desenvolvimento de uma plataforma de software para a modelagem digital de terrenos baseada em TIN]},
	year = {2014},
	journal = {Boletim de Ciencias Geodesicas},
	volume = {20},
	number = {1},
	pages = {117 – 131},
	doi = {10.1590/S1982-21702014000100008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897393772&doi=10.1590%2fS1982-21702014000100008&partnerID=40&md5=7e9d1111b4268fff64361fe365b02f4f},
	abstract = {Topographical surfaces can be represented with a good degree of accuracy by means of maps. However these are not always the best tools for the understanding of more complex reliefs. In this sense, the greatest contribution of this work is to specify and to implement the architecture of an opensource software system capable of representing TIN (Triangular Irregular Network) based digital terrain models. The system implementation follows the object oriented programming and generic paradigms enabling the integration of various opensource tools such as GDAL, OGR, OpenGL, OpenSceneGraph and Qt. Furthermore, the representation core of the system has the ability to work with multiple topological data structures from which can be extracted, in constant time, all the connectivity relations between the entities vertices, edges and faces existing in a planar triangulation what helps enormously the implementation for real time applications. This is an important capability, for example, in the use of laser survey data (Lidar, ALS, TLS), allowing for the generation of triangular mesh models in the order of millions of points.},
	author_keywords = {Delaunay triangulation; DTM; Opensource systems; Topological data structures; Visualization},
	keywords = {digital terrain model; software; topology; triangulated irregular network; visualization},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Ali2010316,
	author = {Ali, Tarig A.},
	title = {Building of robust multi-scale representations of LiDAR-based digital terrain model based on scale-space theory},
	year = {2010},
	journal = {Optics and Lasers in Engineering},
	volume = {48},
	number = {3},
	pages = {316 – 319},
	doi = {10.1016/j.optlaseng.2009.11.003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-72649095835&doi=10.1016%2fj.optlaseng.2009.11.003&partnerID=40&md5=c78409dd5c54dc94b86294609fdaa760},
	abstract = {Scale-space theory is a framework originally developed for representing signal and imagery data at different scales through the re-sampling of the original data model. Such a framework allows for explicit, scale-invariant visualization and analysis of this type of data. The implementation of this theory in geospatial engineering is essential especially when processing a high-resolution digital terrain model (DTM) such as that produced from LiDAR data. This would allow for rapid retrieval, analysis, and display of the DTM at application-appropriate resolutions. The DTM is a means of representing the surface of the earth digitally, and it can be a Digital Elevation Model (DEM) or Triangular Irregular Network (TIN) model. DTMs can be produced from a variety of data types including photogrammetric, surveying, Global Positioning System (GPS), and Light Detection And Ranging (LiDAR) data. The availability and popularity of the LiDAR systems, which produce high intensity point clouds require the development of robust techniques for the retrieval, display, and analysis of the data model. This article presents a method based on cubic convolution for robust scale-space representation of DTMs created from LiDAR data. © 2009 Elsevier Ltd. All rights reserved.},
	author_keywords = {Digital terrain model; LiDAR; Scale-space theory; Triangular Irregular Network},
	keywords = {Data visualization; Global positioning system; Landforms; Models; Optical radar; Temperature control; Cubic convolution; Data models; Data type; Different scale; Digital elevation model; Digital terrain model; Geo-spatial; High intensity; High resolution; Imagery data; LIDAR data; Lidar systems; Light detection and ranging; Multiscale representations; Point cloud; Resampling; Robust technique; Scale-invariant; Scale-space representation; Scale-space theory; Triangular Irregular Networks; Visualization and analysis; Surveying},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@BOOK{Deangeli20151667,
	author = {Deangeli, Chiara and Tiranti, Davide and Marco, Federica and Volpato, Marco},
	title = {Comparison of debris flow depositional scenarios using different DTMs},
	year = {2015},
	journal = {Engineering Geology for Society and Territory - Volume 2: Landslide Processes},
	pages = {1667 – 1671},
	doi = {10.1007/978-3-319-09057-3_296},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944607138&doi=10.1007%2f978-3-319-09057-3_296&partnerID=40&md5=b4627807234111d42ab6b1805e286f05},
	abstract = {The paper reports numerical simulations of two debris flows occurred in 2000, in the middle part of the Stura di Valgrande valley (Lanzo, North-Western Italy). A Cellular Automata Model, based on the dilatant fluid constitutive law, and two different Digital Terrain Models(DTM) were employed. The first simulations were performed with a DTM dated 1999 (grid10 × 10 m) and reproducing the site topography before the events. The other simulations were carried out with a DTM dated 2011 (from Light Detection and Ranging surveys) with a grid5 × 5 m. Although the results obtained with a post event DTM can be affected by morphological changes, however the agreement with in site evidences seems to be more consistent. The rheological parameters calibrated with back analyses for the two DTMs are different. These results seem to indicate the influence of the topography accuracy for the calibration of the rheological parameters . © Springer International Publishing Switzerland 2015.},
	author_keywords = {Cellular Automata Model; Debris Flow; DTM; Numerical Modeling; Rheological Parameters},
	keywords = {Cellular automata; Numerical models; Optical radar; Cellular automata modeling; Constitutive law; Debris flows; Digital terrain model; Dilatant fluids; Light detection and ranging; Morphological changes; Rheological parameter; Debris},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Dong20183196,
	author = {Dong, Zijing and Dai, Erpeng and Wang, Fuyixue and Zhang, Zhe and Ma, Xiaodong and Yuan, Chun and Guo, Hua},
	title = {Model-based reconstruction for simultaneous multislice and parallel imaging accelerated multishot diffusion tensor imaging},
	year = {2018},
	journal = {Medical Physics},
	volume = {45},
	number = {7},
	pages = {3196 – 3204},
	doi = {10.1002/mp.12974},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049786964&doi=10.1002%2fmp.12974&partnerID=40&md5=729f562a30505a4ea4fa6f7f908dff65},
	abstract = {Purpose: Multishot interleaved echo-planar imaging (iEPI) can achieve higher image resolution than single-shot EPI for diffusion tensor imaging (DTI), but its application is limited by the prolonged acquisition time. To reduce the acquisition time, a novel model-based reconstruction for simultaneous multislice (SMS) and parallel imaging (PI) accelerated iEPI DTI is proposed. Materials and methods: DTI datasets acquired by iEPI with SMS and PI acceleration can be regarded as 3D k-space data, which is undersampled along both the slice and phase encoding directions. Instead of reconstruction of individual diffusion-weighted image, diffusion tensors are directly estimated by the joint reconstruction of undersampled 3D k-space from all diffusion-encoding directions using a model-based formulation to exploit the correlation across different directions. DTI simulation and in vivo acquisition were used to demonstrate the superior performance of the proposed method. Results: The proposed method reduced the estimation errors and artifacts than traditional parallel imaging reconstruction in DTI simulation. In the in vivo DTI experiment, the acquisition time of 4-shot iEPI was reduced from 11 min 7 s to 3 min 53 s with an acceleration factor of 4, and the image quality and precision of quantitative parameters were comparable with the fully sampled acquisition. Conclusions: The proposed model-based reconstruction for iEPI DTI with SMS and PI can achieve fourfold acceleration while maintaining high accuracy for tensor measurements. © 2018 American Association of Physicists in Medicine},
	author_keywords = {diffusion tensor imaging; high-resolution DTI; interleaved EPI; model-based reconstruction; simultaneous multislice},
	keywords = {Algorithms; Anisotropy; Diffusion Tensor Imaging; Image Processing, Computer-Assisted; Models, Statistical; Normal Distribution; Signal-To-Noise Ratio; Time Factors; Diffusion; Encoding (symbols); Image resolution; Magnetic resonance imaging; Signal encoding; Tensors; Three dimensional computer graphics; Acquisition time; Echo planar imaging; High resolution; High-resolution diffusion tensor imaging; Interleaved EPI; Model based reconstruction; Multi slices; Parallel imaging; Simultaneous multi-slice imaging; Simultaneous multislice; acceleration; arthroplasty; article; artifact; controlled study; diffusion tensor imaging; echo planar imaging; error; image quality; in vivo study; quantitative analysis; simulation; algorithm; anisotropy; image processing; normal distribution; procedures; signal noise ratio; statistical model; time factor; Diffusion tensor imaging},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access, Bronze Open Access}
}

@ARTICLE{Müller2009717,
	author = {Müller, Daniel and Schröder, Boris and Müller, Jörg},
	title = {Modelling habitat selection of the cryptic Hazel Grouse Bonasa bonasia in a montane forest},
	year = {2009},
	journal = {Journal of Ornithology},
	volume = {150},
	number = {4},
	pages = {717 – 732},
	doi = {10.1007/s10336-009-0390-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349745526&doi=10.1007%2fs10336-009-0390-6&partnerID=40&md5=2ff9ded8af8f8735afa138600cb734d6},
	abstract = {The Hazel Grouse Bonasa bonasia is strongly affected by forest dynamics, and populations in many areas within Europe are declining. As a result of the 'wilding' concept implemented in the National Park Bavarian Forest, this area is one of the refuges for the species in Germany. Even though the effects of prevailing processes make the situation there particularly interesting, no recent investigation about habitat selection in the rapidly changing environment of the national park has been undertaken. We modelled the species-habitat relationship to derive the important habitat features in the national park as well as factors and critical threshold for monitoring, and to evaluate the predictive power of models based on field surveys compared to an analysis of infrared aerial photographs. We conducted our surveys on 49 plots of 25 ha each where Hazel Grouse was recorded and on an equally sized set of plots with no grouse occurrence, and used this dataset to build a predictive habitat-suitability model using logistic regression with backward stepwise variable selection. Habitat heterogeneity, stand structure, presence of mountain ash and willow, root plates, forest aisles, and young broadleaf stands proved to be predictive habitat variables. After internal validation via bootstrapping, our model shows an AUC value of 0.91 and a correct classification rate of 87%. Considering the methodological difficulties attached to backward selection, we applied Bayesian model averaging as an alternative. This multi-model approach also yielded similar results. To derive simple thresholds for important predictors as a basis for management decisions, we alternatively ran tree-based modelling, which also leads to a very similar selection of predictors. Performance of our different survey approaches was assessed by comparing two independent models with a model including both data resources: one constructed only from field survey data, the other based on data derived from aerial photographs. Models based on field data seem to perform slightly better than those based on aerial photography, but models using both predictor datasets provided the highest predictive accuracy. © Dt. Ornithologen-Gesellschaft e.V. 2009.},
	author_keywords = {Aerial photography; BMA; CTREE; GLM; Predictive species distribution modelling; Wilding},
	keywords = {Bavaria; Bavarian Forest National Park; Central Europe; Eurasia; Europe; Germany; Bonasa bonasia; Salix; Sorbus; Tetraonidae; accuracy assessment; aerial photograph; Bayesian analysis; bootstrapping; data set; ecological modeling; forest dynamics; gamebird; habitat selection; infrared imagery; logistics; montane forest; national park; population decline; refuge; regression analysis; species occurrence; species-area relationship; stand structure},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 41}
}

@CONFERENCE{Faltýnová2013261,
	author = {Faltýnová, M. and Pavelka, K.},
	title = {Relics of mining activities in West Bohemia - Mapping by airborne laser scanning},
	year = {2013},
	journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
	volume = {40},
	number = {5W2},
	pages = {261 – 263},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924250168&partnerID=40&md5=69468a5d3ef1f440a3a60940c5a10aef},
	abstract = {The part of the Czech Republic - West Bohemia is well known for mining activities, different types of raw materials have been extracted from mines near Jáchymov, Sokolov and other sites since medieval times till today. There are original maps of some sites, as well there is effort of some geologists to find and map relics of mining activities (such as digs visible in terrain) by land survey. The quality of these available maps is unfortunately questionable - due to its age or used methods. Our aim was to find resource useful for searching for these sites, than to use field survey to confirm our findings. We used available digital terrain model (DTM) based on airborne laser scanning (ALS) technology to map relics of mining activities in West Bohemia. The Czech Office for Surveying, Mapping and Cadastre started in 2008 project for terrain mapping using the ALS method. The aim of mapping was to get authentic and detailed DTM of the Czech Republic. About 2/3 of area is currently covered by the DTM based on ALS, this year the mapping should be complete. The dataset is characterised by the density of 1-2 points/m2 and the standard deviation in altitude of model points is up to 30cm (in forested areas). We had DTM in form of shaded surface for one third of the Czech Republic. The shaded surface enables to highlight terrain break lines, which is suitable for archaeological research. Terrain modifications caused by human activity are characterized by terrain break lines, local tops or pits, which do not fit to local geomorphology. Visual image interpretation of the dataset is in the process.},
	author_keywords = {ALS; Archaeology; DTM; Jáchymov; Medieval mines; Shaded relief},
	keywords = {Landforms; Laser applications; Scanning; Surface analysis; Surveys; Airborne Laser scanning; Archeology; Czech Republic; Digital terrain model; Jachymov; Medieval mine; Mining activities; Model-based OPC; Shaded relief; Shaded surfaces; Mapping},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Ramalingam2018684,
	author = {Ramalingam, V.V. and Dandapath, Ayantan and Karthik Raja, M.},
	title = {Heart disease prediction using machine learning techniques: A survey},
	year = {2018},
	journal = {International Journal of Engineering and Technology(UAE)},
	volume = {7},
	number = {2.8 Special Issue  8},
	pages = {684 – 687},
	doi = {10.14419/ijet.v7i2.8.10557},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070500305&doi=10.14419%2fijet.v7i2.8.10557&partnerID=40&md5=f0305280c08345f911e6d903c4e712bd},
	abstract = {Heart related diseases or Cardiovascular Diseases (CVDs) are the main reason for a huge number of death in the world over the last few decades and has emerged as the most life-threatening disease, not only in India but in the whole world. So, there is a need of reliable, accurate and feasible system to diagnose such diseases in time for proper treatment. Machine Learning algorithms and techniques have been applied to various medical datasets to automate the analysis of large and complex data. Many researchers, in recent times, have been using several machine learning techniques to help the health care industry and the professionals in the diagnosis of heart related diseases. This paper presents a survey of various models based on such algorithms and techniques andanalyze their performance. Models based on supervised learning algorithms such as Support Vector Machines (SVM), K-Nearest Neighbour (KNN), NaïveBayes, Decision Trees (DT), Random Forest (RF) and ensemble models are found very popular among the researchers. © 2018 Authors.},
	author_keywords = {Cardiovascular diseases; Decision tree; Ensemble models; K- nearest neighbour; Naïve bayes; Random forest; Support vector machines},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 205; All Open Access, Bronze Open Access}
}

@BOOK{Ose1999132,
	author = {Ose, B.A. and Bai, Y. and Nystrom, P.R. and Damsleth, P.A.},
	title = {A finite-element model for In-Situ behavior of offshore pipelines on uneven seabed and its application to on-bottom stability},
	year = {1999},
	journal = {Proceedings of the 1999 Ninth International Offshore and Polar Engineering Conference},
	pages = {132 – 140},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033237915&partnerID=40&md5=5131262bcf34d55850ed8d517cd69d1e},
	abstract = {This paper presents a Finite Element (FE) model for simulation of the in-situ behavior of offshore pipelines laid on a 3-D seabed, with focus on on-bottom stability. The seabed is produced in the FE model based on 3-D survey data (Digital Terrain Model, DTM) from the area where the pipeline is to be installed. In the in-place analysis part of the model, the load history of laying, flooding, pressure testing, and operation is simulated. In the on-bottom stability analysis part, wave and current loading is applied as a restart of the relevant loading condition and the dynamic behavior of the pipeline is investigated. The importance of simulating the true load history before applying the hydrodynamic loads is that it gives a more correct initial pipe configuration and force distribution prior to the stability analysis. A matter that complicates the on-bottom stability analysis of pipelines on a 3-D seabed is the fact that the hydrodynamic coefficients among other things are dependent on the seabed proximity. This problem has been solved in the FE model by calculating the gaps between the pipe and the seabed at multiple, equally spaced points along the pipe length, and calculating the hydrodynamic coefficients for drag, lift, and added mass as a function of this gap. The develop FE model may be applied to study the global static/dynamic on-bottom behavior of the pipeline, and to evaluate the effects of seabed intervention such as rock dumping and trenching. 3-D FE simulations are presented at the end of the paper as design examples.},
	author_keywords = {Finite element model; In-place analysis; Offshore pipelines; On-bottom stability; Uneven seabed},
	keywords = {conference proceeding; finite element analysis; seafloor roughness; stability; submarine pipeline; Computer simulation; Drag; Finite element method; Hydrodynamics; Lift; Mathematical models; Ocean currents; Pipeline laying; Structural analysis; Water wave effects; conference proceedings; finite element method; seafloor; stability analysis; submarine pipeline; Digital terrain model (DTM); On-bottom stability analysis; Offshore pipelines},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@ARTICLE{Levin201686,
	author = {Levin, Michael W. and Boyles, Stephen D. and Duthie, Jennifer and Pool, C. Matthew},
	title = {Demand Profiling for Dynamic Traffic Assignment by Integrating Departure Time Choice and Trip Distribution},
	year = {2016},
	journal = {Computer-Aided Civil and Infrastructure Engineering},
	volume = {31},
	number = {2},
	pages = {86 – 99},
	doi = {10.1111/mice.12140},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953839089&doi=10.1111%2fmice.12140&partnerID=40&md5=64c8d6d4e60dca0797db32dd6d94050c},
	abstract = {One challenge in dynamic traffic assignment (DTA) modeling is estimating the finely disaggregated trip matrix required by such models. In previous work, an exogenous time distribution profile for trip departure rates is applied uniformly across all origin-destination (O-D) pairs. This article develops an endogenous departure time choice model based on an arrival time penalty function incorporated into trip distribution, which results in distinct demand profiles by O-D pair. This yields a simultaneous departure time and trip choice making use of the time-varying travel times in DTA. The required input is arrival time preferences, which can be disaggregated by O-D pair and may be easier to collect through surveys than the demand profile. This model is integrated into the four-step planning process with feedback, creating an extension of previous frameworks which aggregate over time. Empirical results from a network representing Austin, Texas indicate variation in departure time choice appropriate to the arrival time penalties and travel times. Our model also appears to converge faster to a dynamic trip table prediction than a time-aggregated coupling of DTA and planning, potentially reducing the substantial computation time of combined planning models that solve DTA as a subproblem of a feedback process. ©2016 Computer-Aided Civil and Infrastructure Engineering.},
	keywords = {Austin; Texas; United States; Aggregates; Transportation; Travel time; Zoning; Departure time choice; Dynamic traffic assignment(DTA); Dynamic traffic assignments; Feedback process; Origin-destination pairs; Penalty function; Time distribution; Trip distribution; empirical analysis; numerical model; traffic management; transportation planning; travel demand; Traffic control},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@ARTICLE{Zhang201812,
	author = {Zhang, Xiaokang and Liu, Huanjun and Zhang, Xinle and Yu, Shengnan and Dou, Xin and Xie, Yahui and Wang, Nan},
	title = {Allocate soil individuals to soil classes with topsoil spectral characteristics and decision trees},
	year = {2018},
	journal = {Geoderma},
	volume = {320},
	pages = {12 – 22},
	doi = {10.1016/j.geoderma.2018.01.023},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042906397&doi=10.1016%2fj.geoderma.2018.01.023&partnerID=40&md5=510e0f68bf995a05f5897717ebed8435},
	abstract = {Spectral reflectance of soil is a function of physical and chemical characteristics and its internal structure. Spectral reflectance provides a novel approach for soil allocation. This paper presents a nondestructive, rapid, and low-cost soil allocation method using topsoil spectral characteristics to allocate soil at the level of soil great group within Genetic Soil Classification of China. We measured the spectral reflectance in the visible and near-infrared regions (400–2500 nm) of 148 soil samples from 4 soil classes in the Songnen Plain of northeast China. We extracted the spectral characteristic parameters with clear physiochemical meanings for the topsoil samples, and compared these to the principle component, first spectral derivative and Continuum Removal of soil reflectance. Models were built using the K-means Clustering (K-mean), Multi-layer Perceptron Neural Network (MLPNN), Support Vector Machine (SVM), and Decision Tree (DT) methods. The DTs allocation model based on topsoil spectral characteristic parameters had the highest allocation accuracy. Only the allocation accuracy of Cambisols was <85%, because the spectral curve of Cambisols topsoil was similar to its adjacent soil due to soil erosion. This new method could simplify digital soil mapping, because topsoil spectra are easier to obtain than multilayer soil spectral data. © 2018 Elsevier B.V.},
	author_keywords = {Decision trees; K-means clustering; Neural network; Soil classification; Support vector machine; Topsoil spectral characteristics},
	keywords = {China; Songnen Plain; Decision trees; Infrared devices; Network layers; Neural networks; Photomapping; Reflection; Soil surveys; Support vector machines; Digital soil mappings; K - means clustering; Multi layer perceptron neural networks (MLPNN); Physical and chemical characteristics; Soil classification; Spectral characteristics; Spectral reflectances; Visible and near infrared; artificial neural network; Cambisol; cluster analysis; decision analysis; physicochemical property; soil analysis; soil classification; spectral reflectance; support vector machine; topsoil; Soils},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 41}
}

@CONFERENCE{Cunin2012271,
	author = {Cunin, L. and Nonin, P. and Janoth, J. and Bernard, M.},
	title = {Extracting precise and affordable dems despite of the clouds. AJAX: The joining of radar and optical strengths},
	year = {2012},
	journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
	volume = {39},
	pages = {271 – 274},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924292509&partnerID=40&md5=6115a19359d282610a74753f29b34874},
	abstract = {On the one hand, onboard SPOT 5, the HRS instrument systematically collects stereopairs around the Globe since 2002. Each stereopair can encompass an area up to 600 km x 120 km within a single pass (i.e. 72 000 km2stereoscopic strips). Covering now more than 120 millions sq.km of the Earth landmasses, SPOT 5 stereoscopic imagery has become one of main satellite data sources for accurate DEM extraction, at least where the cloud coverage leaves a chance to do so ! On the other hand, the TerraSAR-X satellite, launched in June 2007, is able to collect radar data through the clouds in several modes. An approach to extract height information by radargrammetry was developed, and the commercial distribution of Digital Elevation Models based on TerraSAR-X StripMap and SpotLight Modes (resp. 3m and 1m resolution) has started in 2010. To improve the overall height accuracy of the DEM, acquisitions from both orbit directions are utilised, each point on the ground being thus imaged at least 4 times by TerraSAR-X. Since 2002, Spot Image and French National Cartographic Institute (IGN) are building a worldwide database called Elevation30/Reference3D™, which includes a Digital Elevation Model at 1-arc-second resolution (DTED level 2) extracted from HRS stereopairs. To answer the wide demand of precise DEMs over Tropical and Northern areas, frequently covered by clouds, a study was performed to integrate StripMap radargrammetric TerraSAR-X data into the Reference3D process, and two prototype products were issued, over Colombia and Congo areas. During this experiment, efforts have been made to stick to technical steps that could be integrated within a standardized production process, in order to keep offering affordable prices while maintaining a high standard of horizontal and vertical accuracy. The DEMs extracted from TerraSAR-X and HRS proved extremely consistent with each other, showing a mean difference of 0.80m. This allows to propose a unified Elevation30 product to the users, with a guaranteed accuracy materialized into the product through a dedicated vertical Accuracy Commitment Mask. © 2012 ISPRS.},
	author_keywords = {Cost; DEM/DTM; Fusion; Precision; Quality; Radar; SPOT; Three-dimensional},
	keywords = {Costs; Digital instruments; Forestry; Fusion reactions; Geomorphology; Image quality; Orbits; Photogrammetry; Radar; Remote sensing; Satellite imagery; Satellites; Space-based radar; Stereo image processing; Affordable prices; DEM/DTM; Digital elevation model; Precision; Production process; SPOT; Three-dimensional; Vertical accuracy; Surveying},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Dombkowski2004144,
	author = {Dombkowski, Kevin J. and Lantz, Paula M. and Freed, Gary L.},
	title = {Risk factors for delay in age-appropriate vaccination},
	year = {2004},
	journal = {Public Health Reports},
	volume = {119},
	number = {2},
	pages = {144 – 155},
	doi = {10.1177/003335490411900207},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442642733&doi=10.1177%2f003335490411900207&partnerID=40&md5=77f1c73a05821a225347935a3c0ba1a3},
	abstract = {Objective. To estimate the risk factors of children experiencing delay in age-appropriate vaccination using a nationally representative population of children, and to compare risk factors for vaccination delay with those based on up-to-date vaccination status models. Methods. The authors compared predictors of delay in age-appropriate vaccination with those for children who were not up-to-date, using a nationally representative sample of children from five years of pooled data (1992-1996) from the National Health Interview Survey (NHIS) Immunization Supplement. Duration of delay was calculated for the DTP4, Polio3, MMR1 doses and 4:3:1 series using age-appropriate vaccination standards; up-to-date status (i.e., whether or not a dose was received) was also determined. Adjusted odds ratios were estimated using multivariate logistic regression for models of vaccination delay and up-to-date, vaccination status. Results. Absence of a two-parent household, large family size, parental education, Medicaid enrollment, absence of a usual provider, no insurance coverage, and households without a telephone were significantly related to increased odds of a child experiencing vaccination delay (p≤0.05). Conclusions. Many of the risk factors observed in models of vaccination delay were not found to be significant in risk models based upon up-to-date status. Consequently, risk models of delays in age-appropriate vaccination may foster identification of children at increased risk for inadequate vaccination. Populations at increased risk of inadequate vaccination can be more clearly identified through risk models of delays in age-appropriate vaccination.},
	keywords = {diphtheria pertussis tetanus vaccine; measles mumps rubella vaccine; poliomyelitis vaccine; age; article; drug dose regimen; education; family size; female; health insurance; health survey; high risk population; human; major clinical study; male; medicaid; multivariate logistic regression analysis; preschool child; priority journal; risk factor; single parent; telephone; vaccination},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 77; All Open Access, Green Open Access}
}

@ARTICLE{Maoz20123282,
	author = {Maoz, Dan and Mannucci, Filippo and Brandt, Timothy D.},
	title = {The delay-time distribution of Type Ia supernovae from Sloan II},
	year = {2012},
	journal = {Monthly Notices of the Royal Astronomical Society},
	volume = {426},
	number = {4},
	pages = {3282 – 3294},
	doi = {10.1111/j.1365-2966.2012.21871.x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867881050&doi=10.1111%2fj.1365-2966.2012.21871.x&partnerID=40&md5=b438728880015cc6a545a256aa022602},
	abstract = {We derive the delay-time distribution (DTD) of Type Ia supernovae (SNe Ia) using a sample of 132 SNe Ia, discovered by the Sloan Digital Sky Survey II (SDSS2) among 66000 galaxies with spectral-based star formation histories (SFHs). To recover the best-fitting DTD, the SFH of every individual galaxy is compared, using Poisson statistics, to the number of SNe that it hosted (zero or one), based on the method introduced in Maoz et al. This SN sample differs from the SDSS2 SN Ia sample analysed by Brandt et al., using a related, but different, DTD recovery method. Furthermore, we use a simulation-based SN detection-efficiency function, and we apply a number of important corrections to the galaxy SFHs and SN Ia visibility times. The DTD that we find has 4σ detections in all three of its time bins: prompt (τ < 0.42Gyr), intermediate (0.42 < τ < 2.4Gyr) and delayed (τ > 2.4Gyr), indicating a continuous DTD, and it is among the most accurate and precise among recent DTD reconstructions. The best-fitting power-law form to the recovered DTD is t-1.07 ± 0.07, consistent with generic ∼t-1 predictions of SN Ia progenitor models based on the gravitational-wave-induced mergers of binary white dwarfs. The time-integrated number of SNe Ia per formed stellar mass is NSN/M = 0.00130 ± 0.00015M⊙-1, or about 4per cent of the stars formed with initial masses in the 3 - 8M⊙ range. This is lower than, but largely consistent with, several recent DTD estimates based on SN rates in galaxy clusters and in local-volume galaxies, and is higher than, but consistent with NSN/M estimated by comparing volumetric SN Ia rates to cosmic SFH. © 2012 The Authors Monthly Notices of the Royal Astronomical Society © 2012 RAS.},
	author_keywords = {Galaxies: star formation; Methods: data analysis; Supernovae: general},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 218; All Open Access, Bronze Open Access}
}

@ARTICLE{Chen200179,
	author = {Chen, Li and Rundensteiner, Elke and Ally, Afshan and Chen, Rice and Kou, Weidong},
	title = {Active page generation via customizing XML for data beans in E-commerce applications},
	year = {2001},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {2040},
	pages = {79 – 97},
	doi = {10.1007/3-540-45415-2_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959047692&doi=10.1007%2f3-540-45415-2_7&partnerID=40&md5=234033876f0c77631ad1319807a869b6},
	abstract = {In this paper, we analyze enabling technologies for typical e-business applications in terms of their multi-tier system architecture and their Model-Control-View (MCV) programming model. Based on observed limitations of the JSP (Java Server Page) technique commonly adopted for dynamic page generation (the view logic), we instead pro- pose an alternative solution approach, namely, a generic schema map- ping strategy to generate XML documents and DTDs from enterprise data beans. First, we describe in detail the XML generation process for the content composition logic. We also outline the XSL processing for the transformation logic. The separation of these two logics results in a generic solution to the bean viewing problem. In particular, it improves the bean reusability via its XML representative compared to the rigid strategy of hard-coding logics into JSP. Our proposed XML mapping solution represents a potentially valuable addition to future versions of the enterprise data beans specification. The trade-of between performance and exibility of these alternative solutions is discussed. Lastly, we survey the state-of-art research results and emerging standards related to this XML model mapping approach. © Springer-Verlag Berlin Heidelberg 2001.},
	keywords = {Commerce; Computer circuits; Computer systems programming; Electronic commerce; Reconfigurable hardware; Reusability; Alternative solutions; E-business applications; E-Commerce applications; Enabling technologies; Generation process; Generic solutions; Java server pages; Programming models; XML},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}